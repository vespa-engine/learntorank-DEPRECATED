{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to evaluate Vespa ranking functions from python\n",
    "\n",
    "> Using [pyvespa](https://pyvespa.readthedocs.io/en/latest/index.html) to evaluate [cord19 search application](https://cord19.vespa.ai/) ranking functions currently in production.\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/vespa-engine/pyvespa/blob/master/docs/sphinx/source/use_cases/cord19/cord19_connect_evaluate.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download processed data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can start by downloading the data that we [have processed before](https://pyvespa.readthedocs.io/en/latest/use_cases/cord19/cord19_download_parse_trec_covid.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, json\n",
    "from pandas import read_csv\n",
    "\n",
    "topics = json.loads(\n",
    "    requests.get(\"https://thigm85.github.io/data/cord19/topics.json\").text\n",
    ")\n",
    "relevance_data = read_csv(\"https://thigm85.github.io/data/cord19/relevance_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`topics` contain data about the 50 topics available, including `query`, `question` and `narrative`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'coronavirus origin',\n",
       " 'question': 'what is the origin of COVID-19',\n",
       " 'narrative': \"seeking range of information about the SARS-CoV-2 virus's origin, including its evolution, animal source, and first transmission into humans\"}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics[\"1\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`relevance_data` contains the relevance judgments for each of the 50 topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic_id</th>\n",
       "      <th>round_id</th>\n",
       "      <th>cord_uid</th>\n",
       "      <th>relevancy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>4.5</td>\n",
       "      <td>005b2j4b</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>00fmeepz</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>010vptx3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0194oljo</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>021q9884</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   topic_id  round_id  cord_uid  relevancy\n",
       "0         1       4.5  005b2j4b          2\n",
       "1         1       4.0  00fmeepz          1\n",
       "2         1       0.5  010vptx3          2\n",
       "3         1       2.5  0194oljo          1\n",
       "4         1       4.0  021q9884          1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevance_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format the labeled data into expected pyvespa format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pyvespa` expects labeled data to follow the format illustrated below. It is a list of dict where each dict represents a query containing `query_id`, `query` and a list of `relevant_docs`. Each relevant document contains a required `id` key and an optional `score` key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_data = [\n",
    "    {\n",
    "        'query_id': 1,\n",
    "        'query': 'coronavirus origin',\n",
    "        'relevant_docs': [{'id': '005b2j4b', 'score': 2}, {'id': '00fmeepz', 'score': 1}]\n",
    "    },\n",
    "    {\n",
    "        'query_id': 2,\n",
    "        'query': 'coronavirus response to weather changes',\n",
    "        'relevant_docs': [{'id': '01goni72', 'score': 2}, {'id': '03h85lvy', 'score': 2}]\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create `labeled_data` from the `topics` and `relevance_data` that we downloaded before. We are only going to include documents with relevance score > 0 into the final list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_data = [\n",
    "    {\n",
    "        \"query_id\": int(topic_id), \n",
    "        \"query\": topics[topic_id][\"query\"], \n",
    "        \"relevant_docs\": [\n",
    "            {\n",
    "                \"id\": row[\"cord_uid\"], \n",
    "                \"score\": row[\"relevancy\"]\n",
    "            } for idx, row in relevance_data[relevance_data.topic_id == int(topic_id)].iterrows() if row[\"relevancy\"] > 0\n",
    "        ]\n",
    "    } for topic_id in topics.keys()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define query models to be evaluated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to define two query models to be evaluated here. Both will match all the documents that share at least one term with the query. This is defined by setting `match_phase = OR()`. \n",
    "\n",
    "The difference between the query models happens in the ranking phase. The `or_default` model will rank documents based on [nativeRank](https://docs.vespa.ai/en/nativerank.html) while the `or_bm25` model will rank documents based on [BM25](https://docs.vespa.ai/en/reference/bm25.html). Discussion about those two types of ranking is out of the scope of this tutorial. It is enough to know that they rank documents according to two different formulas.\n",
    "\n",
    "Those ranking profiles were defined by the team behind the cord19 app and can be found [here](https://github.com/vespa-cloud/cord-19-search/blob/main/src/main/application/schemas/doc.sd)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from learntorank.query import QueryModel, Ranking, OR\n",
    "\n",
    "query_models = [\n",
    "    QueryModel(\n",
    "        name=\"or_default\",\n",
    "        match_phase = OR(),\n",
    "        ranking = Ranking(name=\"default\")\n",
    "    ),\n",
    "    QueryModel(\n",
    "        name=\"or_bm25\",\n",
    "        match_phase = OR(),\n",
    "        ranking = Ranking(name=\"bm25t5\")\n",
    "    )\n",
    "]       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define metrics to be used in the evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to compute the following metrics:\n",
    "\n",
    "* The percentage of documents matched by the query\n",
    "\n",
    "* Recall @ 10\n",
    "\n",
    "* Reciprocal rank @ 10\n",
    "\n",
    "* NDCG @ 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from learntorank.evaluation import MatchRatio, Recall, ReciprocalRank, NormalizedDiscountedCumulativeGain\n",
    "\n",
    "eval_metrics = [\n",
    "    MatchRatio(), \n",
    "    Recall(at=10), \n",
    "    ReciprocalRank(at=10), \n",
    "    NormalizedDiscountedCumulativeGain(at=10)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connect to a running Vespa instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vespa.application import Vespa\n",
    "\n",
    "app = Vespa(url = \"https://api.cord19.vespa.ai\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the metrics defined above for each query model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>or_bm25</th>\n",
       "      <th>or_default</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">match_ratio</th>\n",
       "      <th>mean</th>\n",
       "      <td>0.411789</td>\n",
       "      <td>0.411789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>median</th>\n",
       "      <td>0.282227</td>\n",
       "      <td>0.282227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.238502</td>\n",
       "      <td>0.238502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">recall_10</th>\n",
       "      <th>mean</th>\n",
       "      <td>0.007720</td>\n",
       "      <td>0.005457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>median</th>\n",
       "      <td>0.006089</td>\n",
       "      <td>0.003753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.006386</td>\n",
       "      <td>0.005458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">reciprocal_rank_10</th>\n",
       "      <th>mean</th>\n",
       "      <td>0.594357</td>\n",
       "      <td>0.561579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>median</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.397597</td>\n",
       "      <td>0.401255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">ndcg_10</th>\n",
       "      <th>mean</th>\n",
       "      <td>0.353095</td>\n",
       "      <td>0.274515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>median</th>\n",
       "      <td>0.355978</td>\n",
       "      <td>0.253619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.216460</td>\n",
       "      <td>0.203170</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "model                       or_bm25  or_default\n",
       "match_ratio        mean    0.411789    0.411789\n",
       "                   median  0.282227    0.282227\n",
       "                   std     0.238502    0.238502\n",
       "recall_10          mean    0.007720    0.005457\n",
       "                   median  0.006089    0.003753\n",
       "                   std     0.006386    0.005458\n",
       "reciprocal_rank_10 mean    0.594357    0.561579\n",
       "                   median  0.500000    0.500000\n",
       "                   std     0.397597    0.401255\n",
       "ndcg_10            mean    0.353095    0.274515\n",
       "                   median  0.355978    0.253619\n",
       "                   std     0.216460    0.203170"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from learntorank.evaluation import evaluate\n",
    "evaluations = evaluate(\n",
    "    app=app,\n",
    "    labeled_data = labeled_data,\n",
    "    eval_metrics = eval_metrics,\n",
    "    query_model = query_models,\n",
    "    id_field = \"cord_uid\",\n",
    "    hits = 10\n",
    ")\n",
    "evaluations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also return per query raw evaluation metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>query_id</th>\n",
       "      <th>match_ratio</th>\n",
       "      <th>recall_10</th>\n",
       "      <th>reciprocal_rank_10</th>\n",
       "      <th>ndcg_10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>or_default</td>\n",
       "      <td>1</td>\n",
       "      <td>0.230847</td>\n",
       "      <td>0.008584</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.519431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>or_default</td>\n",
       "      <td>2</td>\n",
       "      <td>0.755230</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>or_default</td>\n",
       "      <td>3</td>\n",
       "      <td>0.264601</td>\n",
       "      <td>0.001534</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.036682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>or_default</td>\n",
       "      <td>4</td>\n",
       "      <td>0.843341</td>\n",
       "      <td>0.001764</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.110046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>or_default</td>\n",
       "      <td>5</td>\n",
       "      <td>0.901317</td>\n",
       "      <td>0.003096</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.258330</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        model  query_id  match_ratio  recall_10  reciprocal_rank_10   ndcg_10\n",
       "0  or_default         1     0.230847   0.008584            1.000000  0.519431\n",
       "1  or_default         2     0.755230   0.000000            0.000000  0.000000\n",
       "2  or_default         3     0.264601   0.001534            0.142857  0.036682\n",
       "3  or_default         4     0.843341   0.001764            0.333333  0.110046\n",
       "4  or_default         5     0.901317   0.003096            0.250000  0.258330"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluations = evaluate(\n",
    "    app=app,\n",
    "    labeled_data = labeled_data,\n",
    "    eval_metrics = eval_metrics,\n",
    "    query_model = query_models,\n",
    "    id_field = \"cord_uid\",\n",
    "    hits = 10,\n",
    "    per_query = True\n",
    ")\n",
    "evaluations.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
