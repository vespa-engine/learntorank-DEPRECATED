{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "unauthorized-sentence",
   "metadata": {},
   "source": [
    "![Vespa logo](https://vespa.ai/assets/vespa-logo-color.png)\n",
    "\n",
    "# Image search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "initial-height",
   "metadata": {},
   "source": [
    "This notebook walks through the pyvespa code used to create the [text to image search sample application](https://github.com/vespa-engine/sample-apps/tree/master/text-image-search/src/python).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6206545e",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "**ToDo**: This notebook is still work in progress and cannot yet be auto-run\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "trying-jacksonville",
   "metadata": {},
   "source": [
    "![SegmentLocal](demo.gif \"segment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interracial-scientist",
   "metadata": {},
   "source": [
    "## Create the application package\n",
    "Create an application package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inner-minimum",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vespa.package import ApplicationPackage\n",
    "\n",
    "app_package = ApplicationPackage(name=\"imagesearch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gentle-mountain",
   "metadata": {},
   "source": [
    "Add a field to hold the name of the image file. This is used in the sample app to load the final images that should be displayed to the end user. \n",
    "\n",
    "The `summary` indexing ensures this field is returned as part of the query response. The `attribute` indexing store the fields in memory as an attribute for sorting, querying, and grouping:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "micro-oxygen",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vespa.package import Field\n",
    "\n",
    "app_package.schema.add_fields(\n",
    "    Field(name=\"image_file_name\", type=\"string\", indexing=[\"summary\", \"attribute\"]),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "historic-explorer",
   "metadata": {},
   "source": [
    "Add a field to hold an image embedding. The embeddings are usually generated by a ML model. We can add multiple embedding fields to our application. This is useful when making experiments. For example, the sample app adds 6 image embeddings, one for each of the six pre-trained CLIP models available at the time.\n",
    "\n",
    "In the example below, the embedding vector has size `512` and is of type `float`. The `index` is required to enable [approximate matching](https://docs.vespa.ai/en/approximate-nn-hnsw.html) and the `HNSW` instance configure the HNSW index: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "twenty-montgomery",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vespa.package import HNSW\n",
    "\n",
    "app_package.schema.add_fields(\n",
    "    Field(\n",
    "        name=\"embedding_image\",\n",
    "        type=\"tensor<float>(x[512])\",\n",
    "        indexing=[\"attribute\", \"index\"],\n",
    "        ann=HNSW(\n",
    "            distance_metric=\"angular\",\n",
    "            max_links_per_node=16,\n",
    "            neighbors_to_explore_at_insert=500,\n",
    "        ),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "instant-fluid",
   "metadata": {},
   "source": [
    "Add a rank profile that ranks the images by how close the image embedding vector is from the query embedding vector.\n",
    "The tensors used in queries must have their type declared in the application package, the code below declares the text embedding that will be sent in the query - it has the same size and type of the image embedding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "systematic-manitoba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vespa.package import RankProfile\n",
    "\n",
    "app_package.schema.add_rank_profile(\n",
    "    RankProfile(\n",
    "        name=\"embedding_similarity\",\n",
    "        inherits=\"default\",\n",
    "        first_phase=\"closeness(embedding_image)\",\n",
    "        inputs=[(\"query(embedding_text)\", \"tensor<float>(x[512])\")],\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liked-market",
   "metadata": {},
   "source": [
    "## Deploy the application\n",
    "The application package created above can be deployed using\n",
    "[Docker](https://pyvespa.readthedocs.io/en/latest/getting-started-pyvespa.html#Deploy-the-application-using-Docker) or\n",
    "[Vespa Cloud](https://pyvespa.readthedocs.io/en/latest/deploy-vespa-cloud.html).\n",
    "Follow the instructions based on the desired deployment mode.\n",
    "Either option will create a Vespa connection instance\n",
    "that can be stored in a variable that will be denoted here as `app`.\n",
    "\n",
    "We can then use `app` to interact with the deployed application:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b130ea85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for configuration server, 0/300 seconds...\n",
      "Waiting for configuration server, 5/300 seconds...\n",
      "Waiting for application status, 0/300 seconds...\n",
      "Waiting for application status, 5/300 seconds...\n",
      "Waiting for application status, 10/300 seconds...\n",
      "Waiting for application status, 15/300 seconds...\n",
      "Waiting for application status, 20/300 seconds...\n",
      "Waiting for application status, 25/300 seconds...\n",
      "Finished deployment.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from vespa.deployment import VespaDocker\n",
    "\n",
    "vespa_docker = VespaDocker(\n",
    "    port=8080\n",
    ")\n",
    "\n",
    "app = vespa_docker.deploy(application_package = app_package)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overhead-record",
   "metadata": {},
   "source": [
    "## Feed the image data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recovered-championship",
   "metadata": {},
   "source": [
    "ToDo: Add code below to create the feed and set batch - until then, disabled auto testing.\n",
    "\n",
    "To feed the image data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "under-paste",
   "metadata": {},
   "outputs": [],
   "source": [
    "responses = app.feed_batch(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adequate-edinburgh",
   "metadata": {},
   "source": [
    "where `batch` is a list of dictionaries like the one below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "experimental-heritage",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "    \"id\": \"dog1\",\n",
    "    \"fields\": {\n",
    "        \"image_file_name\": \"dog1.jpg\",\n",
    "        \"embedding_image\": {\"values\": [0.884, -0.345, ..., 0.326]},\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sharing-health",
   "metadata": {},
   "source": [
    "One of the advantages of having a python API is that it can integrate with commonly used ML frameworks. The sample application [show how to create a PyTorch DataLoader](https://github.com/vespa-engine/sample-apps/blob/master/text-image-search/src/python/embedding.py#L85-L113) to generate batches of image data by using CLIP models to generate image embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "educational-danish",
   "metadata": {},
   "source": [
    "## Query the application"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continental-student",
   "metadata": {},
   "source": [
    "The following query will use approximate nearest neighbor search to match the closest images to the query text and rank the images according to their distance to the query text. The sample application used CLIP models to generate image and query embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "large-switch",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = app.query(body={\n",
    "    \"yql\": 'select * from sources * where ({targetHits:100}nearestNeighbor(embedding_image,embedding_text));',\n",
    "    \"hits\": 100,\n",
    "    \"input.query(embedding_text)\": [0.632, -0.987, ..., 0.534],\n",
    "    \"ranking.profile\": \"embedding_similarity\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incoming-hollywood",
   "metadata": {},
   "source": [
    "## Evaluate different query models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "governing-morgan",
   "metadata": {},
   "source": [
    "Define metrics to evaluate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elder-tower",
   "metadata": {},
   "outputs": [],
   "source": [
    "from learntorank.evaluation import MatchRatio, Recall, ReciprocalRank\n",
    "\n",
    "eval_metrics = [\n",
    "    MatchRatio(), \n",
    "    Recall(at=100), \n",
    "    ReciprocalRank(at=100)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caroline-devices",
   "metadata": {},
   "source": [
    "The sample application illustrates how to evaluate different CLIP models through the `evaluate` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "functional-stand",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = app.evaluate(\n",
    "    labeled_data=labeled_data,  # Labeled data to define which images should be returned to a given query\n",
    "    eval_metrics=eval_metrics,  # Metrics used\n",
    "    query_model=query_models,   # Each query model uses a different CLIP model version\n",
    "    id_field=\"image_file_name\", # The name of the id field used by the labeled data to identify the image\n",
    "    per_query=True              # Return results per query rather the aggragated.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "level-colors",
   "metadata": {},
   "source": [
    "The figure below is the reciprocal rank at 100 computed based on the output of the `evaluate` method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "canadian-gambling",
   "metadata": {},
   "source": [
    "![evaluation](clip-evaluation-boxplot.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
