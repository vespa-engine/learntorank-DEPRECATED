[
  {
    "objectID": "index.html#motivation",
    "href": "index.html#motivation",
    "title": "Vespa for Data Scientists",
    "section": "Motivation",
    "text": "Motivation\nThis library contains application specific code related to data manipulation and analysis of different Vespa use cases. The Vespa python API is used to interact with Vespa applications from python for faster exploration.\nThe main goal of this space is to facilitate prototyping and experimentation for data scientists. Please visit Vespa sample apps for production-ready use cases and Vespa docs for in-depth Vespa documentation."
  },
  {
    "objectID": "index.html#install",
    "href": "index.html#install",
    "title": "Vespa for Data Scientists",
    "section": "Install",
    "text": "Install\nCode to support and reproduce the use cases documented here can be found in the learntorank library.\nInstall via PyPI:\npip install learntorank"
  },
  {
    "objectID": "index.html#development",
    "href": "index.html#development",
    "title": "Vespa for Data Scientists",
    "section": "Development",
    "text": "Development\nAll the code and content of this repo is created using nbdev by editing notebooks. We will give a summary below about the main points required to contribute, but we suggest going through nbdev tutorials to learn more.\n\nSetting up environment\n\nCreate and activate a virtual environment of your choice. We recommend pipenv.\npipenv shell\nInstall Jupyter Lab (or Jupyter Notebook if you prefer).\npip3 install jupyterlab\nCreate a new kernel for Jupyter that uses the virtual environment created at step 1.\n\nCheck where the current list of kernels is located with jupyter kernelspec list.\nCopy one of the existing folder and rename it to learntorank.\nModify the kernel.json file that is inside the new folder to reflect the python3executable associated with your virtual env.\n\nInstall nbdev library:\npip3 install nbdev\nInstall learntorank in development mode:\npip3 install -e .[dev]\n\n\n\nMost used nbdev commands\nFrom your terminal:\n\nnbdev_help: List all nbdev commands available.\nnbdev_readme: Update README.md based on index.ipynb\nPreview documentation while editing the notebooks:\n\nnbdev_preview --port 3000\n\nWorkflow before pushing code:\n\nnbdev_test --n_workers 2: Execute all the tests inside notebooks.\n\nTests can run in parallel but since we create Docker containers we suggest a low number of workers to preserve memory.\n\nnbdev_export: Export code from notebooks to the python library.\nnbdev_clean: Clean notebooks to avoid merge conflicts.\n\nPublish library\n\nnbdev_bump_version: Bump library version.\nnbdev_pypi: Publish library to PyPI."
  },
  {
    "objectID": "notebooks/learning-to-rank-ignore.html",
    "href": "notebooks/learning-to-rank-ignore.html",
    "title": "Learning to rank",
    "section": "",
    "text": "Vespa logo\nThis notebook is WIP and not runnable - ToDo FIXME"
  },
  {
    "objectID": "notebooks/learning-to-rank-ignore.html#data",
    "href": "notebooks/learning-to-rank-ignore.html#data",
    "title": "Learning to rank",
    "section": "Data",
    "text": "Data\nThis section describes the data that we are going to use to give a brief overview of the pyvespa ranking framework. The data was collected from a running Vespa application indexed with MS MARCO data. For each relevant (document_id, query_id)-pair we collected 9 random matched documents. Relevant documents have label=1 and non-relevant documents have label=0. In addition, many Vespa ranking features computed based on document and query interaction are included.\n\ntrain_df = pd.read_csv(\"https://data.vespa.oath.cloud/blog/ranking/train_sample.csv\")\n\nThe data used here is a sample containing 100.000 rows and 71 features.\n\ntrain_df.shape\n\n(100000, 74)\n\n\n\ntrain_df.head(10)\n\n\n\n\n\n\n\n\ndocument_id\nquery_id\nlabel\nelementCompleteness(body).completeness\nelementCompleteness(body).fieldCompleteness\nelementCompleteness(body).queryCompleteness\nfieldMatch(body)\nfieldMatch(body).absoluteOccurrence\nfieldMatch(body).absoluteProximity\nfieldMatch(body).completeness\n...\nterm(3).significance\nterm(3).weight\nterm(4).connectedness\nterm(4).significance\nterm(4).weight\ntextSimilarity(body).fieldCoverage\ntextSimilarity(body).order\ntextSimilarity(body).proximity\ntextSimilarity(body).queryCoverage\ntextSimilarity(body).score\n\n\n\n\n0\n27061\n3\n0\n0.358796\n0.092593\n0.625\n0.127746\n0.022000\n0.02600\n0.598380\n...\n0.504935\n100.0\n0.1\n0.674337\n100.0\n0.092593\n0.250000\n0.437500\n0.625\n0.396644\n\n\n1\n257\n3\n0\n0.359670\n0.094340\n0.625\n0.092319\n0.018000\n0.03500\n0.598467\n...\n0.504935\n100.0\n0.1\n0.674337\n100.0\n0.094340\n0.750000\n0.234375\n0.625\n0.400899\n\n\n2\n363\n3\n0\n0.277397\n0.054795\n0.500\n0.141511\n0.030000\n0.07100\n0.477740\n...\n0.504935\n100.0\n0.1\n0.674337\n100.0\n0.054795\n0.666667\n0.640625\n0.500\n0.485178\n\n\n3\n22682\n3\n0\n0.333686\n0.042373\n0.625\n0.250817\n0.056000\n0.10000\n0.595869\n...\n0.504935\n100.0\n0.1\n0.674337\n100.0\n0.042373\n0.250000\n0.324219\n0.625\n0.346951\n\n\n4\n160\n3\n0\n0.295455\n0.090909\n0.500\n0.118351\n0.015000\n0.05000\n0.479545\n...\n0.504935\n100.0\n0.1\n0.674337\n100.0\n0.090909\n0.666667\n0.557292\n0.500\n0.463234\n\n\n5\n228\n3\n0\n0.286364\n0.072727\n0.500\n0.148612\n0.015000\n0.10000\n0.478636\n...\n0.504935\n100.0\n0.1\n0.674337\n100.0\n0.072727\n0.000000\n0.286458\n0.500\n0.264806\n\n\n6\n3901893\n3\n0\n0.433824\n0.117647\n0.750\n0.345256\n0.025000\n0.07700\n0.718382\n...\n0.504935\n100.0\n0.1\n0.674337\n100.0\n0.117647\n0.600000\n0.575000\n0.750\n0.539779\n\n\n7\n1142680\n3\n1\n0.412037\n0.074074\n0.750\n0.343120\n0.046667\n0.07700\n0.716204\n...\n0.504935\n100.0\n0.1\n0.674337\n100.0\n0.074074\n0.600000\n0.615625\n0.750\n0.545284\n\n\n8\n141\n3\n0\n0.286364\n0.072727\n0.500\n0.081461\n0.027500\n0.10000\n0.478636\n...\n0.504935\n100.0\n0.1\n0.674337\n100.0\n0.072727\n0.666667\n0.406250\n0.500\n0.406733\n\n\n9\n3060834\n3\n0\n0.410294\n0.070588\n0.750\n0.308250\n0.045000\n0.06675\n0.716029\n...\n0.504935\n100.0\n0.1\n0.674337\n100.0\n0.070588\n0.400000\n0.715625\n0.750\n0.549586\n\n\n\n\n10 rows × 74 columns\n\n\n\nSimilarly, we collected data based on the MS MARCO queries contained on the dev set.\n\ndev_df = pd.read_csv(\"https://data.vespa.oath.cloud/blog/ranking/dev_sample.csv\")\n\n\ndev_df.shape\n\n(74103, 72)\n\n\n\ndev_df.head(10)\n\n\n\n\n\n\n\n\ndocument_id\nquery_id\nlabel\nelementCompleteness(body).completeness\nelementCompleteness(body).fieldCompleteness\nelementCompleteness(body).queryCompleteness\nfieldMatch(body)\nfieldMatch(body).absoluteOccurrence\nfieldMatch(body).absoluteProximity\nfieldMatch(body).completeness\n...\nterm(3).significance\nterm(3).weight\nterm(4).connectedness\nterm(4).significance\nterm(4).weight\ntextSimilarity(body).fieldCoverage\ntextSimilarity(body).order\ntextSimilarity(body).proximity\ntextSimilarity(body).queryCoverage\ntextSimilarity(body).score\n\n\n\n\n0\n8066640\n2\n0\n0.380952\n0.095238\n0.666667\n0.427344\n0.01\n0.1\n0.638095\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.095238\n1.0\n1.0\n0.666667\n0.719048\n\n\n1\n4339068\n2\n1\n0.346667\n0.026667\n0.666667\n0.444933\n0.04\n0.1\n0.634667\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.026667\n1.0\n1.0\n0.666667\n0.705333\n\n\n2\n762768\n2\n0\n0.343750\n0.020833\n0.666667\n0.088859\n0.01\n0.1\n0.634375\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.020833\n1.0\n0.0\n0.666667\n0.354167\n\n\n3\n3370\n2\n0\n0.180180\n0.027027\n0.333333\n0.162049\n0.01\n0.1\n0.318018\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.027027\n0.0\n0.0\n0.333333\n0.105405\n\n\n4\n6060\n2\n0\n0.175287\n0.017241\n0.333333\n0.145722\n0.01\n0.1\n0.317529\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.017241\n0.0\n0.0\n0.333333\n0.103448\n\n\n5\n3798\n2\n0\n0.180556\n0.027778\n0.333333\n0.166942\n0.01\n0.1\n0.318056\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.027778\n0.0\n0.0\n0.333333\n0.105556\n\n\n6\n2731175\n2\n0\n0.345833\n0.025000\n0.666667\n0.398800\n0.01\n0.1\n0.634583\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.025000\n1.0\n1.0\n0.666667\n0.705000\n\n\n7\n3634083\n2\n0\n0.351190\n0.035714\n0.666667\n0.423611\n0.02\n0.1\n0.635119\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.035714\n1.0\n1.0\n0.666667\n0.707143\n\n\n8\n112126\n2\n0\n0.176282\n0.019231\n0.333333\n0.177009\n0.02\n0.1\n0.317628\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.019231\n0.0\n0.0\n0.333333\n0.103846\n\n\n9\n3387\n2\n0\n0.178571\n0.023810\n0.333333\n0.171357\n0.01\n0.1\n0.317857\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.023810\n0.0\n0.0\n0.333333\n0.104762\n\n\n\n\n10 rows × 72 columns"
  },
  {
    "objectID": "notebooks/learning-to-rank-ignore.html#listwise-ranking-framework",
    "href": "notebooks/learning-to-rank-ignore.html#listwise-ranking-framework",
    "title": "Learning to rank",
    "section": "Listwise ranking framework",
    "text": "Listwise ranking framework\nThe ListwiseRankingFramework uses TensorFlow Ranking to minimize a listwise loss function that is a smooth approximation of the NDCG metric. The following parameters need to be specified:\n\nfrom learntorank.ranking import ListwiseRankingFramework\n\nranking_framework = ListwiseRankingFramework(\n    #\n    # Task related \n    #\n    number_documents_per_query=10,  # The size of the list for each sample\n    top_n=10,                       # What NDCG position we want to optmize, e.g. NDCG@10\n    #\n    # Data pipeline \n    #\n    batch_size=32,                  # Batch size used when fitting models to the data\n    shuffle_buffer_size=1000,       # The buffer size used when shuffling data batches.\n    #\n    # Hyperparameter tuning \n    #\n    tuner_max_trials=3,             # How many trials to execute when search hyperparameters\n    tuner_executions_per_trial=1,   # How may model fit per trial\n    tuner_epochs=10,                # How many epochs to use per execution of the trial\n    tuner_early_stop_patience=None, # Set patience number for early stopping\n    #\n    # Final model\n    #\n    final_epochs=30                 # Number of epochs to use when fitting the model with specific hyperparameters.\n)\n\nWARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\nINFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n\n\n\nData pipeline\nIt is possible to create TensorFlow data pipelines (tf.data.Dataset) either from in-memory data frames or directly from .csv files to avoid the need to load large file into memory. The data pipelines are suited for listwise ranking and can be used as part of a custom tensorflow workflow if desired.\nCreate a tf.data.Dataset from in-memory data frames:\n\ntf_ds = ranking_framework.listwise_tf_dataset_from_df(\n    df=train_df, \n    feature_names=[\"nativeFieldMatch\", \"nativeProximity\", \"nativeRank\"],\n    shuffle_buffer_size=3,\n    batch_size=1\n)\n\nNote that the is already suited for listwise learning.\n\nfor batch in tf_ds.take(1):\n    print(batch)\n\n(&lt;tf.Tensor: shape=(1, 10, 3), dtype=float32, numpy=\narray([[[1.9765680e-01, 6.5953881e-02, 9.5175676e-02],\n        [1.3242842e-01, 1.1140537e-01, 7.1235448e-02],\n        [3.4112938e-02, 1.2160993e-37, 1.5161305e-02],\n        [1.5705481e-01, 4.0344268e-02, 7.4284837e-02],\n        [8.6454414e-02, 3.2825880e-02, 4.2071503e-02],\n        [1.9139472e-01, 1.1913208e-01, 9.8301217e-02],\n        [4.8045117e-02, 1.2160993e-37, 2.1353386e-02],\n        [1.4903504e-01, 1.3032080e-01, 8.0717884e-02],\n        [6.3953400e-02, 2.8740479e-02, 3.1617120e-02],\n        [1.5656856e-01, 6.8069249e-02, 7.7149279e-02]]], dtype=float32)&gt;, &lt;tf.Tensor: shape=(1, 10), dtype=float32, numpy=array([[0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]], dtype=float32)&gt;)\n\n\nFor large data, we can also create a listwise tf.data.Dataset directly from a .csv file, without the need to load it into memory:\n\ntrain_df.to_csv(\"train_sample.csv\", index=False)\n\n\ntf_ds = ranking_framework.listwise_tf_dataset_from_csv(\n    file_path=\"train_sample.csv\",\n    feature_names=[\"nativeFieldMatch\", \"nativeProximity\", \"nativeRank\"],\n    shuffle_buffer_size=3,\n    batch_size=1\n)\n\n\nfor batch in tf_ds.take(1):\n    print(batch)\n\n(&lt;tf.Tensor: shape=(1, 10, 3), dtype=float32, numpy=\narray([[[0.08348585, 0.04784278, 0.04242069],\n        [0.08451388, 0.01466913, 0.03919163],\n        [0.07139124, 0.02419666, 0.03441796],\n        [0.07348892, 0.02119719, 0.03501699],\n        [0.11205826, 0.10210748, 0.06114895],\n        [0.06779736, 0.02308168, 0.03269679],\n        [0.08361208, 0.00839302, 0.03809348],\n        [0.13477945, 0.13513905, 0.07491743],\n        [0.17734438, 0.18263273, 0.09911225],\n        [0.12978926, 0.15896696, 0.07534712]]], dtype=float32)&gt;, &lt;tf.Tensor: shape=(1, 10), dtype=float32, numpy=array([[0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]], dtype=float32)&gt;)\n\n\n\n\nPre-defined models\nThe ranking framework comes with same pre-defined models in case you don’t want to use the data pipelines to create your own workflow. It is possible to specify either a DataFrame or a .csv file path as the train and dev input data. If the hyperparameters argument is not specified it will search through the hyperparameter space accordinng to the arguments defined when creating and instance of the ListwiseRankingFramework.\n\nLinear model\n\nweights, dev_eval, best_hyperparams = ranking_framework.fit_linear_model(\n    train_data=train_df, \n    dev_data=dev_df, \n    feature_names=[\n        \"fieldMatch(body).proximity\",\n        \"fieldMatch(body).queryCompleteness\",\n        \"fieldMatch(body).significance\",\n        \"nativeFieldMatch\",\n        \"nativeProximity\",\n        \"nativeRank\",\n    ],\n    hyperparameters=None # Search for best hyperparameters\n)\n\n\nbest_hyperparams\n\n{'learning_rate': 6.018683626059954}\n\n\n\nweights\n\n{'feature_names': ['fieldMatch(body).proximity',\n  'fieldMatch(body).queryCompleteness',\n  'fieldMatch(body).significance',\n  'nativeFieldMatch',\n  'nativeProximity',\n  'nativeRank'],\n 'linear_model_weights': [0.46931159496307373,\n  -30.97307014465332,\n  28.785017013549805,\n  18.257308959960938,\n  12.566983222961426,\n  10.918502807617188]}\n\n\n\ndev_eval\n\n0.7916887402534485\n\n\nIf we instead specify the hyperpameters, hyperparameter search will be skipped.\n\nweights, dev_eval, best_hyperparams = ranking_framework.fit_linear_model(\n    train_data=train_df, \n    dev_data=dev_df, \n    feature_names=[\n        \"fieldMatch(body).proximity\",\n        \"fieldMatch(body).queryCompleteness\",\n        \"fieldMatch(body).significance\",\n        \"nativeFieldMatch\",\n        \"nativeProximity\",\n        \"nativeRank\",\n    ],\n    hyperparameters={'learning_rate': 6.018683626059954} \n)\n\n\n\nLasso model\n\nweights, dev_eval, best_hyperparams = ranking_framework.fit_lasso_linear_model(\n    train_data=train_df, \n    dev_data=dev_df, \n    feature_names=[\n        \"fieldMatch(body).proximity\",\n        \"fieldMatch(body).queryCompleteness\",\n        \"fieldMatch(body).significance\",\n        \"nativeFieldMatch\",\n        \"nativeProximity\",\n        \"nativeRank\",\n    ]\n)\n\n\nprint(best_hyperparams)\n\n{'lambda': 0.0023227311360666802, 'learning_rate': 0.14885653869373894}\n\n\n\nprint(weights)\n\n{'feature_names': ['fieldMatch(body).proximity', 'fieldMatch(body).queryCompleteness', 'fieldMatch(body).significance', 'nativeFieldMatch', 'nativeProximity', 'nativeRank'], 'normalization_mean': [0.8184928894042969, 0.530807375907898, 0.5052036643028259, 0.0906180813908577, 0.039063721895217896, 0.04461509734392166], 'normalization_sd': [0.08662283420562744, 0.05760122463107109, 0.06236378848552704, 0.003072209656238556, 0.003147233510389924, 0.0008713427814655006], 'normalization_number_data': 96990, 'linear_model_weights': [-0.022373167797923088, -2.1850321292877197, 2.055746078491211, 0.21248634159564972, 0.2774745225906372, 0.6118378043174744]}\n\n\n\nprint(dev_eval)\n\n0.7700856328010559\n\n\n\n\n\nFeature selection\nThe are some pre-defined algorithms that can be used for feature selection. The goal is to find a subset of features that are responsible for most of the evaluation metric gains.\n\nLasso model search\nFit a lasso model with all feature_names. Sequentially remove the feature with the smallest absolute weight until there is only one feature in the model.\n\nresults = ranking_framework.lasso_model_search(\n    train_data=train_df, \n    dev_data=dev_df, \n    feature_names=[\n        \"fieldMatch(body).proximity\",\n        \"fieldMatch(body).queryCompleteness\",\n        \"fieldMatch(body).significance\",\n        \"nativeFieldMatch\",\n        \"nativeProximity\",\n        \"nativeRank\",\n    ],\n    output_file=\"lasso_model_search.json\",\n)\n\n\n[\n    f\"Number of features {len(result['weights']['feature_names'])}; Eval metric: {result['evaluation']}\"  \n    for result in results\n]\n\n['Number of features 6; Eval metric: 0.7820510864257812',\n 'Number of features 5; Eval metric: 0.7812100052833557',\n 'Number of features 4; Eval metric: 0.7958707809448242',\n 'Number of features 3; Eval metric: 0.7378504872322083',\n 'Number of features 2; Eval metric: 0.7098456025123596',\n 'Number of features 1; Eval metric: 0.7048170566558838']\n\n\n\n[result['weights']['feature_names'] for result in results]\n\n[['fieldMatch(body).proximity',\n  'fieldMatch(body).queryCompleteness',\n  'fieldMatch(body).significance',\n  'nativeFieldMatch',\n  'nativeProximity',\n  'nativeRank'],\n ['fieldMatch(body).queryCompleteness',\n  'fieldMatch(body).significance',\n  'nativeFieldMatch',\n  'nativeProximity',\n  'nativeRank'],\n ['fieldMatch(body).queryCompleteness',\n  'fieldMatch(body).significance',\n  'nativeFieldMatch',\n  'nativeRank'],\n ['fieldMatch(body).queryCompleteness',\n  'fieldMatch(body).significance',\n  'nativeRank'],\n ['fieldMatch(body).queryCompleteness', 'nativeRank'],\n ['nativeRank']]\n\n\n\n\nForward selection\nIncrementally add one feature at a time and keep the features that maximize the validation metric.\n\nforward_results = ranking_framework.forward_selection_model_search(\n    train_data=train_df, \n    dev_data=dev_df, \n    feature_names=[\n        \"fieldMatch(body).proximity\",\n        \"fieldMatch(body).queryCompleteness\",\n        \"fieldMatch(body).significance\",\n        \"nativeFieldMatch\",\n        \"nativeProximity\",\n        \"nativeRank\",\n    ],\n    output_file=\"forward_model_search.json\",\n)\n\nEvaluation metric for one feature model.\n\n[\n    (result[\"evaluation\"], result[\"weights\"][\"feature_names\"]) for \n     result in forward_results \n     if result[\"number_features\"] == 1\n]\n\n[(0.4771268367767334, ['fieldMatch(body).proximity']),\n (0.5774978995323181, ['fieldMatch(body).queryCompleteness']),\n (0.3523213565349579, ['fieldMatch(body).significance']),\n (0.693596601486206, ['nativeFieldMatch']),\n (0.673930287361145, ['nativeProximity']),\n (0.704784631729126, ['nativeRank'])]\n\n\nEvaluation metric for two features keeping the best feature of the oe-feature model\n\n[\n    (result[\"evaluation\"], result[\"weights\"][\"feature_names\"]) for \n     result in forward_results \n     if result[\"number_features\"] == 2\n]\n\n[(0.7052107453346252, ['nativeRank', 'fieldMatch(body).proximity']),\n (0.7083131670951843, ['nativeRank', 'fieldMatch(body).queryCompleteness']),\n (0.7050297260284424, ['nativeRank', 'fieldMatch(body).significance']),\n (0.7048313617706299, ['nativeRank', 'nativeFieldMatch']),\n (0.7088075876235962, ['nativeRank', 'nativeProximity'])]\n\n\nAnd so on:\n\n[\n    (result[\"evaluation\"], result[\"weights\"][\"feature_names\"]) for \n     result in forward_results \n     if result[\"number_features\"] == 3\n]\n\n[(0.7087035179138184,\n  ['nativeRank', 'nativeProximity', 'fieldMatch(body).proximity']),\n (0.7237873673439026,\n  ['nativeRank', 'nativeProximity', 'fieldMatch(body).queryCompleteness']),\n (0.7073785662651062,\n  ['nativeRank', 'nativeProximity', 'fieldMatch(body).significance']),\n (0.709153413772583, ['nativeRank', 'nativeProximity', 'nativeFieldMatch'])]"
  },
  {
    "objectID": "notebooks/cord19/cord19_download_parse_trec_covid.html",
    "href": "notebooks/cord19/cord19_download_parse_trec_covid.html",
    "title": "How to download and parse TREC-COVID data",
    "section": "",
    "text": "The files used in this section were originally found at https://ir.nist.gov/covidSubmit/data.html. We will download both the topics and the relevance judgements data. Do not worry about what they are just yet, we will explore them soon.\n\n!curl -fsSLO https://data.vespa.oath.cloud/blog/cord19/topics-rnd5.xml\n!curl -fsSLO https://data.vespa.oath.cloud/blog/cord19/qrels-covid_d5_j0.5-5.txt"
  },
  {
    "objectID": "notebooks/cord19/cord19_download_parse_trec_covid.html#download-the-data",
    "href": "notebooks/cord19/cord19_download_parse_trec_covid.html#download-the-data",
    "title": "How to download and parse TREC-COVID data",
    "section": "",
    "text": "The files used in this section were originally found at https://ir.nist.gov/covidSubmit/data.html. We will download both the topics and the relevance judgements data. Do not worry about what they are just yet, we will explore them soon.\n\n!curl -fsSLO https://data.vespa.oath.cloud/blog/cord19/topics-rnd5.xml\n!curl -fsSLO https://data.vespa.oath.cloud/blog/cord19/qrels-covid_d5_j0.5-5.txt"
  },
  {
    "objectID": "notebooks/cord19/cord19_download_parse_trec_covid.html#parse-the-data",
    "href": "notebooks/cord19/cord19_download_parse_trec_covid.html#parse-the-data",
    "title": "How to download and parse TREC-COVID data",
    "section": "Parse the data",
    "text": "Parse the data\n\nTopics\nThe topics file is in XML format. We can parse it and store in a dictionary called topics. We want to extract a query, a question and a narrative from each topic.\n\nimport xml.etree.ElementTree as ET\n\ntopics = {}\nroot = ET.parse(\"topics-rnd5.xml\").getroot()\nfor topic in root.findall(\"topic\"):\n    topic_number = topic.attrib[\"number\"]\n    topics[topic_number] = {}\n    for query in topic.findall(\"query\"):\n        topics[topic_number][\"query\"] = query.text\n    for question in topic.findall(\"question\"):\n        topics[topic_number][\"question\"] = question.text        \n    for narrative in topic.findall(\"narrative\"):\n        topics[topic_number][\"narrative\"] = narrative.text\n\nThere are a total of 50 topics. For example, we can see the first topic below:\n\ntopics[\"1\"]\n\n{'query': 'coronavirus origin',\n 'question': 'what is the origin of COVID-19',\n 'narrative': \"seeking range of information about the SARS-CoV-2 virus's origin, including its evolution, animal source, and first transmission into humans\"}\n\n\nEach topic has many relevance judgements associated with them.\n\n\nRelevance judgements\nWe can load the relevance judgement data directly into a pandas DataFrame.\n\nimport pandas as pd\n\nrelevance_data = pd.read_csv(\"qrels-covid_d5_j0.5-5.txt\", sep=\" \", header=None)\nrelevance_data.columns = [\"topic_id\", \"round_id\", \"cord_uid\", \"relevancy\"]\n\nThe relevance data contains all the relevance judgements made throughout the 5 rounds of the competition. relevancy equals to 0 is irrelevant, 1 is relevant and 2 is highly relevant.\n\nrelevance_data.head()\n\n\n\n\n\n\n\n\ntopic_id\nround_id\ncord_uid\nrelevancy\n\n\n\n\n0\n1\n4.5\n005b2j4b\n2\n\n\n1\n1\n4.0\n00fmeepz\n1\n\n\n2\n1\n0.5\n010vptx3\n2\n\n\n3\n1\n2.5\n0194oljo\n1\n\n\n4\n1\n4.0\n021q9884\n1\n\n\n\n\n\n\n\nWe are going to remove two rows that have relevancy equal to -1, which I am assuming is an error.\n\nrelevance_data[relevance_data.relevancy == -1]\n\n\n\n\n\n\n\n\ntopic_id\nround_id\ncord_uid\nrelevancy\n\n\n\n\n55873\n38\n5.0\n9hbib8b3\n-1\n\n\n69173\n50\n5.0\nucipq8uk\n-1\n\n\n\n\n\n\n\n\nrelevance_data = relevance_data[relevance_data.relevancy &gt;= 0]\n\nNext we will discuss how we can use this data to evaluate and improve cord19 search app."
  },
  {
    "objectID": "notebooks/evaluation.html",
    "href": "notebooks/evaluation.html",
    "title": "Evaluate application",
    "section": "",
    "text": "Connect to the application and define a query model.\n\nfrom vespa.application import Vespa\nfrom learntorank.query import QueryModel, Ranking, OR\n\napp = Vespa(url = \"https://api.cord19.vespa.ai\")\nquery_model = QueryModel(\n    match_phase = OR(),\n    ranking = Ranking(name=\"bm25\", list_features=True))"
  },
  {
    "objectID": "notebooks/evaluation.html#example-setup",
    "href": "notebooks/evaluation.html#example-setup",
    "title": "Evaluate application",
    "section": "",
    "text": "Connect to the application and define a query model.\n\nfrom vespa.application import Vespa\nfrom learntorank.query import QueryModel, Ranking, OR\n\napp = Vespa(url = \"https://api.cord19.vespa.ai\")\nquery_model = QueryModel(\n    match_phase = OR(),\n    ranking = Ranking(name=\"bm25\", list_features=True))"
  },
  {
    "objectID": "notebooks/evaluation.html#labeled-data",
    "href": "notebooks/evaluation.html#labeled-data",
    "title": "Evaluate application",
    "section": "Labeled data",
    "text": "Labeled data\nDefine some labeled data. pyvespa expects labeled data to follow the format illustrated below. It is a list of dict where each dict represents a query containing query_id, query and a list of relevant_docs. Each relevant document contain a required id key and an optional score key.\n\nlabeled_data = [\n    {\n        \"query_id\": 0, \n        \"query\": \"Intrauterine virus infections and congenital heart disease\",\n        \"relevant_docs\": [{\"id\": 0, \"score\": 1}, {\"id\": 3, \"score\": 1}]\n    },\n    {\n        \"query_id\": 1, \n        \"query\": \"Clinical and immunologic studies in identical twins discordant for systemic lupus erythematosus\",\n        \"relevant_docs\": [{\"id\": 1, \"score\": 1}, {\"id\": 5, \"score\": 1}]\n    }\n]"
  },
  {
    "objectID": "notebooks/evaluation.html#define-metrics",
    "href": "notebooks/evaluation.html#define-metrics",
    "title": "Evaluate application",
    "section": "Define metrics",
    "text": "Define metrics\n\nfrom learntorank.evaluation import MatchRatio, Recall, ReciprocalRank\n\neval_metrics = [MatchRatio(), Recall(at=10), ReciprocalRank(at=10)]"
  },
  {
    "objectID": "notebooks/evaluation.html#evaluate-in-batch",
    "href": "notebooks/evaluation.html#evaluate-in-batch",
    "title": "Evaluate application",
    "section": "Evaluate in batch",
    "text": "Evaluate in batch\n\nfrom learntorank.evaluation import evaluate\n\nevaluation = evaluate(\n    app=app,\n    labeled_data = labeled_data,\n    eval_metrics = eval_metrics, \n    query_model = query_model, \n    id_field = \"id\",\n)\nevaluation\n\n\n\n\n\n\n\n\nmodel\ndefault_name\n\n\n\n\nmatch_ratio\nmean\n0.853456\n\n\nmedian\n0.853456\n\n\nstd\n0.055199\n\n\nrecall_10\nmean\n0.000000\n\n\nmedian\n0.000000\n\n\nstd\n0.000000\n\n\nreciprocal_rank_10\nmean\n0.000000\n\n\nmedian\n0.000000\n\n\nstd\n0.000000"
  },
  {
    "objectID": "notebooks/evaluation.html#evaluate-specific-query",
    "href": "notebooks/evaluation.html#evaluate-specific-query",
    "title": "Evaluate application",
    "section": "Evaluate specific query",
    "text": "Evaluate specific query\n\nYou can have finer control with the evaluate_query method.\n\n\nfrom pandas import concat, DataFrame\nfrom learntorank.evaluation import evaluate_query\n\nevaluation = []\nfor query_data in labeled_data:\n    query_evaluation = evaluate_query(\n        app=app,\n        eval_metrics = eval_metrics, \n        query_model = query_model, \n        query_id = query_data[\"query_id\"], \n        query = query_data[\"query\"], \n        id_field = \"id\",\n        relevant_docs = query_data[\"relevant_docs\"],\n        default_score = 0\n    )\n    evaluation.append(query_evaluation)\nevaluation = DataFrame.from_records(evaluation)\nevaluation\n\n\n\n\n\n\n\n\nmodel\nquery_id\nmatch_ratio\nrecall_10\nreciprocal_rank_10\n\n\n\n\n0\ndefault_name\n0\n0.814425\n0.0\n0\n\n\n1\ndefault_name\n1\n0.892487\n0.0\n0"
  },
  {
    "objectID": "notebooks/evaluation.html#evaluate-query-under-specific-document-ids",
    "href": "notebooks/evaluation.html#evaluate-query-under-specific-document-ids",
    "title": "Evaluate application",
    "section": "Evaluate query under specific document ids",
    "text": "Evaluate query under specific document ids\n\nUse recall to specify which documents should be inlcuded in the evaluation\n\nIn the example below, we include documents with id equal to 0, 1 and 2. Since the relevant documents for this query are the documents with id 0 and 3, we should get recall equal to 0.5.\n\nquery_evaluation = evaluate_query(\n    app=app,\n    eval_metrics = eval_metrics, \n    query_model = query_model, \n    query_id = 0, \n    query = \"Intrauterine virus infections and congenital heart disease\", \n    id_field = \"id\",\n    relevant_docs = [{\"id\": 0, \"score\": 1}, {\"id\": 3, \"score\": 1}],\n    default_score = 0,\n    recall = (\"id\", [0, 1, 2])\n)\nquery_evaluation\n\n{'model': 'default_name',\n 'query_id': 0,\n 'match_ratio': 9.70242657688688e-06,\n 'recall_10': 0.5,\n 'reciprocal_rank_10': 1.0}\n\n\nWe now include documents with id equal to 0, 1, 2 and 3. This should give a recall equal to 1.\n\nquery_evaluation = evaluate_query(\n    app=app,\n    eval_metrics = eval_metrics, \n    query_model = query_model, \n    query_id = 0, \n    query = \"Intrauterine virus infections and congenital heart disease\", \n    id_field = \"id\",\n    relevant_docs = [{\"id\": 0, \"score\": 1}, {\"id\": 3, \"score\": 1}],\n    default_score = 0,\n    recall = (\"id\", [0, 1, 2, 3])\n)\nquery_evaluation\n\n{'model': 'default_name',\n 'query_id': 0,\n 'match_ratio': 1.2936568769182506e-05,\n 'recall_10': 1.0,\n 'reciprocal_rank_10': 1.0}"
  },
  {
    "objectID": "notebooks/query-model.html",
    "href": "notebooks/query-model.html",
    "title": "Query models",
    "section": "",
    "text": "from learntorank.query import QueryModel, Ranking, OR\n\nstandard_query_model = QueryModel(\n    name=\"or_bm25\",\n    match_phase = OR(),\n    ranking = Ranking(name=\"bm25\")\n)\nStarting in version 0.5.0 we can bypass the pyvespa high-level API and create a QueryModel with the full flexibility of the Vespa Query API. This is useful for use cases not covered by the pyvespa API and for users that are familiar with and prefer to work with the Vespa Query API.\ndef body_function(query):\n    body = {'yql': 'select * from sources * where userQuery();',\n            'query': query,\n            'type': 'any',\n            'ranking': {'profile': 'bm25', 'listFeatures': 'false'}}\n    return body\n\nflexible_query_model = QueryModel(body_function = body_function)\nThe flexible_query_model defined above is equivalent to the standard_query_model, as we can see when querying the app. We will use the cord19 app in our demonstration.\nfrom vespa.application import Vespa\n\napp = Vespa(url = \"https://api.cord19.vespa.ai\")\nfrom learntorank.query import send_query\n\nstandard_result = send_query(\n    app=app, \n    query=\"this is a test\", \n    query_model=standard_query_model\n)\nstandard_result.get_hits().head(3)\nflexible_result = send_query(\n    app=app, \n    query=\"this is a test\", \n    query_model=flexible_query_model\n)\nflexible_result.get_hits().head(3)"
  },
  {
    "objectID": "notebooks/query-model.html#specify-a-query-model",
    "href": "notebooks/query-model.html#specify-a-query-model",
    "title": "Query models",
    "section": "Specify a query model",
    "text": "Specify a query model\n\nQuery + term-matching + rank profile\n\nfrom learntorank.query import QueryModel, OR, Ranking, send_query\n\nresults = send_query(\n    app=app,\n    query=\"Is remdesivir an effective treatment for COVID-19?\", \n    query_model = QueryModel(\n        match_phase=OR(), \n        ranking=Ranking(name=\"bm25\")\n    )\n)\n\n\nresults.number_documents_retrieved\n\n\n\nQuery + term-matching + ann operator + rank_profile\n\nfrom learntorank.query import QueryModel, QueryRankingFeature, ANN, WeakAnd, Union, Ranking\nfrom random import random\n\nmatch_phase = Union(\n    WeakAnd(hits = 10), \n    ANN(\n        doc_vector=\"specter_embedding\", \n        query_vector=\"specter_vector\", \n        hits = 10,\n        label=\"title\"\n    )\n)\nranking = Ranking(name=\"related-specter\", list_features=True)\nquery_model = QueryModel(\n    query_properties=[QueryRankingFeature(\n        name=\"specter_vector\", \n        mapping=lambda x: [random() for x in range(768)]\n    )],\n    match_phase=match_phase, ranking=ranking\n)\n\n\nresults = send_query(\n    app=app,\n    query=\"Is remdesivir an effective treatment for COVID-19?\", \n    query_model=query_model\n)\n\n\nresults.number_documents_retrieved"
  },
  {
    "objectID": "notebooks/query-model.html#recall-specific-documents",
    "href": "notebooks/query-model.html#recall-specific-documents",
    "title": "Query models",
    "section": "Recall specific documents",
    "text": "Recall specific documents\nLet’s take a look at the top 3 ids from the last query.\n\ntop_ids = [hit[\"fields\"][\"id\"] for hit in results.hits[0:3]]\ntop_ids\n\nAssume that we now want to retrieve the second and third ids above. We can do so with the recall argument.\n\nresults_with_recall = send_query(\n    app=app,\n    query=\"Is remdesivir an effective treatment for COVID-19?\", \n    query_model=query_model,\n    recall = (\"id\", top_ids[1:3])\n)\n\nIt will only retrieve the documents with Vespa field id that is defined on the list that is inside the tuple.\n\nid_recalled = [hit[\"fields\"][\"id\"] for hit in results_with_recall.hits]\nid_recalled"
  },
  {
    "objectID": "notebooks/image_search/image-search-scratch.html",
    "href": "notebooks/image_search/image-search-scratch.html",
    "title": "Image search",
    "section": "",
    "text": "This notebook walks through the pyvespa code used to create the text to image search sample application."
  },
  {
    "objectID": "notebooks/image_search/image-search-scratch.html#create-the-application-package",
    "href": "notebooks/image_search/image-search-scratch.html#create-the-application-package",
    "title": "Image search",
    "section": "Create the application package",
    "text": "Create the application package\nCreate an application package:\n\nfrom vespa.package import ApplicationPackage\n\napp_package = ApplicationPackage(name=\"imagesearch\")\n\nAdd a field to hold the name of the image file. This is used in the sample app to load the final images that should be displayed to the end user.\nThe summary indexing ensures this field is returned as part of the query response. The attribute indexing store the fields in memory as an attribute for sorting, querying, and grouping:\n\nfrom vespa.package import Field\n\napp_package.schema.add_fields(\n    Field(name=\"image_file_name\", type=\"string\", indexing=[\"summary\", \"attribute\"]),\n)\n\nAdd a field to hold an image embedding. The embeddings are usually generated by a ML model. We can add multiple embedding fields to our application. This is useful when making experiments. For example, the sample app adds 6 image embeddings, one for each of the six pre-trained CLIP models available at the time.\nIn the example below, the embedding vector has size 512 and is of type float. The index is required to enable approximate matching and the HNSW instance configure the HNSW index:\n\nfrom vespa.package import HNSW\n\napp_package.schema.add_fields(\n    Field(\n        name=\"embedding_image\",\n        type=\"tensor&lt;float&gt;(x[512])\",\n        indexing=[\"attribute\", \"index\"],\n        ann=HNSW(\n            distance_metric=\"angular\",\n            max_links_per_node=16,\n            neighbors_to_explore_at_insert=500,\n        ),\n    )\n)\n\nAdd a rank profile that ranks the images by how close the image embedding vector is from the query embedding vector. The tensors used in queries must have their type declared in the application package, the code below declares the text embedding that will be sent in the query - it has the same size and type of the image embedding:\n\nfrom vespa.package import RankProfile\n\napp_package.schema.add_rank_profile(\n    RankProfile(\n        name=\"embedding_similarity\",\n        inherits=\"default\",\n        first_phase=\"closeness(embedding_image)\",\n        inputs=[(\"query(embedding_text)\", \"tensor&lt;float&gt;(x[512])\")],\n    )\n)"
  },
  {
    "objectID": "notebooks/image_search/image-search-scratch.html#deploy-the-application",
    "href": "notebooks/image_search/image-search-scratch.html#deploy-the-application",
    "title": "Image search",
    "section": "Deploy the application",
    "text": "Deploy the application\nThe application package created above can be deployed using Docker or Vespa Cloud. Follow the instructions based on the desired deployment mode. Either option will create a Vespa connection instance that can be stored in a variable that will be denoted here as app.\nWe can then use app to interact with the deployed application:\n\nimport os\nfrom vespa.deployment import VespaDocker\n\nvespa_docker = VespaDocker(\n    port=8080\n)\n\napp = vespa_docker.deploy(application_package = app_package)\n\nWaiting for configuration server, 0/300 seconds...\nWaiting for configuration server, 5/300 seconds...\nWaiting for application status, 0/300 seconds...\nWaiting for application status, 5/300 seconds...\nWaiting for application status, 10/300 seconds...\nWaiting for application status, 15/300 seconds...\nWaiting for application status, 20/300 seconds...\nWaiting for application status, 25/300 seconds...\nFinished deployment."
  },
  {
    "objectID": "notebooks/image_search/image-search-scratch.html#feed-the-image-data",
    "href": "notebooks/image_search/image-search-scratch.html#feed-the-image-data",
    "title": "Image search",
    "section": "Feed the image data",
    "text": "Feed the image data\nToDo: Add code below to create the feed and set batch - until then, disabled auto testing.\nTo feed the image data:\n\nresponses = app.feed_batch(batch)\n\nwhere batch is a list of dictionaries like the one below:\n\n{\n    \"id\": \"dog1\",\n    \"fields\": {\n        \"image_file_name\": \"dog1.jpg\",\n        \"embedding_image\": {\"values\": [0.884, -0.345, ..., 0.326]},\n    }\n}\n\nOne of the advantages of having a python API is that it can integrate with commonly used ML frameworks. The sample application show how to create a PyTorch DataLoader to generate batches of image data by using CLIP models to generate image embeddings."
  },
  {
    "objectID": "notebooks/image_search/image-search-scratch.html#query-the-application",
    "href": "notebooks/image_search/image-search-scratch.html#query-the-application",
    "title": "Image search",
    "section": "Query the application",
    "text": "Query the application\nThe following query will use approximate nearest neighbor search to match the closest images to the query text and rank the images according to their distance to the query text. The sample application used CLIP models to generate image and query embeddings.\n\nresponse = app.query(body={\n    \"yql\": 'select * from sources * where ({targetHits:100}nearestNeighbor(embedding_image,embedding_text));',\n    \"hits\": 100,\n    \"input.query(embedding_text)\": [0.632, -0.987, ..., 0.534],\n    \"ranking.profile\": \"embedding_similarity\"\n})"
  },
  {
    "objectID": "notebooks/image_search/image-search-scratch.html#evaluate-different-query-models",
    "href": "notebooks/image_search/image-search-scratch.html#evaluate-different-query-models",
    "title": "Image search",
    "section": "Evaluate different query models",
    "text": "Evaluate different query models\nDefine metrics to evaluate:\n\nfrom learntorank.evaluation import MatchRatio, Recall, ReciprocalRank\n\neval_metrics = [\n    MatchRatio(), \n    Recall(at=100), \n    ReciprocalRank(at=100)\n]\n\nThe sample application illustrates how to evaluate different CLIP models through the evaluate method:\n\nresult = app.evaluate(\n    labeled_data=labeled_data,  # Labeled data to define which images should be returned to a given query\n    eval_metrics=eval_metrics,  # Metrics used\n    query_model=query_models,   # Each query model uses a different CLIP model version\n    id_field=\"image_file_name\", # The name of the id field used by the labeled data to identify the image\n    per_query=True              # Return results per query rather the aggragated.\n)\n\nThe figure below is the reciprocal rank at 100 computed based on the output of the evaluate method.\n\n\n\nevaluation"
  },
  {
    "objectID": "passage_uncertainty_evaluation.html",
    "href": "passage_uncertainty_evaluation.html",
    "title": "IR evaluation metrics with uncertainty estimates",
    "section": "",
    "text": "When working with search engine apps, be it a text search or a recommendation system, part of the job is doing experiments around components such as ranking functions and deciding which experiments deliver the best result.\nThis tutorial builds a text search app with Vespa, feeds a sample of the passage ranking dataset to the app, and evaluates two ranking functions across three different metrics. In addition to return point estimates of the evaluation metrics, we compute confidence intervals as illustrated in the plot below. Measuring uncertainty around the metric estimates gives us a better sense of how significant is the impact of our changes in the application.\nThe code and the data used in this end-to-end tutorial are available and can be reproduced in a Jupyter Notebook."
  },
  {
    "objectID": "passage_uncertainty_evaluation.html#create-the-vespa-application-package",
    "href": "passage_uncertainty_evaluation.html#create-the-vespa-application-package",
    "title": "IR evaluation metrics with uncertainty estimates",
    "section": "Create the Vespa application package",
    "text": "Create the Vespa application package\nCreate a Vespa application package to perform passage ranking experiments using the create_basic_search_package.\n\nfrom learntorank.passage import create_basic_search_package\n\napp_package = create_basic_search_package()\n\nWe can inspect how the Vespa search definition file looks like:\n\nprint(app_package.schema.schema_to_text)\n\nschema PassageRanking {\n    document PassageRanking {\n        field doc_id type string {\n            indexing: attribute | summary\n        }\n        field text type string {\n            indexing: index | summary\n            index: enable-bm25\n        }\n    }\n    fieldset default {\n        fields: text\n    }\n    rank-profile bm25 {\n        first-phase {\n            expression: bm25(text)\n        }\n        summary-features {\n            bm25(text)\n        }\n    }\n    rank-profile native_rank {\n        first-phase {\n            expression: nativeRank(text)\n        }\n    }\n}\n\n\nIn this tutorial, we are going to compare two ranking functions. One is based on NativeRank, and the other is based on BM25."
  },
  {
    "objectID": "passage_uncertainty_evaluation.html#deploy-the-application",
    "href": "passage_uncertainty_evaluation.html#deploy-the-application",
    "title": "IR evaluation metrics with uncertainty estimates",
    "section": "Deploy the application",
    "text": "Deploy the application\nDeploy the application package in a Docker container for local development. Alternatively, it is possible to deploy the application package to Vespa Cloud.\n\nfrom vespa.deployment import VespaDocker\n\nvespa_docker = VespaDocker()\napp = vespa_docker.deploy(application_package=app_package)\n\nWaiting for configuration server, 0/300 seconds...\nWaiting for configuration server, 5/300 seconds...\nWaiting for application status, 0/300 seconds...\nWaiting for application status, 5/300 seconds...\nWaiting for application status, 10/300 seconds...\nWaiting for application status, 15/300 seconds...\nWaiting for application status, 20/300 seconds...\nFinished deployment.\n\n\nOnce the deployment is finished, we can interact with the deployed application through the app variable."
  },
  {
    "objectID": "passage_uncertainty_evaluation.html#get-sample-data",
    "href": "passage_uncertainty_evaluation.html#get-sample-data",
    "title": "IR evaluation metrics with uncertainty estimates",
    "section": "Get sample data",
    "text": "Get sample data\nWe can load passage ranking sample data with PassageData.load. By default, it will download pre-generated sample data.\n\nfrom learntorank.passage import PassageData\n\ndata = PassageData.load()\n\n\ndata\n\nPassageData(corpus, train_qrels, train_queries, dev_qrels, dev_queries)\n\n\n\ndata.summary\n\nNumber of documents: 1000\nNumber of train queries: 100\nNumber of train relevance judgments: 100\nNumber of dev queries: 100\nNumber of dev relevance judgments: 100"
  },
  {
    "objectID": "passage_uncertainty_evaluation.html#feed-the-application",
    "href": "passage_uncertainty_evaluation.html#feed-the-application",
    "title": "IR evaluation metrics with uncertainty estimates",
    "section": "Feed the application",
    "text": "Feed the application\nGet the document corpus in a DataFrame format.\n\ncorpus_df = data.get_corpus()\ncorpus_df.head()\n\n\n\n\n\n\n\n\ndoc_id\ntext\n\n\n\n\n0\n5954248\nWhy GameStop is excited for Dragon Age: Inquis...\n\n\n1\n7290700\nmetaplasia definition: 1. abnormal change of o...\n\n\n2\n5465518\nCandice Net Worth. According to the report of ...\n\n\n3\n3100518\nUnder the Base Closure Act, March AFB was down...\n\n\n4\n3207764\nThere are a number of career opportunities for...\n\n\n\n\n\n\n\nFeed the data to the deployed application.\n\nresponses = app.feed_df(df=corpus_df, include_id=True, id_field=\"doc_id\")\n\nSuccessful documents fed: 1000/1000.\nBatch progress: 1/1.\n\n\nWe can also check the number of successfully fed documents through the responses status code:\n\nsum([response.status_code == 200 for response in responses])\n\n1000"
  },
  {
    "objectID": "passage_uncertainty_evaluation.html#query-the-application",
    "href": "passage_uncertainty_evaluation.html#query-the-application",
    "title": "IR evaluation metrics with uncertainty estimates",
    "section": "Query the application",
    "text": "Query the application\nGet the dev set queries in a DataFrame format.\n\ndev_queries_df = data.get_queries(type=\"dev\")\ndev_queries_df.head()\n\n\n\n\n\n\n\n\nquery_id\nquery\n\n\n\n\n0\n1101971\nwhy say the sky is the limit\n\n\n1\n712898\nwhat is an cvc in radiology\n\n\n2\n154469\ndmv california how long does it take to get id\n\n\n3\n930015\nwhat's an epigraph\n\n\n4\n860085\nwhat is va tax\n\n\n\n\n\n\n\nGet the first query text to use as an example when querying our passage search application.\n\nsample_query = dev_queries_df.loc[0, \"query\"]\nsample_query\n\n'why say the sky is the limit'\n\n\n\nQuery with QueryModel\nCreate the bm25 QueryModel, which uses Vespa’s weakAnd operator to match documents relevant to the query and use the bm25 rank-profile that we defined in the application package above to rank the documents.\n\nfrom learntorank.query import QueryModel, WeakAnd, Ranking\n\nbm25_query_model = QueryModel(\n    name=\"bm25\", \n    match_phase=WeakAnd(hits=100), \n    ranking=Ranking(name=\"bm25\")\n)\n\nOnce a QueryModel is specified, we can use it to query our application.\n\nfrom learntorank.query import send_query\nfrom pprint import pprint\n\nresponse = send_query(\n    app=app,\n    query=sample_query, \n    query_model=bm25_query_model\n)\npprint(response.hits[0:2])\n\n[{'fields': {'doc_id': '7407715',\n             'documentid': 'id:PassageRanking:PassageRanking::7407715',\n             'sddocname': 'PassageRanking',\n             'summaryfeatures': {'bm25(text)': 11.979235042476953,\n                                 'vespa.summaryFeatures.cached': 0.0},\n             'text': 'The Sky is the Limit also known as TSITL is a global '\n                     'effort designed to influence, motivate and inspire '\n                     'people all over the world to achieve their goals and '\n                     'dreams in life. TSITL’s collaborative community on '\n                     'social media provides you with a vast archive of '\n                     'motivational pictures/quotes/videos.'},\n  'id': 'id:PassageRanking:PassageRanking::7407715',\n  'relevance': 11.979235042476953,\n  'source': 'PassageRanking_content'},\n {'fields': {'doc_id': '84721',\n             'documentid': 'id:PassageRanking:PassageRanking::84721',\n             'sddocname': 'PassageRanking',\n             'summaryfeatures': {'bm25(text)': 11.310323797415357,\n                                 'vespa.summaryFeatures.cached': 0.0},\n             'text': 'Sky Customer Service 0870 280 2564. Use the Sky contact '\n                     'number to get in contact with the Sky customer services '\n                     'team to speak to a representative about your Sky TV, Sky '\n                     'Internet or Sky telephone services. The Sky customer '\n                     'Services team is operational between 8:30am and 11:30pm '\n                     'seven days a week.'},\n  'id': 'id:PassageRanking:PassageRanking::84721',\n  'relevance': 11.310323797415357,\n  'source': 'PassageRanking_content'}]\n\n\n\n\nQuery with Vespa Query Language\nWe can also translate the query created with the QueryModel into the Vespa Query Language (YQL) by setting debug_request=True:\n\nresponse = send_query(\n    app=app,\n    query = sample_query, \n    query_model=bm25_query_model, \n    debug_request=True\n)\nyql_body = response.request_body\npprint(yql_body)\n\n{'ranking': {'listFeatures': 'false', 'profile': 'bm25'},\n 'yql': 'select * from sources * where ({targetHits: 100}weakAnd(default '\n        'contains \"why\", default contains \"say\", default contains \"the\", '\n        'default contains \"sky\", default contains \"is\", default contains '\n        '\"the\", default contains \"limit\"));'}\n\n\nWe can use Vespa YQL directly via the body parameter:\n\nyql_response = send_query(app=app, body=yql_body)\npprint(yql_response.hits[0:2])\n\n[{'fields': {'doc_id': '7407715',\n             'documentid': 'id:PassageRanking:PassageRanking::7407715',\n             'sddocname': 'PassageRanking',\n             'summaryfeatures': {'bm25(text)': 11.979235042476953,\n                                 'vespa.summaryFeatures.cached': 0.0},\n             'text': 'The Sky is the Limit also known as TSITL is a global '\n                     'effort designed to influence, motivate and inspire '\n                     'people all over the world to achieve their goals and '\n                     'dreams in life. TSITL’s collaborative community on '\n                     'social media provides you with a vast archive of '\n                     'motivational pictures/quotes/videos.'},\n  'id': 'id:PassageRanking:PassageRanking::7407715',\n  'relevance': 11.979235042476953,\n  'source': 'PassageRanking_content'},\n {'fields': {'doc_id': '84721',\n             'documentid': 'id:PassageRanking:PassageRanking::84721',\n             'sddocname': 'PassageRanking',\n             'summaryfeatures': {'bm25(text)': 11.310323797415357,\n                                 'vespa.summaryFeatures.cached': 0.0},\n             'text': 'Sky Customer Service 0870 280 2564. Use the Sky contact '\n                     'number to get in contact with the Sky customer services '\n                     'team to speak to a representative about your Sky TV, Sky '\n                     'Internet or Sky telephone services. The Sky customer '\n                     'Services team is operational between 8:30am and 11:30pm '\n                     'seven days a week.'},\n  'id': 'id:PassageRanking:PassageRanking::84721',\n  'relevance': 11.310323797415357,\n  'source': 'PassageRanking_content'}]"
  },
  {
    "objectID": "passage_uncertainty_evaluation.html#evaluate-query-models",
    "href": "passage_uncertainty_evaluation.html#evaluate-query-models",
    "title": "IR evaluation metrics with uncertainty estimates",
    "section": "Evaluate query models",
    "text": "Evaluate query models\nIn this section, we want to evaluate and compare the bm25_query_model defined above with the native_query_model defined below:\n\nnative_query_model = QueryModel(\n    name=\"native_rank\", \n    match_phase=WeakAnd(hits=100), \n    ranking=Ranking(name=\"native_rank\")\n)\n\nWe specify three metrics to evaluate the models.\n\nfrom learntorank.evaluation import (\n    Recall, \n    ReciprocalRank, \n    NormalizedDiscountedCumulativeGain\n)\n\nmetrics = [\n    Recall(at=10), \n    ReciprocalRank(at=3), \n    NormalizedDiscountedCumulativeGain(at=3)\n]\n\n\nPoint estimates\nIt is straightforward to obtain point estimates of the evaluation metrics for each query model being compared. In this case, we computed the mean and the standard deviation for each of the metrics.\n\nfrom learntorank.evaluation import evaluate\n\nevaluation = evaluate(\n    app=app,\n    labeled_data=data.get_labels(type=\"dev\"), \n    eval_metrics=metrics, \n    query_model=[native_query_model, bm25_query_model], \n    id_field=\"doc_id\",\n    aggregators=[\"mean\", \"std\"]\n )\n\n\nevaluation\n\n\n\n\n\n\n\n\nmodel\nbm25\nnative_rank\n\n\n\n\nrecall_10\nmean\n0.935833\n0.845833\n\n\nstd\n0.215444\n0.342749\n\n\nreciprocal_rank_3\nmean\n0.935000\n0.755000\n\n\nstd\n0.231977\n0.394587\n\n\nndcg_3\nmean\n0.912839\n0.749504\n\n\nstd\n0.242272\n0.381792\n\n\n\n\n\n\n\nGiven the nature of the data distribution of the metrics described above, it is not trivial to compute a confidence interval from the mean and the standard deviation computed above. In the next section, we solve this by using bootstrap sampling on a per query metric evaluation.\n\n\nUncertainty estimates\nInstead of returning aggregated point estimates, we can also compute the metrics per query by setting per_query=True. This gives us more granular information on the distribution function of the metrics.\n\nevaluation_per_query = evaluate(\n    app=app,\n    labeled_data=data.get_labels(type=\"dev\"), \n    eval_metrics=metrics, \n    query_model=[native_query_model, bm25_query_model], \n    id_field=\"doc_id\",\n    per_query=True\n)\n\n\nevaluation_per_query.head()\n\n\n\n\n\n\n\n\nmodel\nquery_id\nrecall_10\nreciprocal_rank_3\nndcg_3\n\n\n\n\n0\nnative_rank\n1101971\n1.0\n1.0\n1.0\n\n\n1\nnative_rank\n712898\n0.0\n0.0\n0.0\n\n\n2\nnative_rank\n154469\n1.0\n0.0\n0.0\n\n\n3\nnative_rank\n930015\n1.0\n1.0\n1.0\n\n\n4\nnative_rank\n860085\n0.0\n0.0\n0.0\n\n\n\n\n\n\n\nWe then created a function that uses the evaluation per query data and computes uncertainty estimates via bootstrap sampling.\n\nfrom learntorank.stats import compute_evaluation_estimates\n\nestimates = compute_evaluation_estimates(\n    df = evaluation_per_query\n)\n\n\nestimates\n\n\n\n\n\n\n\n\nmetric\nmodel\nlow\nmedian\nhigh\n\n\n\n\n0\nndcg_3\nbm25\n0.865279\n0.914863\n0.958766\n\n\n1\nndcg_3\nnative_rank\n0.675220\n0.752400\n0.820318\n\n\n2\nrecall_10\nbm25\n0.891667\n0.935833\n0.974187\n\n\n3\nrecall_10\nnative_rank\n0.778312\n0.847917\n0.910000\n\n\n4\nreciprocal_rank_3\nbm25\n0.890000\n0.935000\n0.975000\n\n\n5\nreciprocal_rank_3\nnative_rank\n0.678333\n0.758333\n0.831667\n\n\n\n\n\n\n\nWe can then create plots based on this data to make it easier to judge the magnitude of the differences between ranking functions.\n\nfrom plotnine import *\n\nprint((ggplot(estimates) + \n geom_point(aes(\"model\", \"median\")) + \n geom_errorbar(aes(x=\"model\", ymin=\"low\",ymax=\"high\")) + \n facet_wrap(\"metric\") + labs(y=\"Metric value\")\n))"
  },
  {
    "objectID": "passage_uncertainty_evaluation.html#cleanup-the-environment",
    "href": "passage_uncertainty_evaluation.html#cleanup-the-environment",
    "title": "IR evaluation metrics with uncertainty estimates",
    "section": "Cleanup the environment",
    "text": "Cleanup the environment\n\nvespa_docker.container.stop(timeout=600)\nvespa_docker.container.remove()"
  },
  {
    "objectID": "module_query.html",
    "href": "module_query.html",
    "title": "query",
    "section": "",
    "text": "source\n\n\n\n MatchFilter ()\n\nAbstract class for match filters.\n\nsource\n\n\n\n\n AND ()\n\nFilter that match document containing all the query terms.\nUsage: The AND filter is usually used when specifying query models.\n\nand_filter = AND()\n\n\nsource\n\n\n\n\n OR ()\n\nFilter that match any document containing at least one query term.\nUsage: The OR filter is usually used when specifying query models.\n\nor_filter = OR()\n\n\nsource\n\n\n\n\n WeakAnd (hits:int, field:str='default')\n\nMatch documents according to the weakAND algorithm.\nReference: https://docs.vespa.ai/en/using-wand-with-vespa.html\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nhits\nint\n\nLower bound on the number of hits to be retrieved.\n\n\nfield\nstr\ndefault\nWhich Vespa field to search.\n\n\nReturns\nNone\n\n\n\n\n\nUsage: The WeakAnd filter is usually used when specifying query models.\n\nweakand_filter = WeakAnd(hits=10, field=\"default\")\n\n\nsource\n\n\n\n\n Tokenize (hits:int, field:str='default')\n\nMatch documents according to the weakAND algorithm without parsing specials characters.\nReference: https://docs.vespa.ai/en/reference/simple-query-language-reference.html\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nhits\nint\n\nLower bound on the number of hits to be retrieved.\n\n\nfield\nstr\ndefault\nWhich Vespa field to search.\n\n\nReturns\nNone\n\n\n\n\n\nUsage: The Tokenize filter is usually used when specifying query models.\n\ntokenize_filter = Tokenize(hits=10, field=\"default\")\n\n\nsource\n\n\n\n\n ANN (doc_vector:str, query_vector:str, hits:int, label:str,\n      approximate:bool=True)\n\nMatch documents according to the nearest neighbor operator.\nReference: https://docs.vespa.ai/en/reference/query-language-reference.html\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndoc_vector\nstr\n\nName of the document field to be used in the distance calculation.\n\n\nquery_vector\nstr\n\nName of the query field to be used in the distance calculation.\n\n\nhits\nint\n\nLower bound on the number of hits to return.\n\n\nlabel\nstr\n\nA label to identify this specific operator instance.\n\n\napproximate\nbool\nTrue\nTrue to use approximate nearest neighbor and False to use brute force. Default to True.\n\n\nReturns\nNone\n\n\n\n\n\nUsage: The ANN filter is usually used when specifying query models.\nBy default, the ANN operator uses approximate nearest neighbor:\n\nmatch_filter = ANN(\n    doc_vector=\"doc_vector\",\n    query_vector=\"query_vector\",\n    hits=10,\n    label=\"label\",\n)\n\nBrute-force can be used by specifying approximate=False:\n\nann_filter = ANN(\n    doc_vector=\"doc_vector\",\n    query_vector=\"query_vector\",\n    hits=10,\n    label=\"label\",\n    approximate=False,\n)\n\n\nsource\n\n\n\n\n Union (*args:__main__.MatchFilter)\n\nMatch documents that belongs to the union of many match filters.\n\n\n\n\nType\nDetails\n\n\n\n\nargs\nMatchFilter\n\n\n\nReturns\nNone\nMatch filters to be taken the union of.\n\n\n\nUsage: The Union filter is usually used when specifying query models.\n\nunion_filter = Union(\n    WeakAnd(hits=10, field=\"field_name\"),\n    ANN(\n        doc_vector=\"doc_vector\",\n        query_vector=\"query_vector\",\n        hits=10,\n        label=\"label\",\n    ),\n)"
  },
  {
    "objectID": "module_query.html#match-filters",
    "href": "module_query.html#match-filters",
    "title": "query",
    "section": "",
    "text": "source\n\n\n\n MatchFilter ()\n\nAbstract class for match filters.\n\nsource\n\n\n\n\n AND ()\n\nFilter that match document containing all the query terms.\nUsage: The AND filter is usually used when specifying query models.\n\nand_filter = AND()\n\n\nsource\n\n\n\n\n OR ()\n\nFilter that match any document containing at least one query term.\nUsage: The OR filter is usually used when specifying query models.\n\nor_filter = OR()\n\n\nsource\n\n\n\n\n WeakAnd (hits:int, field:str='default')\n\nMatch documents according to the weakAND algorithm.\nReference: https://docs.vespa.ai/en/using-wand-with-vespa.html\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nhits\nint\n\nLower bound on the number of hits to be retrieved.\n\n\nfield\nstr\ndefault\nWhich Vespa field to search.\n\n\nReturns\nNone\n\n\n\n\n\nUsage: The WeakAnd filter is usually used when specifying query models.\n\nweakand_filter = WeakAnd(hits=10, field=\"default\")\n\n\nsource\n\n\n\n\n Tokenize (hits:int, field:str='default')\n\nMatch documents according to the weakAND algorithm without parsing specials characters.\nReference: https://docs.vespa.ai/en/reference/simple-query-language-reference.html\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nhits\nint\n\nLower bound on the number of hits to be retrieved.\n\n\nfield\nstr\ndefault\nWhich Vespa field to search.\n\n\nReturns\nNone\n\n\n\n\n\nUsage: The Tokenize filter is usually used when specifying query models.\n\ntokenize_filter = Tokenize(hits=10, field=\"default\")\n\n\nsource\n\n\n\n\n ANN (doc_vector:str, query_vector:str, hits:int, label:str,\n      approximate:bool=True)\n\nMatch documents according to the nearest neighbor operator.\nReference: https://docs.vespa.ai/en/reference/query-language-reference.html\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndoc_vector\nstr\n\nName of the document field to be used in the distance calculation.\n\n\nquery_vector\nstr\n\nName of the query field to be used in the distance calculation.\n\n\nhits\nint\n\nLower bound on the number of hits to return.\n\n\nlabel\nstr\n\nA label to identify this specific operator instance.\n\n\napproximate\nbool\nTrue\nTrue to use approximate nearest neighbor and False to use brute force. Default to True.\n\n\nReturns\nNone\n\n\n\n\n\nUsage: The ANN filter is usually used when specifying query models.\nBy default, the ANN operator uses approximate nearest neighbor:\n\nmatch_filter = ANN(\n    doc_vector=\"doc_vector\",\n    query_vector=\"query_vector\",\n    hits=10,\n    label=\"label\",\n)\n\nBrute-force can be used by specifying approximate=False:\n\nann_filter = ANN(\n    doc_vector=\"doc_vector\",\n    query_vector=\"query_vector\",\n    hits=10,\n    label=\"label\",\n    approximate=False,\n)\n\n\nsource\n\n\n\n\n Union (*args:__main__.MatchFilter)\n\nMatch documents that belongs to the union of many match filters.\n\n\n\n\nType\nDetails\n\n\n\n\nargs\nMatchFilter\n\n\n\nReturns\nNone\nMatch filters to be taken the union of.\n\n\n\nUsage: The Union filter is usually used when specifying query models.\n\nunion_filter = Union(\n    WeakAnd(hits=10, field=\"field_name\"),\n    ANN(\n        doc_vector=\"doc_vector\",\n        query_vector=\"query_vector\",\n        hits=10,\n        label=\"label\",\n    ),\n)"
  },
  {
    "objectID": "module_query.html#ranking",
    "href": "module_query.html#ranking",
    "title": "query",
    "section": "Ranking",
    "text": "Ranking\n\nsource\n\nRanking\n\n Ranking (name:str='default', list_features:bool=False)\n\nDefine the rank profile to be used during ranking.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nname\nstr\ndefault\nName of the rank profile as defined in a Vespa search definition.\n\n\nlist_features\nbool\nFalse\nShould the ranking features be returned. Either ‘true’ or ‘false’.\n\n\nReturns\nNone\n\n\n\n\n\nUsage: Ranking is usually used when specifying query models.\n\nranking = Ranking(name=\"bm25\", list_features=True)"
  },
  {
    "objectID": "module_query.html#query-properties",
    "href": "module_query.html#query-properties",
    "title": "query",
    "section": "Query properties",
    "text": "Query properties\n\nsource\n\nQueryProperty\n\n QueryProperty ()\n\nAbstract class for query property.\n\nsource\n\n\nQueryRankingFeature\n\n QueryRankingFeature (name:str, mapping:Callable[[str],List[float]])\n\nInclude ranking.feature.query into a Vespa query.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nname\nstr\nName of the feature.\n\n\nmapping\ntyping.Callable[[str], typing.List[float]]\nFunction mapping a string to a list of floats.\n\n\nReturns\nNone\n\n\n\n\nUsage: QueryRankingFeature is usually used when specifying query models.\n\nquery_property = QueryRankingFeature(\n    name=\"query_vector\", mapping=lambda x: [1, 2, 3]\n)"
  },
  {
    "objectID": "module_query.html#query-model",
    "href": "module_query.html#query-model",
    "title": "query",
    "section": "Query model",
    "text": "Query model\n\nsource\n\nQueryModel\n\n QueryModel (name:str='default_name',\n             query_properties:Optional[List[__main__.QueryProperty]]=None,\n             match_phase:__main__.MatchFilter=&lt;__main__.AND object at\n             0x7fe734343a30&gt;, ranking:__main__.Ranking=&lt;__main__.Ranking\n             object at 0x7fe73305ba60&gt;,\n             body_function:Optional[Callable[[str],Dict]]=None)\n\nDefine a query model.\nA QueryModel is an abstraction that encapsulates all the relevant information controlling how a Vespa app matches and ranks documents.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nname\nstr\ndefault_name\nName of the query model. Used to tag model-related quantities, like evaluation metrics.\n\n\nquery_properties\ntyping.Optional[typing.List[main.QueryProperty]]\nNone\nQuery properties to be included in the queries.\n\n\nmatch_phase\nMatchFilter\n&lt;main.AND object at 0x7fe734343a30&gt;\nDefine the match criteria.\n\n\nranking\nRanking\n&lt;main.Ranking object at 0x7fe73305ba60&gt;\nDefine the rank criteria.\n\n\nbody_function\ntyping.Optional[typing.Callable[[str], typing.Dict]]\nNone\nFunction that take query as parameter and returns the body of a Vespa query.\n\n\nReturns\nNone\n\n\n\n\n\nUsage:\nSpecify a query model with default configurations:\n\nquery_model = QueryModel()\n\nSpecify match phase, ranking phase and properties used by them.\n\nquery_model = QueryModel(\n    query_properties=[\n        QueryRankingFeature(name=\"query_embedding\", mapping=lambda x: [1, 2, 3])\n    ],\n    match_phase=ANN(\n        doc_vector=\"document_embedding\",\n        query_vector=\"query_embedding\",\n        hits=10,\n        label=\"label\",\n    ),\n    ranking=Ranking(name=\"bm25_plus_embeddings\", list_features=True),\n)\n\nSpecify a query model based on a function that output Vespa YQL.\n\ndef body_function(query):\n    body = {\n        \"yql\": \"select * from sources * where userQuery();\",\n        \"query\": query,\n        \"type\": \"any\",\n        \"ranking\": {\"profile\": \"bm25\", \"listFeatures\": \"true\"},\n    }\n    return body\n\nquery_model = QueryModel(body_function=body_function)"
  },
  {
    "objectID": "module_query.html#send-query-with-querymodel",
    "href": "module_query.html#send-query-with-querymodel",
    "title": "query",
    "section": "Send query with QueryModel",
    "text": "Send query with QueryModel\n\nsource\n\nsend_query\n\n send_query (app:vespa.application.Vespa, body:Optional[Dict]=None,\n             query:Optional[str]=None,\n             query_model:Optional[__main__.QueryModel]=None,\n             debug_request:bool=False, recall:Optional[Tuple]=None,\n             **kwargs)\n\nSend a query request to a Vespa application.\nEither send ‘body’ containing all the request parameters or specify ‘query’ and ‘query_model’.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\napp\nVespa\n\nConnection to a Vespa application\n\n\nbody\ntyping.Optional[typing.Dict]\nNone\nContains all the request parameters. None when using query_model.\n\n\nquery\ntyping.Optional[str]\nNone\nQuery string. None when using body.\n\n\nquery_model\ntyping.Optional[main.QueryModel]\nNone\nQuery model. None when using body.\n\n\ndebug_request\nbool\nFalse\nReturn request body for debugging instead of sending the request.\n\n\nrecall\ntyping.Optional[typing.Tuple]\nNone\nTuple of size 2 where the first element is the name of the field to use to recall and the second element is a list of the values to be recalled.\n\n\nkwargs\n\n\n\n\n\nReturns\nVespaQueryResponse\n\nEither the request body if debug_request is True or the result from the Vespa application.\n\n\n\nUsage: Assume app is a Vespa connection.\nSend request body.\n\nbody = {\"yql\": \"select * from sources * where test\"}\nresult = send_query(app=app, body=body)\n\nUse query and query_model:\n\nresult = send_query(\n    app=app,\n    query=\"this is a test\",\n    query_model=QueryModel(\n        match_phase=OR(), \n        ranking=Ranking()\n    ),\n    hits=10,\n)\n\nDebug the output of the QueryModel by setting debug_request=True:\n\nsend_query(\n    app=app,\n    query=\"this is a test\",\n    query_model=QueryModel(match_phase=OR(), ranking=Ranking()),\n    debug_request=True,\n    hits=10,\n).request_body\n\n{'yql': 'select * from sources * where ({grammar: \"any\"}userInput(\"this is a test\"));',\n 'ranking': {'profile': 'default', 'listFeatures': 'false'},\n 'hits': 10}\n\n\nRecall documents using the id field:\n\nresult = send_query(\n    app=app,\n    query=\"this is a test\",\n    query_model=QueryModel(match_phase=OR(), ranking=Ranking()),\n    hits=10,\n    recall=(\"id\", [1, 5]),\n)\n\nUse a body_function to specify a QueryModel:\n\ndef body_function(query):\n    body = {\n        \"yql\": \"select * from sources * where userQuery();\",\n        \"query\": query,\n        \"type\": \"any\",\n        \"ranking\": {\"profile\": \"bm25\", \"listFeatures\": \"true\"},\n    }\n    return body\n\nquery_model = QueryModel(body_function=body_function)\n\nresult = send_query(\n        app=app,\n        query=\"this is a test\",\n        query_model=query_model,\n        hits=10\n)\n\n\nsource\n\n\nsend_query_batch\n\n send_query_batch (app, body_batch:Optional[List[Dict]]=None,\n                   query_batch:Optional[List[str]]=None,\n                   query_model:Optional[__main__.QueryModel]=None,\n                   recall_batch:Optional[List[Tuple]]=None,\n                   asynchronous=True, connections:Optional[int]=100,\n                   total_timeout:int=100, **kwargs)\n\nSend queries in batch to a Vespa app.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\napp\n\n\nConnection to a Vespa application\n\n\nbody_batch\ntyping.Optional[typing.List[typing.Dict]]\nNone\nContains all the request parameters. Set to None if using ‘query_batch’.\n\n\nquery_batch\ntyping.Optional[typing.List[str]]\nNone\nQuery strings. Set to None if using ‘body_batch’.\n\n\nquery_model\ntyping.Optional[main.QueryModel]\nNone\nQuery model to use when sending query strings. Set to None if using ‘body_batch’.\n\n\nrecall_batch\ntyping.Optional[typing.List[typing.Tuple]]\nNone\nOne tuple for each query. Tuple of size 2 where the first element is the name of the field to use to recall and the second element is a list of the values to be recalled.\n\n\nasynchronous\nbool\nTrue\nSet True to send data in async mode. Default to True.\n\n\nconnections\ntyping.Optional[int]\n100\nNumber of allowed concurrent connections, valid only if asynchronous=True.\n\n\ntotal_timeout\nint\n100\nTotal timeout in secs for each of the concurrent requests when using asynchronous=True.\n\n\nkwargs\n\n\n\n\n\nReturns\ntyping.List[vespa.io.VespaQueryResponse]\n\nHTTP POST responses.\n\n\n\nUse body_batch to send a batch of body requests.\n\nbody_batch = [\n    {\"yql\": \"select * from sources * where test\"},\n    {\"yql\": \"select * from sources * where test2\"}\n]\nresult = send_query_batch(app=app, body_batch=body_batch)\n\nUse query_batch to send a batch of query strings to be ranked according a QueryModel.\n\nresult = send_query_batch(\n    app=app,\n    query_batch=[\"this is a test\", \"this is a test 2\"],\n    query_model=QueryModel(\n        match_phase=OR(), \n        ranking=Ranking()\n    ),\n    hits=10,\n)\n\nUse recall_batch to send one tuple for each query in query_batch.\n\nresult = send_query_batch(\n    app=app,\n    query_batch=[\"this is a test\", \"this is a test 2\"],\n    query_model=QueryModel(match_phase=OR(), ranking=Ranking()),\n    hits=10,\n    recall_batch=[(\"doc_id\", [2, 7]), (\"doc_id\", [0, 5])],\n)"
  },
  {
    "objectID": "module_query.html#collect-vespa-features",
    "href": "module_query.html#collect-vespa-features",
    "title": "query",
    "section": "Collect Vespa features",
    "text": "Collect Vespa features\n\nsource\n\ncollect_vespa_features\n\n collect_vespa_features (app:vespa.application.Vespa, labeled_data,\n                         id_field:str, query_model:__main__.QueryModel,\n                         number_additional_docs:int, fields:List[str],\n                         keep_features:Optional[List[str]]=None,\n                         relevant_score:int=1, default_score:int=0,\n                         **kwargs)\n\nCollect Vespa features based on a set of labelled data.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\napp\nVespa\n\nConnection to a Vespa application.\n\n\nlabeled_data\n\n\nLabelled data containing query, query_id and relevant ids. See examples about data format.\n\n\nid_field\nstr\n\nThe Vespa field representing the document id.\n\n\nquery_model\nQueryModel\n\nQuery model.\n\n\nnumber_additional_docs\nint\n\nNumber of additional documents to retrieve for each relevant document. Duplicate documents will be dropped.\n\n\nfields\ntyping.List[str]\n\nVespa fields to collect, e.g. [“rankfeatures”, “summaryfeatures”]\n\n\nkeep_features\ntyping.Optional[typing.List[str]]\nNone\nList containing the names of the features that should be returned. Default to None, which return all the features contained in the ‘fields’ argument.\n\n\nrelevant_score\nint\n1\nScore to assign to relevant documents. Default to 1.\n\n\ndefault_score\nint\n0\nScore to assign to the additional documents that are not relevant. Default to 0.\n\n\nkwargs\n\n\n\n\n\nReturns\nDataFrame\n\nDataFrame containing document id (document_id), query id (query_id), scores (relevant) and vespa rank features returned by the Query model RankProfile used.\n\n\n\nUsage:\nDefine labeled_data as a list of dict containing relevant documents:\n\nlabeled_data = [\n    {\n        \"query_id\": 0,\n        \"query\": \"give me title 1\",\n        \"relevant_docs\": [{\"id\": \"1\", \"score\": 1}],\n    },\n    {\n        \"query_id\": 1,\n        \"query\": \"give me title 3\",\n        \"relevant_docs\": [{\"id\": \"3\", \"score\": 1}],\n    },\n]\n\nCollect vespa features:\n\nrank_features = collect_vespa_features(\n    app=app,\n    labeled_data=labeled_data,\n    id_field=\"doc_id\",\n    query_model=QueryModel(\n        match_phase=OR(), \n        ranking=Ranking(name=\"bm25\", list_features=True)\n    ),\n    number_additional_docs=2,\n    fields=[\"rankfeatures\"],\n)\nrank_features\n\n\n\n\n\n\n\n\ndocument_id\nquery_id\nlabel\nattributeMatch(doc_id)\nattributeMatch(doc_id).averageWeight\nattributeMatch(doc_id).completeness\nattributeMatch(doc_id).fieldCompleteness\nattributeMatch(doc_id).importance\nattributeMatch(doc_id).matches\nattributeMatch(doc_id).maxWeight\n...\nterm(3).significance\nterm(3).weight\nterm(4).connectedness\nterm(4).significance\nterm(4).weight\ntextSimilarity(text).fieldCoverage\ntextSimilarity(text).order\ntextSimilarity(text).proximity\ntextSimilarity(text).queryCoverage\ntextSimilarity(text).score\n\n\n\n\n0\n1\n0\n1\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.583333\n100.0\n0.0\n0.0\n0.0\n0.50\n1.0\n1.000000\n0.50\n0.750000\n\n\n3\n7\n0\n0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.583333\n100.0\n0.0\n0.0\n0.0\n0.25\n0.0\n0.859375\n0.25\n0.425781\n\n\n1\n3\n1\n1\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.583333\n100.0\n0.0\n0.0\n0.0\n0.50\n1.0\n1.000000\n0.50\n0.750000\n\n\n5\n7\n1\n0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.583333\n100.0\n0.0\n0.0\n0.0\n0.25\n0.0\n0.859375\n0.25\n0.425781\n\n\n\n\n4 rows × 94 columns\n\n\n\nUse a DataFrame for labeled_data instead of a list of dict:\n\nlabeled_data = [\n    {\n        \"qid\": 0,\n        \"query\": \"give me title 1\",\n        \"doc_id\": 1, \n        \"relevance\": 1\n    },\n    {\n        \"qid\": 1,\n        \"query\": \"give me title 3\",\n        \"doc_id\": 3, \n        \"relevance\": 1\n    },\n]\nlabeled_data_df = DataFrame.from_records(labeled_data)\nlabeled_data_df\n\n\n\n\n\n\n\n\nqid\nquery\ndoc_id\nrelevance\n\n\n\n\n0\n0\ngive me title 1\n1\n1\n\n\n1\n1\ngive me title 3\n3\n1\n\n\n\n\n\n\n\n\nrank_features = collect_vespa_features(\n    app=app,\n    labeled_data=labeled_data_df,\n    id_field=\"doc_id\",\n    query_model=QueryModel(\n        match_phase=OR(), ranking=Ranking(name=\"bm25\", list_features=True)\n    ),\n    number_additional_docs=2,\n    fields=[\"rankfeatures\"],\n)\nrank_features\n\n\n\n\n\n\n\n\ndocument_id\nquery_id\nlabel\nattributeMatch(doc_id)\nattributeMatch(doc_id).averageWeight\nattributeMatch(doc_id).completeness\nattributeMatch(doc_id).fieldCompleteness\nattributeMatch(doc_id).importance\nattributeMatch(doc_id).matches\nattributeMatch(doc_id).maxWeight\n...\nterm(3).significance\nterm(3).weight\nterm(4).connectedness\nterm(4).significance\nterm(4).weight\ntextSimilarity(text).fieldCoverage\ntextSimilarity(text).order\ntextSimilarity(text).proximity\ntextSimilarity(text).queryCoverage\ntextSimilarity(text).score\n\n\n\n\n0\n1\n0\n0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.583333\n100.0\n0.0\n0.0\n0.0\n0.50\n1.0\n1.000000\n0.50\n0.750000\n\n\n3\n7\n0\n0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.583333\n100.0\n0.0\n0.0\n0.0\n0.25\n0.0\n0.859375\n0.25\n0.425781\n\n\n1\n3\n1\n0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.583333\n100.0\n0.0\n0.0\n0.0\n0.50\n1.0\n1.000000\n0.50\n0.750000\n\n\n5\n7\n1\n0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.583333\n100.0\n0.0\n0.0\n0.0\n0.25\n0.0\n0.859375\n0.25\n0.425781\n\n\n\n\n4 rows × 94 columns\n\n\n\nKeep only selected features by specifying their names in the keep_features argument:\n\nrank_features = collect_vespa_features(\n    app=app,\n    labeled_data=labeled_data_df,\n    id_field=\"doc_id\",\n    query_model=QueryModel(\n        match_phase=OR(), ranking=Ranking(name=\"bm25\", list_features=True)\n    ),\n    number_additional_docs=2,\n    fields=[\"rankfeatures\"],\n    keep_features=[\"textSimilarity(text).score\"],\n)\nrank_features\n\n\n\n\n\n\n\n\ndocument_id\nquery_id\nlabel\ntextSimilarity(text).score\n\n\n\n\n0\n1\n0\n0\n0.750000\n\n\n3\n7\n0\n0\n0.425781\n\n\n1\n3\n1\n0\n0.750000\n\n\n5\n7\n1\n0\n0.425781\n\n\n\n\n\n\n\n\nsource\n\n\nstore_vespa_features\n\n store_vespa_features (app:vespa.application.Vespa, output_file_path:str,\n                       labeled_data, id_field:str,\n                       query_model:__main__.QueryModel,\n                       number_additional_docs:int, fields:List[str],\n                       keep_features:Optional[List[str]]=None,\n                       relevant_score:int=1, default_score:int=0,\n                       batch_size=1000, **kwargs)\n\nRetrieve Vespa rank features and store them in a .csv file.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\napp\nVespa\n\nConnection to a Vespa application.\n\n\noutput_file_path\nstr\n\nPath of the .csv output file. It will create the file of it does not exist and append the vespa features to an pre-existing file.\n\n\nlabeled_data\n\n\nLabelled data containing query, query_id and relevant ids. See details about data format.\n\n\nid_field\nstr\n\nThe Vespa field representing the document id.\n\n\nquery_model\nQueryModel\n\nQuery model.\n\n\nnumber_additional_docs\nint\n\nNumber of additional documents to retrieve for each relevant document.\n\n\nfields\ntyping.List[str]\n\nList of Vespa fields to collect, e.g. [“rankfeatures”, “summaryfeatures”]\n\n\nkeep_features\ntyping.Optional[typing.List[str]]\nNone\nList containing the names of the features that should be returned. Default to None, which return all the features contained in the ‘fields’ argument.\n\n\nrelevant_score\nint\n1\nScore to assign to relevant documents.\n\n\ndefault_score\nint\n0\nScore to assign to the additional documents that are not relevant.\n\n\nbatch_size\nint\n1000\nThe size of the batch of labeled data points to be processed.\n\n\nkwargs\n\n\n\n\n\nReturns\nint\n\nreturns 0 upon success.\n\n\n\nUsage:\n\nlabeled_data = [\n    {\n        \"query_id\": 0,\n        \"query\": \"give me title 1\",\n        \"relevant_docs\": [{\"id\": \"1\", \"score\": 1}],\n    },\n    {\n        \"query_id\": 1,\n        \"query\": \"give me title 3\",\n        \"relevant_docs\": [{\"id\": \"3\", \"score\": 1}],\n    },\n]\n\nstore_vespa_features(\n    app=app,\n    output_file_path=\"vespa_features.csv\",\n    labeled_data=labeled_data,\n    id_field=\"doc_id\",\n    query_model=QueryModel(\n        match_phase=OR(), ranking=Ranking(name=\"bm25\", list_features=True)\n    ),\n    number_additional_docs=2,\n    fields=[\"rankfeatures\", \"summaryfeatures\"],\n)\nrank_features = read_csv(\"vespa_features.csv\")\nrank_features\n\nRows collected: 4.\nBatch progress: 1/1.\n\n\n\n\n\n\n\n\n\ndocument_id\nquery_id\nlabel\nattributeMatch(doc_id)\nattributeMatch(doc_id).averageWeight\nattributeMatch(doc_id).completeness\nattributeMatch(doc_id).fieldCompleteness\nattributeMatch(doc_id).importance\nattributeMatch(doc_id).matches\nattributeMatch(doc_id).maxWeight\n...\nterm(3).weight\nterm(4).connectedness\nterm(4).significance\nterm(4).weight\ntextSimilarity(text).fieldCoverage\ntextSimilarity(text).order\ntextSimilarity(text).proximity\ntextSimilarity(text).queryCoverage\ntextSimilarity(text).score\nvespa.summaryFeatures.cached\n\n\n\n\n0\n1\n0\n1\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n100.0\n0.0\n0.0\n0.0\n0.50\n1.0\n1.000000\n0.50\n0.750000\n0.0\n\n\n1\n7\n0\n0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n100.0\n0.0\n0.0\n0.0\n0.25\n0.0\n0.859375\n0.25\n0.425781\n0.0\n\n\n2\n3\n1\n1\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n100.0\n0.0\n0.0\n0.0\n0.50\n1.0\n1.000000\n0.50\n0.750000\n0.0\n\n\n3\n7\n1\n0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n100.0\n0.0\n0.0\n0.0\n0.25\n0.0\n0.859375\n0.25\n0.425781\n0.0\n\n\n\n\n4 rows × 95 columns"
  },
  {
    "objectID": "module_stats.html",
    "href": "module_stats.html",
    "title": "stats",
    "section": "",
    "text": "source\n\n\n\n bootstrap_sampling (data:pandas.core.frame.DataFrame,\n                     estimator:Callable=&lt;function mean at 0x7fb085310df0&gt;,\n                     n_boot:int=1000, columns_to_exclude:List[str]=None)\n\nCompute bootstrap estimates of the data distribution\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndata\nDataFrame\n\nData containing the columns we want to generate bootstrap estimates from.\n\n\nestimator\ntyping.Callable\nmean\nestimator function that accepts an array-like argument.\n\n\nn_boot\nint\n1000\nNumber of bootstrap estimates to compute.\n\n\ncolumns_to_exclude\ntyping.List[str]\nNone\nColumn names to exclude.\n\n\n\nUsage:\nGenerate data with columns containing data that we want to compute estimates from. The values in the column a comes from Normal distribution with mean 0 and standard deviation 1. The values from column b comes from Normal distribution with mean 100 and standard deviation 10.\n\ndata = pd.DataFrame(\n    data={\n        \"a\": np.random.normal(size = 100), \n        \"b\": np.random.normal(loc=100, scale = 10, size = 100)\n    }\n)\ndata.head()\n\n\n\n\n\n\n\n\na\nb\n\n\n\n\n0\n0.605639\n92.817505\n\n\n1\n-0.775791\n92.750026\n\n\n2\n-1.265231\n107.981771\n\n\n3\n0.981306\n101.388385\n\n\n4\n0.029075\n122.700172\n\n\n\n\n\n\n\n\n\nBy default, the function generates the mean of each column n_boot times. Each value represents the mean obtained from a bootstrap sample of the original data.\n\nestimates = bootstrap_sampling(data, n_boot=100)\nestimates\n\n\n\n\n\n\n\n\na\nb\n\n\n\n\n0\n0.012356\n100.018394\n\n\n1\n0.143189\n100.691872\n\n\n2\n-0.002554\n99.874399\n\n\n3\n0.079395\n99.539636\n\n\n4\n0.055096\n100.452383\n\n\n...\n...\n...\n\n\n95\n0.063409\n100.439363\n\n\n96\n-0.024455\n98.607045\n\n\n97\n0.209427\n99.866736\n\n\n98\n0.061323\n98.680469\n\n\n99\n0.289456\n99.980295\n\n\n\n\n100 rows × 2 columns\n\n\n\nWe can check if the estimates make sense by compute the mean of the bootstrap estimates and comparing with the mean of the Normal distribution they were generated from.\n\nestimates.mean()\n\na      0.089538\nb    100.099900\ndtype: float64\n\n\n\n\n\nWe can specify other functions, such as np.std to compute the standard deviation.\n\nestimates = bootstrap_sampling(data, estimator=np.std, n_boot=100)\nestimates\n\n\n\n\n\n\n\n\na\nb\n\n\n\n\n0\n0.933496\n10.126658\n\n\n1\n0.929125\n9.852667\n\n\n2\n0.899762\n10.307814\n\n\n3\n0.968039\n10.416074\n\n\n4\n1.004349\n10.441463\n\n\n...\n...\n...\n\n\n95\n0.910904\n10.357727\n\n\n96\n0.818276\n12.358640\n\n\n97\n0.981826\n9.622724\n\n\n98\n0.962237\n10.897055\n\n\n99\n0.913994\n11.096338\n\n\n\n\n100 rows × 2 columns\n\n\n\nIf we take the mean of the bootstrap estimates of the standard deviation, we should recover a value close to the standard deviation of the distribution that the data were generated from.\n\nestimates.mean()\n\na     0.943942\nb    10.480457\ndtype: float64\n\n\n\n\n\n\nestimates = bootstrap_sampling(\n    data, n_boot=100, columns_to_exclude=[\"b\"]\n)\nestimates\n\n\n\n\n\n\n\n\na\n\n\n\n\n0\n0.259128\n\n\n1\n0.098232\n\n\n2\n0.087111\n\n\n3\n-0.131376\n\n\n4\n0.050997\n\n\n...\n...\n\n\n95\n0.129835\n\n\n96\n-0.004873\n\n\n97\n-0.046338\n\n\n98\n0.246239\n\n\n99\n0.355848\n\n\n\n\n100 rows × 1 columns\n\n\n\n\nsource\n\n\n\n\n\n compute_evaluation_estimates (df:pandas.core.frame.DataFrame,\n                               n_boot:int=1000,\n                               estimator:Callable=&lt;function mean at\n                               0x7fb085310df0&gt;, quantile_low:float=0.025,\n                               quantile_high=0.975)\n\nCompute estimate and confidence interval for evaluation per query metrics.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndf\nDataFrame\n\nEvaluations per query data, usually obtained pyvespa evaluate method.\n\n\nn_boot\nint\n1000\nNumber of bootstrap samples.\n\n\nestimator\ntyping.Callable\nmean\nestimator function that accepts an array-like argument.\n\n\nquantile_low\nfloat\n0.025\nlower quantile to compute confidence interval\n\n\nquantile_high\nfloat\n0.975\nupper quantile to compute confidence interval\n\n\n\nUsage:\nGenerate sample data frame, which must contain the column model.\n\nnumber_data_points = 1000\ndata = pd.DataFrame(\n    data = {\n        \"model\": (\n            [\"A\"] * number_data_points + \n            [\"B\"] * number_data_points\n        ),\n        \"query_id\": (\n            list(range(number_data_points)) + \n            list(range(number_data_points))\n        ),\n        \"metric_1\": (\n            np.random.binomial(size=number_data_points, n=1, p=0.3).tolist() + \n            np.random.binomial(size=number_data_points, n=1, p=0.7).tolist()\n        ),\n        \"metric_2\": (\n            np.random.binomial(size=number_data_points, n=1, p=0.1).tolist() + \n            np.random.binomial(size=number_data_points, n=1, p=0.9).tolist()\n        )\n        \n    }\n).sort_values(\"query_id\").reset_index(drop=True)\ndata\n\n\n\n\n\n\n\n\nmodel\nquery_id\nmetric_1\nmetric_2\n\n\n\n\n0\nA\n0\n0\n0\n\n\n1\nB\n0\n1\n1\n\n\n2\nA\n1\n0\n1\n\n\n3\nB\n1\n1\n1\n\n\n4\nA\n2\n0\n0\n\n\n...\n...\n...\n...\n...\n\n\n1995\nA\n997\n1\n0\n\n\n1996\nB\n998\n1\n1\n\n\n1997\nA\n998\n1\n0\n\n\n1998\nA\n999\n0\n0\n\n\n1999\nB\n999\n0\n1\n\n\n\n\n2000 rows × 4 columns\n\n\n\n\n\n\ncompute_evaluation_estimates(data)\n\n\n\n\n\n\n\n\nmetric\nmodel\nlow\nmedian\nhigh\n\n\n\n\n0\nmetric_1\nA\n0.268000\n0.296\n0.325\n\n\n1\nmetric_1\nB\n0.667000\n0.696\n0.724\n\n\n2\nmetric_2\nA\n0.091000\n0.109\n0.129\n\n\n3\nmetric_2\nB\n0.887975\n0.907\n0.924\n\n\n\n\n\n\n\n\n\n\n\ncompute_evaluation_estimates(data, estimator=np.std)\n\n\n\n\n\n\n\n\nmetric\nmodel\nlow\nmedian\nhigh\n\n\n\n\n0\nmetric_1\nA\n0.442918\n0.456491\n0.468375\n\n\n1\nmetric_1\nB\n0.448001\n0.459983\n0.470931\n\n\n2\nmetric_2\nA\n0.289026\n0.311639\n0.335200\n\n\n3\nmetric_2\nB\n0.264998\n0.291829\n0.315366\n\n\n\n\n\n\n\n\n\n\n\ncompute_evaluation_estimates(\n    data, \n    quantile_low=0.2, \n    quantile_high=0.8\n)\n\n\n\n\n\n\n\n\nmetric\nmodel\nlow\nmedian\nhigh\n\n\n\n\n0\nmetric_1\nA\n0.285\n0.296\n0.308\n\n\n1\nmetric_1\nB\n0.684\n0.696\n0.708\n\n\n2\nmetric_2\nA\n0.102\n0.110\n0.118\n\n\n3\nmetric_2\nB\n0.898\n0.906\n0.914\n\n\n\n\n\n\n\n\ncompute_evaluation_estimates(data[[\"model\", \"metric_1\", \"metric_2\"]])\n\n\n\n\n\n\n\n\nmetric\nmodel\nlow\nmedian\nhigh\n\n\n\n\n0\nmetric_1\nA\n0.269975\n0.297\n0.326000\n\n\n1\nmetric_1\nB\n0.667975\n0.696\n0.726000\n\n\n2\nmetric_2\nA\n0.091000\n0.109\n0.129025\n\n\n3\nmetric_2\nB\n0.888000\n0.907\n0.923000"
  },
  {
    "objectID": "module_stats.html#bootstrap-estimates",
    "href": "module_stats.html#bootstrap-estimates",
    "title": "stats",
    "section": "",
    "text": "source\n\n\n\n bootstrap_sampling (data:pandas.core.frame.DataFrame,\n                     estimator:Callable=&lt;function mean at 0x7fb085310df0&gt;,\n                     n_boot:int=1000, columns_to_exclude:List[str]=None)\n\nCompute bootstrap estimates of the data distribution\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndata\nDataFrame\n\nData containing the columns we want to generate bootstrap estimates from.\n\n\nestimator\ntyping.Callable\nmean\nestimator function that accepts an array-like argument.\n\n\nn_boot\nint\n1000\nNumber of bootstrap estimates to compute.\n\n\ncolumns_to_exclude\ntyping.List[str]\nNone\nColumn names to exclude.\n\n\n\nUsage:\nGenerate data with columns containing data that we want to compute estimates from. The values in the column a comes from Normal distribution with mean 0 and standard deviation 1. The values from column b comes from Normal distribution with mean 100 and standard deviation 10.\n\ndata = pd.DataFrame(\n    data={\n        \"a\": np.random.normal(size = 100), \n        \"b\": np.random.normal(loc=100, scale = 10, size = 100)\n    }\n)\ndata.head()\n\n\n\n\n\n\n\n\na\nb\n\n\n\n\n0\n0.605639\n92.817505\n\n\n1\n-0.775791\n92.750026\n\n\n2\n-1.265231\n107.981771\n\n\n3\n0.981306\n101.388385\n\n\n4\n0.029075\n122.700172\n\n\n\n\n\n\n\n\n\nBy default, the function generates the mean of each column n_boot times. Each value represents the mean obtained from a bootstrap sample of the original data.\n\nestimates = bootstrap_sampling(data, n_boot=100)\nestimates\n\n\n\n\n\n\n\n\na\nb\n\n\n\n\n0\n0.012356\n100.018394\n\n\n1\n0.143189\n100.691872\n\n\n2\n-0.002554\n99.874399\n\n\n3\n0.079395\n99.539636\n\n\n4\n0.055096\n100.452383\n\n\n...\n...\n...\n\n\n95\n0.063409\n100.439363\n\n\n96\n-0.024455\n98.607045\n\n\n97\n0.209427\n99.866736\n\n\n98\n0.061323\n98.680469\n\n\n99\n0.289456\n99.980295\n\n\n\n\n100 rows × 2 columns\n\n\n\nWe can check if the estimates make sense by compute the mean of the bootstrap estimates and comparing with the mean of the Normal distribution they were generated from.\n\nestimates.mean()\n\na      0.089538\nb    100.099900\ndtype: float64\n\n\n\n\n\nWe can specify other functions, such as np.std to compute the standard deviation.\n\nestimates = bootstrap_sampling(data, estimator=np.std, n_boot=100)\nestimates\n\n\n\n\n\n\n\n\na\nb\n\n\n\n\n0\n0.933496\n10.126658\n\n\n1\n0.929125\n9.852667\n\n\n2\n0.899762\n10.307814\n\n\n3\n0.968039\n10.416074\n\n\n4\n1.004349\n10.441463\n\n\n...\n...\n...\n\n\n95\n0.910904\n10.357727\n\n\n96\n0.818276\n12.358640\n\n\n97\n0.981826\n9.622724\n\n\n98\n0.962237\n10.897055\n\n\n99\n0.913994\n11.096338\n\n\n\n\n100 rows × 2 columns\n\n\n\nIf we take the mean of the bootstrap estimates of the standard deviation, we should recover a value close to the standard deviation of the distribution that the data were generated from.\n\nestimates.mean()\n\na     0.943942\nb    10.480457\ndtype: float64\n\n\n\n\n\n\nestimates = bootstrap_sampling(\n    data, n_boot=100, columns_to_exclude=[\"b\"]\n)\nestimates\n\n\n\n\n\n\n\n\na\n\n\n\n\n0\n0.259128\n\n\n1\n0.098232\n\n\n2\n0.087111\n\n\n3\n-0.131376\n\n\n4\n0.050997\n\n\n...\n...\n\n\n95\n0.129835\n\n\n96\n-0.004873\n\n\n97\n-0.046338\n\n\n98\n0.246239\n\n\n99\n0.355848\n\n\n\n\n100 rows × 1 columns\n\n\n\n\nsource\n\n\n\n\n\n compute_evaluation_estimates (df:pandas.core.frame.DataFrame,\n                               n_boot:int=1000,\n                               estimator:Callable=&lt;function mean at\n                               0x7fb085310df0&gt;, quantile_low:float=0.025,\n                               quantile_high=0.975)\n\nCompute estimate and confidence interval for evaluation per query metrics.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndf\nDataFrame\n\nEvaluations per query data, usually obtained pyvespa evaluate method.\n\n\nn_boot\nint\n1000\nNumber of bootstrap samples.\n\n\nestimator\ntyping.Callable\nmean\nestimator function that accepts an array-like argument.\n\n\nquantile_low\nfloat\n0.025\nlower quantile to compute confidence interval\n\n\nquantile_high\nfloat\n0.975\nupper quantile to compute confidence interval\n\n\n\nUsage:\nGenerate sample data frame, which must contain the column model.\n\nnumber_data_points = 1000\ndata = pd.DataFrame(\n    data = {\n        \"model\": (\n            [\"A\"] * number_data_points + \n            [\"B\"] * number_data_points\n        ),\n        \"query_id\": (\n            list(range(number_data_points)) + \n            list(range(number_data_points))\n        ),\n        \"metric_1\": (\n            np.random.binomial(size=number_data_points, n=1, p=0.3).tolist() + \n            np.random.binomial(size=number_data_points, n=1, p=0.7).tolist()\n        ),\n        \"metric_2\": (\n            np.random.binomial(size=number_data_points, n=1, p=0.1).tolist() + \n            np.random.binomial(size=number_data_points, n=1, p=0.9).tolist()\n        )\n        \n    }\n).sort_values(\"query_id\").reset_index(drop=True)\ndata\n\n\n\n\n\n\n\n\nmodel\nquery_id\nmetric_1\nmetric_2\n\n\n\n\n0\nA\n0\n0\n0\n\n\n1\nB\n0\n1\n1\n\n\n2\nA\n1\n0\n1\n\n\n3\nB\n1\n1\n1\n\n\n4\nA\n2\n0\n0\n\n\n...\n...\n...\n...\n...\n\n\n1995\nA\n997\n1\n0\n\n\n1996\nB\n998\n1\n1\n\n\n1997\nA\n998\n1\n0\n\n\n1998\nA\n999\n0\n0\n\n\n1999\nB\n999\n0\n1\n\n\n\n\n2000 rows × 4 columns\n\n\n\n\n\n\ncompute_evaluation_estimates(data)\n\n\n\n\n\n\n\n\nmetric\nmodel\nlow\nmedian\nhigh\n\n\n\n\n0\nmetric_1\nA\n0.268000\n0.296\n0.325\n\n\n1\nmetric_1\nB\n0.667000\n0.696\n0.724\n\n\n2\nmetric_2\nA\n0.091000\n0.109\n0.129\n\n\n3\nmetric_2\nB\n0.887975\n0.907\n0.924\n\n\n\n\n\n\n\n\n\n\n\ncompute_evaluation_estimates(data, estimator=np.std)\n\n\n\n\n\n\n\n\nmetric\nmodel\nlow\nmedian\nhigh\n\n\n\n\n0\nmetric_1\nA\n0.442918\n0.456491\n0.468375\n\n\n1\nmetric_1\nB\n0.448001\n0.459983\n0.470931\n\n\n2\nmetric_2\nA\n0.289026\n0.311639\n0.335200\n\n\n3\nmetric_2\nB\n0.264998\n0.291829\n0.315366\n\n\n\n\n\n\n\n\n\n\n\ncompute_evaluation_estimates(\n    data, \n    quantile_low=0.2, \n    quantile_high=0.8\n)\n\n\n\n\n\n\n\n\nmetric\nmodel\nlow\nmedian\nhigh\n\n\n\n\n0\nmetric_1\nA\n0.285\n0.296\n0.308\n\n\n1\nmetric_1\nB\n0.684\n0.696\n0.708\n\n\n2\nmetric_2\nA\n0.102\n0.110\n0.118\n\n\n3\nmetric_2\nB\n0.898\n0.906\n0.914\n\n\n\n\n\n\n\n\ncompute_evaluation_estimates(data[[\"model\", \"metric_1\", \"metric_2\"]])\n\n\n\n\n\n\n\n\nmetric\nmodel\nlow\nmedian\nhigh\n\n\n\n\n0\nmetric_1\nA\n0.269975\n0.297\n0.326000\n\n\n1\nmetric_1\nB\n0.667975\n0.696\n0.726000\n\n\n2\nmetric_2\nA\n0.091000\n0.109\n0.129025\n\n\n3\nmetric_2\nB\n0.888000\n0.907\n0.923000"
  },
  {
    "objectID": "module_passage.html",
    "href": "module_passage.html",
    "title": "passage",
    "section": "",
    "text": "Code related to the manipulation of passage ranking data.\n\nsource\n\n\n\n sample_dict_items (d:Dict, n:int)\n\nSample items from a dict.\n\n\n\n\nType\nDetails\n\n\n\n\nd\ntyping.Dict\ndict to be samples from.\n\n\nn\nint\nNumber of samples\n\n\nReturns\ntyping.Dict\ndict with sampled values\n\n\n\nUsage:\n\nd = {\"a\": 1, \"b\":2, \"c\":3}\n\n\nsample_dict_items(d, 1)\n\n{'a': 1}\n\n\n\nsample_dict_items(d, 2)\n\n{'c': 3, 'b': 2}\n\n\n\nsample_dict_items(d, 3)\n\n{'a': 1, 'c': 3, 'b': 2}\n\n\nReturn full dict in case number of samples is higher than length of the dict:\n\nsample_dict_items(d, 4)\n\n{'a': 1, 'c': 3, 'b': 2}\n\n\n\nsource\n\n\n\n\n save_data (corpus:Dict, train_qrels:Dict, train_queries:Dict,\n            dev_qrels:Dict, dev_queries:Dict,\n            file_path:str='passage_sample.json')\n\nSave data to disk.\nThe main goal is to save sample data to disk.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ncorpus\ntyping.Dict\n\nDocument corpus, see usage example below.\n\n\ntrain_qrels\ntyping.Dict\n\nTraining relevance scores, see usage example below.\n\n\ntrain_queries\ntyping.Dict\n\nTraining queries, see usage example below.\n\n\ndev_qrels\ntyping.Dict\n\nDevelopment relevance scores, see usage example below.\n\n\ndev_queries\ntyping.Dict\n\nDevelopment queries, see usage example below.\n\n\nfile_path\nstr\npassage_sample.json\nValid JSON file path.\n\n\nReturns\nNone\n\nSide-effect: data is saved to file_path.\n\n\n\nUsage:\n\ncorpus = {\n    \"0\": \"sentence 0\", \n    \"1\": \"sentence 1\", \n    \"2\": \"sentence 2\", \n    \"3\": \"sentence 3\"\n}\ntrain_queries = {\n    \"10\": \"train query 10\",\n    \"11\": \"train query 11\"\n}\ntrain_qrels = {\n    \"10\": {\"0\": 1},\n    \"11\": {\"2\": 1}\n}\ndev_queries = {\n    \"20\": \"train query 20\",\n    \"21\": \"train query 21\"\n}\ndev_qrels = {\n    \"20\": {\"1\": 1},\n    \"21\": {\"3\": 1}\n}\n\n\nsave_data(\n    corpus, \n    train_qrels, train_queries, \n    dev_qrels, dev_queries, \n    file_path=\"passage_sample.json\"\n)\n\n\nsource\n\n\n\n\n load_data (file_path:Optional[str]=None)\n\nLoad data.\nThe main goal is to load sample data from disk. If a file_path is not provided, a pre-generated data sample will be downloaded.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nfile_path\ntyping.Optional[str]\nNone\nvalid JSON file path contain data saved by save_data. If None, a pre-generated sample will be downloaded.\n\n\nReturns\ntyping.Dict\n\nSee usage example below for expected format.\n\n\n\nUsage:\n\nWith file_path:\n\n\ndata = load_data(\"passage_sample.json\")\n\n\ndata\n\n{'corpus': {'0': 'sentence 0',\n  '1': 'sentence 1',\n  '2': 'sentence 2',\n  '3': 'sentence 3'},\n 'train_qrels': {'10': {'0': 1}, '11': {'2': 1}},\n 'train_queries': {'10': 'train query 10', '11': 'train query 11'},\n 'dev_qrels': {'20': {'1': 1}, '21': {'3': 1}},\n 'dev_queries': {'20': 'train query 20', '21': 'train query 21'}}\n\n\n\nWithout file_path specified, a pre-generated sample data will be downloaded:\n\n\ndata = load_data()\n\n\ndata.keys()\n\ndict_keys(['corpus', 'train_qrels', 'train_queries', 'dev_qrels', 'dev_queries'])\n\n\n\nlen(data[\"corpus\"])\n\n1000\n\n\n\nsource\n\n\n\n\n PassageData (corpus:Optional[Dict]=None, train_qrels:Optional[Dict]=None,\n              train_queries:Optional[Dict]=None,\n              dev_qrels:Optional[Dict]=None,\n              dev_queries:Optional[Dict]=None)\n\nContainer for passage data\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ncorpus\ntyping.Optional[typing.Dict]\nNone\nDocument corpus, see usage example below.\n\n\ntrain_qrels\ntyping.Optional[typing.Dict]\nNone\nTraining relevance scores, see usage example below.\n\n\ntrain_queries\ntyping.Optional[typing.Dict]\nNone\nTraining queries, see usage example below.\n\n\ndev_qrels\ntyping.Optional[typing.Dict]\nNone\nDevelopment relevance scores, see usage example below.\n\n\ndev_queries\ntyping.Optional[typing.Dict]\nNone\nDevelopment queries, see usage example below.\n\n\n\nUsage:\n\ncorpus = {\n    \"0\": \"sentence 0\", \n    \"1\": \"sentence 1\", \n    \"2\": \"sentence 2\", \n    \"3\": \"sentence 3\"\n}\ntrain_queries = {\n    \"10\": \"train query 10\",\n    \"11\": \"train query 11\"\n}\ntrain_qrels = {\n    \"10\": {\"0\": 1},\n    \"11\": {\"2\": 1}\n}\ndev_queries = {\n    \"20\": \"train query 20\",\n    \"21\": \"train query 21\"\n}\ndev_qrels = {\n    \"20\": {\"1\": 1},\n    \"21\": {\"3\": 1}\n}\n\n\npassage_data = PassageData(\n    corpus=corpus, \n    train_queries = train_queries, \n    train_qrels=train_qrels,\n    dev_queries = dev_queries,\n    dev_qrels = dev_qrels\n)\n\n\npassage_data\n\nPassageData(corpus, train_qrels, train_queries, dev_qrels, dev_queries)\n\n\n\nsource\n\n\n\n\n PassageData.save (file_path:str='passage_sample.json')\n\n\npassage_data.save()\n\n\nsource\n\n\n\n\n PassageData.load (file_path:Optional[str]=None)\n\nLoad passage data from disk.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nfile_path\ntyping.Optional[str]\nNone\nvalid JSON file path contain data saved by save_data. If None, a pre-generated sample will be downloaded.\n\n\nReturns\nPassageData\n\n\n\n\n\n\ndata = PassageData.load(file_path=\"passage_sample.json\")\n\n\ndata\n\nPassageData(corpus, train_qrels, train_queries, dev_qrels, dev_queries)\n\n\n\nsource\n\n\n\n\n PassageData.summary ()\n\nSummary of the size of the dataset components.\n\ndata.summary\n\nNumber of documents: 4\nNumber of train queries: 2\nNumber of train relevance judgments: 2\nNumber of dev queries: 2\nNumber of dev relevance judgments: 2\n\n\n\nsource\n\n\n\n\n PassageData.get_corpus ()\n\n\npassage_data.get_corpus()\n\n\n\n\n\n\n\n\ndoc_id\ntext\n\n\n\n\n0\n0\nsentence 0\n\n\n1\n1\nsentence 1\n\n\n2\n2\nsentence 2\n\n\n3\n3\nsentence 3\n\n\n\n\n\n\n\n\nsource\n\n\n\n\n PassageData.get_queries (type:str)\n\nGet query data.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\ntype\nstr\nEither ‘train’ or ‘dev’.\n\n\nReturns\nDataFrame\nDataFrame conaining ‘query_id’ and ‘query’.\n\n\n\n\npassage_data.get_queries(type=\"train\")\n\n\n\n\n\n\n\n\nquery_id\nquery\n\n\n\n\n0\n10\ntrain query 10\n\n\n1\n11\ntrain query 11\n\n\n\n\n\n\n\n\npassage_data.get_queries(type=\"dev\")\n\n\n\n\n\n\n\n\nquery_id\nquery\n\n\n\n\n0\n20\ntrain query 20\n\n\n1\n21\ntrain query 21\n\n\n\n\n\n\n\n\nsource\n\n\n\n\n PassageData.get_labels (type:str)\n\nGet labeled data\n\n\n\n\nType\nDetails\n\n\n\n\ntype\nstr\nEither ‘train’ or ‘dev’.\n\n\nReturns\ntyping.Dict\npyvespa-formatted labeled data\n\n\n\n\npassage_data.get_labels(type=\"train\")\n\n[{'query_id': '10',\n  'query': 'train query 10',\n  'relevant_docs': [{'id': '0', 'score': 1}]},\n {'query_id': '11',\n  'query': 'train query 11',\n  'relevant_docs': [{'id': '2', 'score': 1}]}]\n\n\n\npassage_data.get_labels(type=\"dev\")\n\n[{'query_id': '20',\n  'query': 'train query 20',\n  'relevant_docs': [{'id': '1', 'score': 1}]},\n {'query_id': '21',\n  'query': 'train query 21',\n  'relevant_docs': [{'id': '3', 'score': 1}]}]\n\n\n\nsource\n\n\n\n\n sample_data (n_relevant:int, n_irrelevant:int)\n\nSample data from the passage ranking dataset.\nThe final sample contains n_relevant train relevant documents, n_relevant dev relevant documents and n_irrelevant random documents sampled from the entire corpus.\nAll the relevant sampled documents, both from train and dev sets, are guaranteed to be on the corpus_sample, which will contain 2 * n_relevant + n_irrelevant documents.\n\n\n\n\nType\nDetails\n\n\n\n\nn_relevant\nint\nThe number of relevant documents to sample.\n\n\nn_irrelevant\nint\nThe number of non-judged documents to sample.\n\n\nReturns\nPassageData\n\n\n\n\nUsage:\n\nsample = sample_data(n_relevant=1, n_irrelevant=3)\n\nThe sampled corpus is a dict containing document id as key and the passage text as value.\n\nsample.corpus\n\n{'890370': 'the map of europe gives you a clear view of the political boundaries that segregate the countries in the continent including germany uk france spain italy greece romania ukraine hungary austria sweden finland norway czech republic belgium luxembourg switzerland croatia and albaniahe map of europe gives you a clear view of the political boundaries that segregate the countries in the continent including germany uk france spain italy greece romania ukraine hungary austria sweden finland norway czech republic belgium luxembourg switzerland croatia and albania',\n '5060205': 'Setting custom HTTP headers with cURL can be done by using the CURLOPT_HTTPHEADER option, which can be set with the curl_setopt function. To add headers to your HTTP request you need to put them into a PHP Array, which you can then pass to the cul_setopt function, like demonstrated in the below example.',\n '6096573': \"The sugar in RNA is ribose, whereas the sugar in DNA is deoxyribose. The only difference between the two is that in deoxyribose, there is an oxygen missing from the 2' carbon …(there is a H there instead of an OH). This makes DNA more stable/less reactive than RNA. 1 person found this useful.\",\n '3092885': 'All three C-Ph bonds are typical of sp 3 - sp 2 carbon-carbon bonds with lengths of approximately 1.47 A, å while The-C o bond length is approximately.1 42. A å the presence of three adjacent phenyl groups confers special properties manifested in the reactivity of. the alcohol',\n '7275560': 'shortest phase of mitosis Anaphase is the shortest phase of mitosis. During anaphase the arranged chromosomes at the metaphase plate are migrate towards their respective poles. Before this migration started, chromosomes are divided into sister chromatids, by the separation of joined centromere of two sister chromatids of a chromosomes.'}\n\n\nThe size of the sampled document corpus is equal to 2 * n_relevant + n_irrelevant.\n\nlen(sample.corpus)\n\n5\n\n\nSampled queries are dict containing query id as key and query text as value.\n\nprint(sample.train_queries)\nprint(sample.dev_queries)\n\n{'899723': 'what sugar is found in rna'}\n{'994205': 'which is the shortest stage in duration'}\n\n\nSampled qrels contains one relevant document for each query.\n\nprint(sample.train_qrels)\nprint(sample.dev_qrels)\n\n{'899723': {'6096573': 1}}\n{'994205': {'7275560': 1}}\n\n\nThe following relevant documents are guaranteed to be included in the corpus_sample.\n\n\n['6096573', '7275560']"
  },
  {
    "objectID": "module_passage.html#data-manipulation",
    "href": "module_passage.html#data-manipulation",
    "title": "passage",
    "section": "",
    "text": "Code related to the manipulation of passage ranking data.\n\nsource\n\n\n\n sample_dict_items (d:Dict, n:int)\n\nSample items from a dict.\n\n\n\n\nType\nDetails\n\n\n\n\nd\ntyping.Dict\ndict to be samples from.\n\n\nn\nint\nNumber of samples\n\n\nReturns\ntyping.Dict\ndict with sampled values\n\n\n\nUsage:\n\nd = {\"a\": 1, \"b\":2, \"c\":3}\n\n\nsample_dict_items(d, 1)\n\n{'a': 1}\n\n\n\nsample_dict_items(d, 2)\n\n{'c': 3, 'b': 2}\n\n\n\nsample_dict_items(d, 3)\n\n{'a': 1, 'c': 3, 'b': 2}\n\n\nReturn full dict in case number of samples is higher than length of the dict:\n\nsample_dict_items(d, 4)\n\n{'a': 1, 'c': 3, 'b': 2}\n\n\n\nsource\n\n\n\n\n save_data (corpus:Dict, train_qrels:Dict, train_queries:Dict,\n            dev_qrels:Dict, dev_queries:Dict,\n            file_path:str='passage_sample.json')\n\nSave data to disk.\nThe main goal is to save sample data to disk.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ncorpus\ntyping.Dict\n\nDocument corpus, see usage example below.\n\n\ntrain_qrels\ntyping.Dict\n\nTraining relevance scores, see usage example below.\n\n\ntrain_queries\ntyping.Dict\n\nTraining queries, see usage example below.\n\n\ndev_qrels\ntyping.Dict\n\nDevelopment relevance scores, see usage example below.\n\n\ndev_queries\ntyping.Dict\n\nDevelopment queries, see usage example below.\n\n\nfile_path\nstr\npassage_sample.json\nValid JSON file path.\n\n\nReturns\nNone\n\nSide-effect: data is saved to file_path.\n\n\n\nUsage:\n\ncorpus = {\n    \"0\": \"sentence 0\", \n    \"1\": \"sentence 1\", \n    \"2\": \"sentence 2\", \n    \"3\": \"sentence 3\"\n}\ntrain_queries = {\n    \"10\": \"train query 10\",\n    \"11\": \"train query 11\"\n}\ntrain_qrels = {\n    \"10\": {\"0\": 1},\n    \"11\": {\"2\": 1}\n}\ndev_queries = {\n    \"20\": \"train query 20\",\n    \"21\": \"train query 21\"\n}\ndev_qrels = {\n    \"20\": {\"1\": 1},\n    \"21\": {\"3\": 1}\n}\n\n\nsave_data(\n    corpus, \n    train_qrels, train_queries, \n    dev_qrels, dev_queries, \n    file_path=\"passage_sample.json\"\n)\n\n\nsource\n\n\n\n\n load_data (file_path:Optional[str]=None)\n\nLoad data.\nThe main goal is to load sample data from disk. If a file_path is not provided, a pre-generated data sample will be downloaded.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nfile_path\ntyping.Optional[str]\nNone\nvalid JSON file path contain data saved by save_data. If None, a pre-generated sample will be downloaded.\n\n\nReturns\ntyping.Dict\n\nSee usage example below for expected format.\n\n\n\nUsage:\n\nWith file_path:\n\n\ndata = load_data(\"passage_sample.json\")\n\n\ndata\n\n{'corpus': {'0': 'sentence 0',\n  '1': 'sentence 1',\n  '2': 'sentence 2',\n  '3': 'sentence 3'},\n 'train_qrels': {'10': {'0': 1}, '11': {'2': 1}},\n 'train_queries': {'10': 'train query 10', '11': 'train query 11'},\n 'dev_qrels': {'20': {'1': 1}, '21': {'3': 1}},\n 'dev_queries': {'20': 'train query 20', '21': 'train query 21'}}\n\n\n\nWithout file_path specified, a pre-generated sample data will be downloaded:\n\n\ndata = load_data()\n\n\ndata.keys()\n\ndict_keys(['corpus', 'train_qrels', 'train_queries', 'dev_qrels', 'dev_queries'])\n\n\n\nlen(data[\"corpus\"])\n\n1000\n\n\n\nsource\n\n\n\n\n PassageData (corpus:Optional[Dict]=None, train_qrels:Optional[Dict]=None,\n              train_queries:Optional[Dict]=None,\n              dev_qrels:Optional[Dict]=None,\n              dev_queries:Optional[Dict]=None)\n\nContainer for passage data\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ncorpus\ntyping.Optional[typing.Dict]\nNone\nDocument corpus, see usage example below.\n\n\ntrain_qrels\ntyping.Optional[typing.Dict]\nNone\nTraining relevance scores, see usage example below.\n\n\ntrain_queries\ntyping.Optional[typing.Dict]\nNone\nTraining queries, see usage example below.\n\n\ndev_qrels\ntyping.Optional[typing.Dict]\nNone\nDevelopment relevance scores, see usage example below.\n\n\ndev_queries\ntyping.Optional[typing.Dict]\nNone\nDevelopment queries, see usage example below.\n\n\n\nUsage:\n\ncorpus = {\n    \"0\": \"sentence 0\", \n    \"1\": \"sentence 1\", \n    \"2\": \"sentence 2\", \n    \"3\": \"sentence 3\"\n}\ntrain_queries = {\n    \"10\": \"train query 10\",\n    \"11\": \"train query 11\"\n}\ntrain_qrels = {\n    \"10\": {\"0\": 1},\n    \"11\": {\"2\": 1}\n}\ndev_queries = {\n    \"20\": \"train query 20\",\n    \"21\": \"train query 21\"\n}\ndev_qrels = {\n    \"20\": {\"1\": 1},\n    \"21\": {\"3\": 1}\n}\n\n\npassage_data = PassageData(\n    corpus=corpus, \n    train_queries = train_queries, \n    train_qrels=train_qrels,\n    dev_queries = dev_queries,\n    dev_qrels = dev_qrels\n)\n\n\npassage_data\n\nPassageData(corpus, train_qrels, train_queries, dev_qrels, dev_queries)\n\n\n\nsource\n\n\n\n\n PassageData.save (file_path:str='passage_sample.json')\n\n\npassage_data.save()\n\n\nsource\n\n\n\n\n PassageData.load (file_path:Optional[str]=None)\n\nLoad passage data from disk.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nfile_path\ntyping.Optional[str]\nNone\nvalid JSON file path contain data saved by save_data. If None, a pre-generated sample will be downloaded.\n\n\nReturns\nPassageData\n\n\n\n\n\n\ndata = PassageData.load(file_path=\"passage_sample.json\")\n\n\ndata\n\nPassageData(corpus, train_qrels, train_queries, dev_qrels, dev_queries)\n\n\n\nsource\n\n\n\n\n PassageData.summary ()\n\nSummary of the size of the dataset components.\n\ndata.summary\n\nNumber of documents: 4\nNumber of train queries: 2\nNumber of train relevance judgments: 2\nNumber of dev queries: 2\nNumber of dev relevance judgments: 2\n\n\n\nsource\n\n\n\n\n PassageData.get_corpus ()\n\n\npassage_data.get_corpus()\n\n\n\n\n\n\n\n\ndoc_id\ntext\n\n\n\n\n0\n0\nsentence 0\n\n\n1\n1\nsentence 1\n\n\n2\n2\nsentence 2\n\n\n3\n3\nsentence 3\n\n\n\n\n\n\n\n\nsource\n\n\n\n\n PassageData.get_queries (type:str)\n\nGet query data.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\ntype\nstr\nEither ‘train’ or ‘dev’.\n\n\nReturns\nDataFrame\nDataFrame conaining ‘query_id’ and ‘query’.\n\n\n\n\npassage_data.get_queries(type=\"train\")\n\n\n\n\n\n\n\n\nquery_id\nquery\n\n\n\n\n0\n10\ntrain query 10\n\n\n1\n11\ntrain query 11\n\n\n\n\n\n\n\n\npassage_data.get_queries(type=\"dev\")\n\n\n\n\n\n\n\n\nquery_id\nquery\n\n\n\n\n0\n20\ntrain query 20\n\n\n1\n21\ntrain query 21\n\n\n\n\n\n\n\n\nsource\n\n\n\n\n PassageData.get_labels (type:str)\n\nGet labeled data\n\n\n\n\nType\nDetails\n\n\n\n\ntype\nstr\nEither ‘train’ or ‘dev’.\n\n\nReturns\ntyping.Dict\npyvespa-formatted labeled data\n\n\n\n\npassage_data.get_labels(type=\"train\")\n\n[{'query_id': '10',\n  'query': 'train query 10',\n  'relevant_docs': [{'id': '0', 'score': 1}]},\n {'query_id': '11',\n  'query': 'train query 11',\n  'relevant_docs': [{'id': '2', 'score': 1}]}]\n\n\n\npassage_data.get_labels(type=\"dev\")\n\n[{'query_id': '20',\n  'query': 'train query 20',\n  'relevant_docs': [{'id': '1', 'score': 1}]},\n {'query_id': '21',\n  'query': 'train query 21',\n  'relevant_docs': [{'id': '3', 'score': 1}]}]\n\n\n\nsource\n\n\n\n\n sample_data (n_relevant:int, n_irrelevant:int)\n\nSample data from the passage ranking dataset.\nThe final sample contains n_relevant train relevant documents, n_relevant dev relevant documents and n_irrelevant random documents sampled from the entire corpus.\nAll the relevant sampled documents, both from train and dev sets, are guaranteed to be on the corpus_sample, which will contain 2 * n_relevant + n_irrelevant documents.\n\n\n\n\nType\nDetails\n\n\n\n\nn_relevant\nint\nThe number of relevant documents to sample.\n\n\nn_irrelevant\nint\nThe number of non-judged documents to sample.\n\n\nReturns\nPassageData\n\n\n\n\nUsage:\n\nsample = sample_data(n_relevant=1, n_irrelevant=3)\n\nThe sampled corpus is a dict containing document id as key and the passage text as value.\n\nsample.corpus\n\n{'890370': 'the map of europe gives you a clear view of the political boundaries that segregate the countries in the continent including germany uk france spain italy greece romania ukraine hungary austria sweden finland norway czech republic belgium luxembourg switzerland croatia and albaniahe map of europe gives you a clear view of the political boundaries that segregate the countries in the continent including germany uk france spain italy greece romania ukraine hungary austria sweden finland norway czech republic belgium luxembourg switzerland croatia and albania',\n '5060205': 'Setting custom HTTP headers with cURL can be done by using the CURLOPT_HTTPHEADER option, which can be set with the curl_setopt function. To add headers to your HTTP request you need to put them into a PHP Array, which you can then pass to the cul_setopt function, like demonstrated in the below example.',\n '6096573': \"The sugar in RNA is ribose, whereas the sugar in DNA is deoxyribose. The only difference between the two is that in deoxyribose, there is an oxygen missing from the 2' carbon …(there is a H there instead of an OH). This makes DNA more stable/less reactive than RNA. 1 person found this useful.\",\n '3092885': 'All three C-Ph bonds are typical of sp 3 - sp 2 carbon-carbon bonds with lengths of approximately 1.47 A, å while The-C o bond length is approximately.1 42. A å the presence of three adjacent phenyl groups confers special properties manifested in the reactivity of. the alcohol',\n '7275560': 'shortest phase of mitosis Anaphase is the shortest phase of mitosis. During anaphase the arranged chromosomes at the metaphase plate are migrate towards their respective poles. Before this migration started, chromosomes are divided into sister chromatids, by the separation of joined centromere of two sister chromatids of a chromosomes.'}\n\n\nThe size of the sampled document corpus is equal to 2 * n_relevant + n_irrelevant.\n\nlen(sample.corpus)\n\n5\n\n\nSampled queries are dict containing query id as key and query text as value.\n\nprint(sample.train_queries)\nprint(sample.dev_queries)\n\n{'899723': 'what sugar is found in rna'}\n{'994205': 'which is the shortest stage in duration'}\n\n\nSampled qrels contains one relevant document for each query.\n\nprint(sample.train_qrels)\nprint(sample.dev_qrels)\n\n{'899723': {'6096573': 1}}\n{'994205': {'7275560': 1}}\n\n\nThe following relevant documents are guaranteed to be included in the corpus_sample.\n\n\n['6096573', '7275560']"
  },
  {
    "objectID": "module_passage.html#basic-search",
    "href": "module_passage.html#basic-search",
    "title": "passage",
    "section": "Basic search",
    "text": "Basic search\nCode related to a basic search search engine for passage ranking.\n\nsource\n\ncreate_basic_search_package\n\n create_basic_search_package (name:str='PassageRanking')\n\nCreate a basic Vespa application package for passage ranking.\nVespa fields:\nThe application contain two string fields: doc_id and text.\nVespa rank functions:\nThe application contain two rank profiles: bm25 and nativeRank.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nname\nstr\nPassageRanking\nName of the application\n\n\nReturns\nApplicationPackage\n\npyvespa ApplicationPackage instance.\n\n\n\nUsage:\n\napp_package = create_basic_search_package(name=\"PassageModuleApp\")\n\nCheck how the Vespa schema definition for this application looks like:\n\nprint(app_package.schema.schema_to_text)\n\nschema PassageModuleApp {\n    document PassageModuleApp {\n        field doc_id type string {\n            indexing: attribute | summary\n        }\n        field text type string {\n            indexing: index | summary\n            index: enable-bm25\n        }\n    }\n    fieldset default {\n        fields: text\n    }\n    rank-profile bm25 {\n        first-phase {\n            expression: bm25(text)\n        }\n        summary-features {\n            bm25(text)\n        }\n    }\n    rank-profile native_rank {\n        first-phase {\n            expression: nativeRank(text)\n        }\n    }\n}"
  },
  {
    "objectID": "module_passage.html#evaluate-query-models",
    "href": "module_passage.html#evaluate-query-models",
    "title": "passage",
    "section": "Evaluate query models",
    "text": "Evaluate query models\n\nsource\n\nevaluate_query_models\n\n evaluate_query_models (app_package:vespa.package.ApplicationPackage,\n                        query_models:List[learntorank.query.QueryModel],\n                        metrics:List[learntorank.evaluation.EvalMetric],\n                        corpus_size:List[int], output_file_path:str,\n                        dev_query_percentage:float=0.006285807802305023,\n                        verbose:bool=True, **kwargs)\n\n\nfrom learntorank.evaluation import (\n    MatchRatio,\n    Recall, \n    ReciprocalRank, \n    NormalizedDiscountedCumulativeGain\n)\nfrom learntorank.query import QueryModel, OR, Ranking\n\ncorpus_size = [100, 200]\napp_package = create_basic_search_package(name=\"PassageEvaluationApp\")\nquery_models = [\n    QueryModel(\n        name=\"bm25\", \n        match_phase=OR(), \n        ranking=Ranking(name=\"bm25\")\n    ),\n    QueryModel(\n        name=\"native_rank\", \n        match_phase=OR(), \n        ranking=Ranking(name=\"native_rank\")\n    )\n]\nmetrics = [\n    MatchRatio(),\n    Recall(at=100), \n    ReciprocalRank(at=10), \n    NormalizedDiscountedCumulativeGain(at=10)\n]\noutput_file_path = \"test.csv\"\n\n\nestimates = evaluate_query_models(\n    app_package=app_package,\n    query_models=query_models,\n    metrics=metrics,\n    corpus_size=corpus_size,\n    dev_query_percentage=0.5,\n    output_file_path=output_file_path, \n    verbose=False\n)\n\n*****\nDeploy Vespa application:\n*****\nWaiting for configuration server, 0/300 seconds...\nWaiting for configuration server, 5/300 seconds...\nWaiting for configuration server, 10/300 seconds...\nWaiting for application status, 0/300 seconds...\nWaiting for application status, 5/300 seconds...\nWaiting for application status, 10/300 seconds...\nWaiting for application status, 15/300 seconds...\nWaiting for application status, 20/300 seconds...\nWaiting for application status, 25/300 seconds...\nWaiting for application status, 30/300 seconds...\nWaiting for application status, 35/300 seconds...\nWaiting for application status, 40/300 seconds...\nWaiting for application status, 45/300 seconds...\nWaiting for application status, 50/300 seconds...\nWaiting for application status, 55/300 seconds...\nWaiting for application status, 60/300 seconds...\nWaiting for application status, 65/300 seconds...\nWaiting for application status, 70/300 seconds...\nWaiting for application status, 75/300 seconds...\nWaiting for application status, 80/300 seconds..."
  },
  {
    "objectID": "stateless_sequence_classification_task.html",
    "href": "stateless_sequence_classification_task.html",
    "title": "Sequence Classification task",
    "section": "",
    "text": "Vespa has implemented accelerated model evaluation using ONNX Runtime in the stateless cluster. This opens up new usage areas for Vespa, such as serving model predictions."
  },
  {
    "objectID": "stateless_sequence_classification_task.html#define-the-model-server",
    "href": "stateless_sequence_classification_task.html#define-the-model-server",
    "title": "Sequence Classification task",
    "section": "Define the model server",
    "text": "Define the model server\nThe SequenceClassification task takes a text input and returns an array of floats that depends on the model used to solve the task. The model argument can be the id of the model as defined by the huggingface model hub.\n\nfrom learntorank.ml import SequenceClassification\n\ntask = SequenceClassification(\n    model_id=\"bert_tiny\", \n    model=\"google/bert_uncased_L-2_H-128_A-2\"\n)\n\nA ModelServer is a simplified application package focused on stateless model evaluation. It can take as many tasks as we want.\n\nfrom learntorank.ml import ModelServer\n\nmodel_server = ModelServer(\n    name=\"bertModelServer\",\n    tasks=[task],\n)"
  },
  {
    "objectID": "stateless_sequence_classification_task.html#deploy-the-model-server",
    "href": "stateless_sequence_classification_task.html#deploy-the-model-server",
    "title": "Sequence Classification task",
    "section": "Deploy the model server",
    "text": "Deploy the model server\nWe can either host our model server on Vespa Cloud or deploy it locally using a Docker container.\n\nfrom vespa.deployment import VespaDocker\n\nvespa_docker = VespaDocker()\napp = vespa_docker.deploy(application_package=model_server)\n\nUsing framework PyTorch: 1.12.1\nFound input input_ids with shape: {0: 'batch', 1: 'sequence'}\nFound input token_type_ids with shape: {0: 'batch', 1: 'sequence'}\nFound input attention_mask with shape: {0: 'batch', 1: 'sequence'}\nFound output output_0 with shape: {0: 'batch'}\nEnsuring inputs are in correct order\nposition_ids is not present in the generated input list.\nGenerated inputs order: ['input_ids', 'attention_mask', 'token_type_ids']\nWaiting for configuration server, 0/300 seconds...\nWaiting for configuration server, 5/300 seconds...\nWaiting for application status, 0/300 seconds...\nWaiting for application status, 5/300 seconds...\nFinished deployment."
  },
  {
    "objectID": "stateless_sequence_classification_task.html#get-model-information",
    "href": "stateless_sequence_classification_task.html#get-model-information",
    "title": "Sequence Classification task",
    "section": "Get model information",
    "text": "Get model information\nGet models available:\n\napp.get_model_endpoint()\n\n{'bert_tiny': 'http://localhost:8080/model-evaluation/v1/bert_tiny'}\n\n\nGet information about a specific model:\n\napp.get_model_endpoint(model_id=\"bert_tiny\")\n\n{'model': 'bert_tiny',\n 'functions': [{'function': 'output_0',\n   'info': 'http://localhost:8080/model-evaluation/v1/bert_tiny/output_0',\n   'eval': 'http://localhost:8080/model-evaluation/v1/bert_tiny/output_0/eval',\n   'arguments': [{'name': 'input_ids', 'type': 'tensor(d0[],d1[])'},\n    {'name': 'attention_mask', 'type': 'tensor(d0[],d1[])'},\n    {'name': 'token_type_ids', 'type': 'tensor(d0[],d1[])'}]}]}"
  },
  {
    "objectID": "stateless_sequence_classification_task.html#get-predictions",
    "href": "stateless_sequence_classification_task.html#get-predictions",
    "title": "Sequence Classification task",
    "section": "Get predictions",
    "text": "Get predictions\nGet a prediction:\n\napp.predict(x=\"this is a test\", model_id=\"bert_tiny\")\n\n[-0.00954509899020195, 0.2504960000514984]"
  },
  {
    "objectID": "stateless_sequence_classification_task.html#cleanup",
    "href": "stateless_sequence_classification_task.html#cleanup",
    "title": "Sequence Classification task",
    "section": "Cleanup",
    "text": "Cleanup\n\nfrom shutil import rmtree\n\nvespa_docker.container.stop(timeout=600)\nvespa_docker.container.remove()"
  },
  {
    "objectID": "passage_dataset.html",
    "href": "passage_dataset.html",
    "title": "Dataset",
    "section": "",
    "text": "For the passage ranking use case, we will use the MS MARCO passage dataset 1 through the ir_datasets library. Besides being convenient, ir_datasets solves encoding errors in the original dataset source files.\nimport ir_datasets\nimport pandas as pd"
  },
  {
    "objectID": "passage_dataset.html#data-exploration",
    "href": "passage_dataset.html#data-exploration",
    "title": "Dataset",
    "section": "Data Exploration",
    "text": "Data Exploration\n\nDocument corpus\nStart by loading the data. The dataset will be downloaded once and cached on disk for future use, so it takes a while the first time the command below is run.\n\npassage_corpus = ir_datasets.load(\"msmarco-passage\")\n\nNumber of passages in the document corpus:\n\npassage_corpus.docs_count()\n\n8841823\n\n\nSample a few passages of the document corpus.\n\npd.DataFrame(passage_corpus.docs_iter()[0:5])\n\n\n\n\n\n\n\n\ndoc_id\ntext\n\n\n\n\n0\n0\nThe presence of communication amid scientific ...\n\n\n1\n1\nThe Manhattan Project and its atomic bomb help...\n\n\n2\n2\nEssay on The Manhattan Project - The Manhattan...\n\n\n3\n3\nThe Manhattan Project was the name for a proje...\n\n\n4\n4\nversions of each volume as well as complementa...\n\n\n\n\n\n\n\n\n\nTraining data\nLoad the training data. We use the judged version that only include queries with at least one relevance judgement.\n\npassage_train = ir_datasets.load(\"msmarco-passage/train/judged\")\n\n\nRelevant documents\nNumber of relevant judgements:\n\npassage_train.qrels_count()\n\n532761\n\n\nFor each query id, there is a dict of relevant documents containing the document id as key and the relevance score as value.\n\nfrom learntorank.passage import sample_dict_items\n\ntrain_qrels_dict = passage_train.qrels_dict()\nsample_dict_items(train_qrels_dict, 5)\n\n{'1038069': {'2293922': 1},\n '700425': {'4351261': 1},\n '926242': {'3500124': 1},\n '690553': {'2877918': 1},\n '411317': {'2230220': 1}}\n\n\nIt is interesting to check what is the range of values of the relevance score. The code below shows that the only score available is 1, indicating that the particular document id is relevant to the query id.\n\nset([score \n     for relevant in train_qrels_dict.values() \n     for score in relevant.values()]\n   )\n\n{1}\n\n\n\n\nQueries\nNumber of training queries:\n\npassage_train.queries_count()\n\n502939\n\n\nThe number of queries differs from the number of relevant documents because some of the queries have more than one relevant document associated with it.\nEach query contains a query id and a query text.\n\ntraining_queries = pd.DataFrame(passage_train.queries_iter())\ntraining_queries.head()\n\n\n\n\n\n\n\n\nquery_id\ntext\n\n\n\n\n0\n121352\ndefine extreme\n\n\n1\n634306\nwhat does chattel mean on credit history\n\n\n2\n920825\nwhat was the great leap forward brainly\n\n\n3\n510633\ntattoo fixers how much does it cost\n\n\n4\n737889\nwhat is decentralization process.\n\n\n\n\n\n\n\n\n\n\nDevelopment data\nSimilarly to the training data, we can load the judged development data and take a look at the queries and relevance judgements.\n\npassage_dev = ir_datasets.load(\"msmarco-passage/dev/judged\")\n\n\nRelevant documents\nNumber of relevant judgements:\n\npassage_dev.qrels_count()\n\n59273\n\n\nFor each query id, there is a dict of relevant documents containing the document id as key and the relevance score as value.\n\ndev_qrels_dict = passage_dev.qrels_dict()\nsample_dict_items(dev_qrels_dict, 5)\n\n{'255': {'7629892': 1},\n '611327': {'7610137': 1},\n '584695': {'7408281': 1},\n '300246': {'7814106': 1, '7814107': 1},\n '739094': {'7640560': 1}}\n\n\n\n\nQueries\nNumber of dev queries:\n\npassage_dev.queries_count()\n\n55578\n\n\nEach query contains a query id and a query text.\n\ndev_queries = pd.DataFrame(passage_dev.queries_iter())\ndev_queries.head()\n\n\n\n\n\n\n\n\nquery_id\ntext\n\n\n\n\n0\n1048578\ncost of endless pools/swim spa\n\n\n1\n1048579\nwhat is pcnt\n\n\n2\n1048582\nwhat is paysky\n\n\n3\n1048583\nwhat is paydata\n\n\n4\n1048585\nwhat is paula deen's brother"
  },
  {
    "objectID": "passage_dataset.html#data-manipulation",
    "href": "passage_dataset.html#data-manipulation",
    "title": "Dataset",
    "section": "Data Manipulation",
    "text": "Data Manipulation\n\nSample data\nGiven the large amount of data, it is useful to properly sample data when prototyping, which can be done with the sample_data function. This might take same time in case the full dataset needs to be downloaded for the first time.\n\nfrom learntorank.passage import sample_data\n\npassage_sample = sample_data(n_relevant=100, n_irrelevant=800)\n\n\npassage_sample\n\nPassageData(corpus, train_qrels, train_queries, dev_qrels, dev_queries)\n\n\n\nSave\nWe can save the sampled data to disk to avoid regenerating it everytime we need to use it.\n\npassage_sample.save(\"sample.json\")\n\n\n\nLoad\nLoad the data back when needed with PassageData.load:\n\nfrom learntorank.passage import PassageData\n\nloaded_sample = PassageData.load(file_path=\"sample.json\")"
  },
  {
    "objectID": "passage_dataset.html#footnotes",
    "href": "passage_dataset.html#footnotes",
    "title": "Dataset",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nMS MARCO: A Human Generated MAchine Reading COmprehension Dataset↩︎"
  },
  {
    "objectID": "module_evaluation.html",
    "href": "module_evaluation.html",
    "title": "evaluation",
    "section": "",
    "text": "Abstract and concrete classes related to evaluation metrics.\n\nsource\n\n\n\n EvalMetric ()\n\nAbstract class for evaluation metric.\n\nsource\n\n\n\n\n EvalMetric.evaluate_query (query_results, relevant_docs, id_field,\n                            default_score, detailed_metrics=False)\n\nAbstract method to be implemented by metrics inheriting from EvalMetric to evaluate query results.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nquery_results\n\n\nRaw query results returned by Vespa.\n\n\nrelevant_docs\n\n\nEach dict contains a doc id a optionally a doc score.\n\n\nid_field\n\n\nThe Vespa field representing the document id.\n\n\ndefault_score\n\n\nScore to assign to the additional documents that are not relevant. Default to 0.\n\n\ndetailed_metrics\nbool\nFalse\nReturn intermediate computations if available.\n\n\nReturns\ntyping.Dict\n\nMetric values.\n\n\n\n\nsource\n\n\n\n\n MatchRatio ()\n\nComputes the ratio of documents retrieved by the match phase.\nInstantiate the metric:\n\nmetric = MatchRatio()\n\n\nsource\n\n\n\n\n MatchRatio.evaluate_query (query_results:vespa.io.VespaQueryResponse,\n                            relevant_docs:List[Dict], id_field:str,\n                            default_score:int, detailed_metrics=False)\n\nEvaluate query results according to match ratio metric.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nquery_results\nVespaQueryResponse\n\nRaw query results returned by Vespa.\n\n\nrelevant_docs\ntyping.List[typing.Dict]\n\nEach dict contains a doc id a optionally a doc score.\n\n\nid_field\nstr\n\nThe Vespa field representing the document id.\n\n\ndefault_score\nint\n\nScore to assign to the additional documents that are not relevant. Default to 0.\n\n\ndetailed_metrics\nbool\nFalse\nReturn intermediate computations if available.\n\n\nReturns\ntyping.Dict\n\nReturns the match ratio. In addition, if detailed_metrics=False, returns the number of retrieved docs _retrieved_docs and the number of docs available in the corpus _docs_available.\n\n\n\nCompute match ratio:\n\nevaluation = metric.evaluate_query(\n    query_results=query_results, \n    relevant_docs=None,\n    id_field=\"vespa_id_field\",\n    default_score=0,\n)\nevaluation\n\n{'match_ratio': 0.01731996353691887}\n\n\nReturn detailed metrics, in addition to match ratio:\n\nevaluation = metric.evaluate_query(\n    query_results=query_results,\n    relevant_docs=None,\n    id_field=\"vespa_id_field\",\n    default_score=0,\n    detailed_metrics=True,\n)\nevaluation\n\n{'match_ratio': 0.01731996353691887,\n 'match_ratio_retrieved_docs': 1083,\n 'match_ratio_docs_available': 62529}\n\n\n\nsource\n\n\n\n\n TimeQuery ()\n\nCompute the time it takes for Vespa to execute the query..\nInstantiate the metric:\n\ntime_metric = TimeQuery()\n\n\nsource\n\n\n\n\n TimeQuery.evaluate_query (query_results:vespa.io.VespaQueryResponse,\n                           relevant_docs:List[Dict], id_field:str,\n                           default_score:int, detailed_metrics=False)\n\nEvaluate query results according to query time metric.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nquery_results\nVespaQueryResponse\n\nRaw query results returned by Vespa.\n\n\nrelevant_docs\ntyping.List[typing.Dict]\n\nEach dict contains a doc id a optionally a doc score.\n\n\nid_field\nstr\n\nThe Vespa field representing the document id.\n\n\ndefault_score\nint\n\nScore to assign to the additional documents that are not relevant. Default to 0.\n\n\ndetailed_metrics\nbool\nFalse\nReturn intermediate computations if available.\n\n\nReturns\ntyping.Dict\n\nReturns the match ratio. In addition, if detailed_metrics=False, returns the number of retrieved docs _retrieved_docs and the number of docs available in the corpus _docs_available.\n\n\n\nCompute the query time a client would observe (except network latency).\n\ntime_metric.evaluate_query(\n    query_results=query_results, \n    relevant_docs=None, \n    id_field=\"vespa_id_field\",\n    default_score=0\n)\n\n{'search_time': 0.013}\n\n\nInclude detailed metrics. In addition to the search_time above, it returns the time to execute the first protocol phase/matching phase (search_time_query_time) and the time to execute the summary fill protocol phase for the globally ordered top-k hits (search_time_summary_fetch_time).\n\ntime_metric.evaluate_query(\n    query_results=query_results, \n    relevant_docs=None, \n    id_field=\"vespa_id_field\",\n    default_score=0,\n    detailed_metrics=True\n)\n\n{'search_time': 0.013,\n 'search_time_query_time': 0.01,\n 'search_time_summary_fetch_time': 0.002}\n\n\n\nsource\n\n\n\n\n Recall (at:int)\n\nCompute the recall at position at.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nat\nint\nMaximum position on the resulting list to look for relevant docs.\n\n\nReturns\nNone\n\n\n\n\nInstantiate the metric:\n\nrecall_1 = Recall(at=1)\nrecall_2 = Recall(at=2)\nrecall_3 = Recall(at=3)\n\n\nsource\n\n\n\n\n Recall.evaluate_query (query_results:vespa.io.VespaQueryResponse,\n                        relevant_docs:List[Dict], id_field:str,\n                        default_score:int, detailed_metrics=False)\n\nEvaluate query results according to recall metric.\nThere is an assumption that only documents with score &gt; 0 are relevant. Recall is equal to zero in case no relevant documents with score &gt; 0 is provided.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nquery_results\nVespaQueryResponse\n\nRaw query results returned by Vespa.\n\n\nrelevant_docs\ntyping.List[typing.Dict]\n\nEach dict contains a doc id a optionally a doc score.\n\n\nid_field\nstr\n\nThe Vespa field representing the document id.\n\n\ndefault_score\nint\n\nScore to assign to the additional documents that are not relevant. Default to 0.\n\n\ndetailed_metrics\nbool\nFalse\nReturn intermediate computations if available.\n\n\nReturns\ntyping.Dict\n\nReturns the recall value.\n\n\n\nCompute recall:\n\nevaluation = recall_2.evaluate_query(\n    query_results=query_results,\n    relevant_docs=relevant_docs,\n    id_field=\"vespa_id_field\",\n    default_score=0,\n)\nevaluation\n\n{'recall_2': 0.5}\n\n\nCompute recall:\n\nsource\n\n\n\n\n ReciprocalRank (at:int)\n\nCompute the reciprocal rank at position at\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nat\nint\nMaximum position on the resulting list to look for relevant docs.\n\n\n\nInstantiate the metric:\n\nrr_1 = ReciprocalRank(at=1)\nrr_2 = ReciprocalRank(at=2)\nrr_3 = ReciprocalRank(at=3)\n\n\nsource\n\n\n\n\n ReciprocalRank.evaluate_query (query_results:vespa.io.VespaQueryResponse,\n                                relevant_docs:List[Dict], id_field:str,\n                                default_score:int, detailed_metrics=False)\n\nEvaluate query results according to reciprocal rank metric.\nThere is an assumption that only documents with score &gt; 0 are relevant.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nquery_results\nVespaQueryResponse\n\nRaw query results returned by Vespa.\n\n\nrelevant_docs\ntyping.List[typing.Dict]\n\nEach dict contains a doc id a optionally a doc score.\n\n\nid_field\nstr\n\nThe Vespa field representing the document id.\n\n\ndefault_score\nint\n\nScore to assign to the additional documents that are not relevant. Default to 0.\n\n\ndetailed_metrics\nbool\nFalse\nReturn intermediate computations if available.\n\n\nReturns\ntyping.Dict\n\nReturns the reciprocal rank value.\n\n\n\nCompute reciprocal rank:\n\nevaluation = rr_2.evaluate_query(\n    query_results=query_results,\n    relevant_docs=relevant_docs,\n    id_field=\"vespa_id_field\",\n    default_score=0,\n)\nevaluation\n\n{'reciprocal_rank_2': 0.5}\n\n\n\nsource\n\n\n\n\n NormalizedDiscountedCumulativeGain (at:int)\n\nCompute the normalized discounted cumulative gain at position at.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nat\nint\nMaximum position on the resulting list to look for relevant docs.\n\n\n\nInstantiate the metric:\n\nndcg_1 = NormalizedDiscountedCumulativeGain(at=1)\nndcg_2 = NormalizedDiscountedCumulativeGain(at=2)\nndcg_3 = NormalizedDiscountedCumulativeGain(at=3)\n\n\nsource\n\n\n\n\n NormalizedDiscountedCumulativeGain.evaluate_query\n                                                    (query_results:vespa.i\n                                                    o.VespaQueryResponse, \n                                                    relevant_docs:List[Dic\n                                                    t], id_field:str,\n                                                    default_score:int, det\n                                                    ailed_metrics=False)\n\nEvaluate query results according to normalized discounted cumulative gain.\nThere is an assumption that documents returned by the query that are not included in the set of relevant documents have score equal to zero. Similarly, if the query returns a number N &lt; at documents, we will assume that those N - at missing scores are equal to zero.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nquery_results\nVespaQueryResponse\n\nRaw query results returned by Vespa.\n\n\nrelevant_docs\ntyping.List[typing.Dict]\n\nEach dict contains a doc id a optionally a doc score.\n\n\nid_field\nstr\n\nThe Vespa field representing the document id.\n\n\ndefault_score\nint\n\nScore to assign to the additional documents that are not relevant. Default to 0.\n\n\ndetailed_metrics\nbool\nFalse\nReturn intermediate computations if available.\n\n\nReturns\ntyping.Dict\n\nReturns the normalized discounted cumulative gain. In addition, if detailed_metrics=False, returns the ideal discounted cumulative gain _ideal_dcg, the discounted cumulative gain _dcg.\n\n\n\nCompute NDCG:\n\nmetric = NormalizedDiscountedCumulativeGain(at=2)\nevaluation = ndcg_2.evaluate_query(\n    query_results=query_results,\n    relevant_docs=relevant_docs,\n    id_field=\"vespa_id_field\",\n    default_score=0,\n)\nevaluation\n\n{'ndcg_2': 0.38685280723454163}\n\n\nReturn detailed metrics, in addition to NDCG:\n\nevaluation = ndcg_2.evaluate_query(\n    query_results=query_results,\n    relevant_docs=relevant_docs,\n    id_field=\"vespa_id_field\",\n    default_score=0,\n    detailed_metrics=True,\n)\nevaluation\n\n{'ndcg_2': 0.38685280723454163,\n 'ndcg_2_ideal_dcg': 1.6309297535714575,\n 'ndcg_2_dcg': 0.6309297535714575}"
  },
  {
    "objectID": "module_evaluation.html#metrics",
    "href": "module_evaluation.html#metrics",
    "title": "evaluation",
    "section": "",
    "text": "Abstract and concrete classes related to evaluation metrics.\n\nsource\n\n\n\n EvalMetric ()\n\nAbstract class for evaluation metric.\n\nsource\n\n\n\n\n EvalMetric.evaluate_query (query_results, relevant_docs, id_field,\n                            default_score, detailed_metrics=False)\n\nAbstract method to be implemented by metrics inheriting from EvalMetric to evaluate query results.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nquery_results\n\n\nRaw query results returned by Vespa.\n\n\nrelevant_docs\n\n\nEach dict contains a doc id a optionally a doc score.\n\n\nid_field\n\n\nThe Vespa field representing the document id.\n\n\ndefault_score\n\n\nScore to assign to the additional documents that are not relevant. Default to 0.\n\n\ndetailed_metrics\nbool\nFalse\nReturn intermediate computations if available.\n\n\nReturns\ntyping.Dict\n\nMetric values.\n\n\n\n\nsource\n\n\n\n\n MatchRatio ()\n\nComputes the ratio of documents retrieved by the match phase.\nInstantiate the metric:\n\nmetric = MatchRatio()\n\n\nsource\n\n\n\n\n MatchRatio.evaluate_query (query_results:vespa.io.VespaQueryResponse,\n                            relevant_docs:List[Dict], id_field:str,\n                            default_score:int, detailed_metrics=False)\n\nEvaluate query results according to match ratio metric.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nquery_results\nVespaQueryResponse\n\nRaw query results returned by Vespa.\n\n\nrelevant_docs\ntyping.List[typing.Dict]\n\nEach dict contains a doc id a optionally a doc score.\n\n\nid_field\nstr\n\nThe Vespa field representing the document id.\n\n\ndefault_score\nint\n\nScore to assign to the additional documents that are not relevant. Default to 0.\n\n\ndetailed_metrics\nbool\nFalse\nReturn intermediate computations if available.\n\n\nReturns\ntyping.Dict\n\nReturns the match ratio. In addition, if detailed_metrics=False, returns the number of retrieved docs _retrieved_docs and the number of docs available in the corpus _docs_available.\n\n\n\nCompute match ratio:\n\nevaluation = metric.evaluate_query(\n    query_results=query_results, \n    relevant_docs=None,\n    id_field=\"vespa_id_field\",\n    default_score=0,\n)\nevaluation\n\n{'match_ratio': 0.01731996353691887}\n\n\nReturn detailed metrics, in addition to match ratio:\n\nevaluation = metric.evaluate_query(\n    query_results=query_results,\n    relevant_docs=None,\n    id_field=\"vespa_id_field\",\n    default_score=0,\n    detailed_metrics=True,\n)\nevaluation\n\n{'match_ratio': 0.01731996353691887,\n 'match_ratio_retrieved_docs': 1083,\n 'match_ratio_docs_available': 62529}\n\n\n\nsource\n\n\n\n\n TimeQuery ()\n\nCompute the time it takes for Vespa to execute the query..\nInstantiate the metric:\n\ntime_metric = TimeQuery()\n\n\nsource\n\n\n\n\n TimeQuery.evaluate_query (query_results:vespa.io.VespaQueryResponse,\n                           relevant_docs:List[Dict], id_field:str,\n                           default_score:int, detailed_metrics=False)\n\nEvaluate query results according to query time metric.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nquery_results\nVespaQueryResponse\n\nRaw query results returned by Vespa.\n\n\nrelevant_docs\ntyping.List[typing.Dict]\n\nEach dict contains a doc id a optionally a doc score.\n\n\nid_field\nstr\n\nThe Vespa field representing the document id.\n\n\ndefault_score\nint\n\nScore to assign to the additional documents that are not relevant. Default to 0.\n\n\ndetailed_metrics\nbool\nFalse\nReturn intermediate computations if available.\n\n\nReturns\ntyping.Dict\n\nReturns the match ratio. In addition, if detailed_metrics=False, returns the number of retrieved docs _retrieved_docs and the number of docs available in the corpus _docs_available.\n\n\n\nCompute the query time a client would observe (except network latency).\n\ntime_metric.evaluate_query(\n    query_results=query_results, \n    relevant_docs=None, \n    id_field=\"vespa_id_field\",\n    default_score=0\n)\n\n{'search_time': 0.013}\n\n\nInclude detailed metrics. In addition to the search_time above, it returns the time to execute the first protocol phase/matching phase (search_time_query_time) and the time to execute the summary fill protocol phase for the globally ordered top-k hits (search_time_summary_fetch_time).\n\ntime_metric.evaluate_query(\n    query_results=query_results, \n    relevant_docs=None, \n    id_field=\"vespa_id_field\",\n    default_score=0,\n    detailed_metrics=True\n)\n\n{'search_time': 0.013,\n 'search_time_query_time': 0.01,\n 'search_time_summary_fetch_time': 0.002}\n\n\n\nsource\n\n\n\n\n Recall (at:int)\n\nCompute the recall at position at.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nat\nint\nMaximum position on the resulting list to look for relevant docs.\n\n\nReturns\nNone\n\n\n\n\nInstantiate the metric:\n\nrecall_1 = Recall(at=1)\nrecall_2 = Recall(at=2)\nrecall_3 = Recall(at=3)\n\n\nsource\n\n\n\n\n Recall.evaluate_query (query_results:vespa.io.VespaQueryResponse,\n                        relevant_docs:List[Dict], id_field:str,\n                        default_score:int, detailed_metrics=False)\n\nEvaluate query results according to recall metric.\nThere is an assumption that only documents with score &gt; 0 are relevant. Recall is equal to zero in case no relevant documents with score &gt; 0 is provided.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nquery_results\nVespaQueryResponse\n\nRaw query results returned by Vespa.\n\n\nrelevant_docs\ntyping.List[typing.Dict]\n\nEach dict contains a doc id a optionally a doc score.\n\n\nid_field\nstr\n\nThe Vespa field representing the document id.\n\n\ndefault_score\nint\n\nScore to assign to the additional documents that are not relevant. Default to 0.\n\n\ndetailed_metrics\nbool\nFalse\nReturn intermediate computations if available.\n\n\nReturns\ntyping.Dict\n\nReturns the recall value.\n\n\n\nCompute recall:\n\nevaluation = recall_2.evaluate_query(\n    query_results=query_results,\n    relevant_docs=relevant_docs,\n    id_field=\"vespa_id_field\",\n    default_score=0,\n)\nevaluation\n\n{'recall_2': 0.5}\n\n\nCompute recall:\n\nsource\n\n\n\n\n ReciprocalRank (at:int)\n\nCompute the reciprocal rank at position at\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nat\nint\nMaximum position on the resulting list to look for relevant docs.\n\n\n\nInstantiate the metric:\n\nrr_1 = ReciprocalRank(at=1)\nrr_2 = ReciprocalRank(at=2)\nrr_3 = ReciprocalRank(at=3)\n\n\nsource\n\n\n\n\n ReciprocalRank.evaluate_query (query_results:vespa.io.VespaQueryResponse,\n                                relevant_docs:List[Dict], id_field:str,\n                                default_score:int, detailed_metrics=False)\n\nEvaluate query results according to reciprocal rank metric.\nThere is an assumption that only documents with score &gt; 0 are relevant.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nquery_results\nVespaQueryResponse\n\nRaw query results returned by Vespa.\n\n\nrelevant_docs\ntyping.List[typing.Dict]\n\nEach dict contains a doc id a optionally a doc score.\n\n\nid_field\nstr\n\nThe Vespa field representing the document id.\n\n\ndefault_score\nint\n\nScore to assign to the additional documents that are not relevant. Default to 0.\n\n\ndetailed_metrics\nbool\nFalse\nReturn intermediate computations if available.\n\n\nReturns\ntyping.Dict\n\nReturns the reciprocal rank value.\n\n\n\nCompute reciprocal rank:\n\nevaluation = rr_2.evaluate_query(\n    query_results=query_results,\n    relevant_docs=relevant_docs,\n    id_field=\"vespa_id_field\",\n    default_score=0,\n)\nevaluation\n\n{'reciprocal_rank_2': 0.5}\n\n\n\nsource\n\n\n\n\n NormalizedDiscountedCumulativeGain (at:int)\n\nCompute the normalized discounted cumulative gain at position at.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nat\nint\nMaximum position on the resulting list to look for relevant docs.\n\n\n\nInstantiate the metric:\n\nndcg_1 = NormalizedDiscountedCumulativeGain(at=1)\nndcg_2 = NormalizedDiscountedCumulativeGain(at=2)\nndcg_3 = NormalizedDiscountedCumulativeGain(at=3)\n\n\nsource\n\n\n\n\n NormalizedDiscountedCumulativeGain.evaluate_query\n                                                    (query_results:vespa.i\n                                                    o.VespaQueryResponse, \n                                                    relevant_docs:List[Dic\n                                                    t], id_field:str,\n                                                    default_score:int, det\n                                                    ailed_metrics=False)\n\nEvaluate query results according to normalized discounted cumulative gain.\nThere is an assumption that documents returned by the query that are not included in the set of relevant documents have score equal to zero. Similarly, if the query returns a number N &lt; at documents, we will assume that those N - at missing scores are equal to zero.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nquery_results\nVespaQueryResponse\n\nRaw query results returned by Vespa.\n\n\nrelevant_docs\ntyping.List[typing.Dict]\n\nEach dict contains a doc id a optionally a doc score.\n\n\nid_field\nstr\n\nThe Vespa field representing the document id.\n\n\ndefault_score\nint\n\nScore to assign to the additional documents that are not relevant. Default to 0.\n\n\ndetailed_metrics\nbool\nFalse\nReturn intermediate computations if available.\n\n\nReturns\ntyping.Dict\n\nReturns the normalized discounted cumulative gain. In addition, if detailed_metrics=False, returns the ideal discounted cumulative gain _ideal_dcg, the discounted cumulative gain _dcg.\n\n\n\nCompute NDCG:\n\nmetric = NormalizedDiscountedCumulativeGain(at=2)\nevaluation = ndcg_2.evaluate_query(\n    query_results=query_results,\n    relevant_docs=relevant_docs,\n    id_field=\"vespa_id_field\",\n    default_score=0,\n)\nevaluation\n\n{'ndcg_2': 0.38685280723454163}\n\n\nReturn detailed metrics, in addition to NDCG:\n\nevaluation = ndcg_2.evaluate_query(\n    query_results=query_results,\n    relevant_docs=relevant_docs,\n    id_field=\"vespa_id_field\",\n    default_score=0,\n    detailed_metrics=True,\n)\nevaluation\n\n{'ndcg_2': 0.38685280723454163,\n 'ndcg_2_ideal_dcg': 1.6309297535714575,\n 'ndcg_2_dcg': 0.6309297535714575}"
  },
  {
    "objectID": "module_evaluation.html#evaluation-queries-in-batch",
    "href": "module_evaluation.html#evaluation-queries-in-batch",
    "title": "evaluation",
    "section": "Evaluation queries in batch",
    "text": "Evaluation queries in batch\n\nsource\n\nevaluate\n\n evaluate (app:vespa.application.Vespa,\n           labeled_data:Union[List[Dict],pandas.core.frame.DataFrame],\n           eval_metrics:List[__main__.EvalMetric], query_model:Union[learn\n           torank.query.QueryModel,List[learntorank.query.QueryModel]],\n           id_field:str, default_score:int=0, detailed_metrics=False,\n           per_query=False, aggregators=None, timeout=1000, **kwargs)\n\nEvaluate a QueryModel according to a list of EvalMetric.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\napp\nVespa\n\nConnection to a Vespa application.\n\n\nlabeled_data\ntyping.Union[typing.List[typing.Dict], pandas.core.frame.DataFrame]\n\nData containing query, query_id and relevant docs. See examples below for format.\n\n\neval_metrics\ntyping.List[main.EvalMetric]\n\nEvaluation metrics\n\n\nquery_model\ntyping.Union[learntorank.query.QueryModel, typing.List[learntorank.query.QueryModel]]\n\nQuery models to be evaluated\n\n\nid_field\nstr\n\nThe Vespa field representing the document id.\n\n\ndefault_score\nint\n0\nScore to assign to the additional documents that are not relevant.\n\n\ndetailed_metrics\nbool\nFalse\nReturn intermediate computations if available.\n\n\nper_query\nbool\nFalse\nSet to True to return evaluation metrics per query.\n\n\naggregators\nNoneType\nNone\nUsed only if per_query=False. List of pandas friendly aggregators to summarize per model metrics. We use [“mean”, “median”, “std”] by default.\n\n\ntimeout\nint\n1000\nVespa query timeout in ms.\n\n\nkwargs\n\n\n\n\n\nReturns\nDataFrame\n\nReturns query_id and metrics according to the selected evaluation metrics.\n\n\n\nUsage:\nSetup and feed a Vespa application:\n\nfrom learntorank.passage import create_basic_search_package\nfrom learntorank.passage import PassageData\nfrom vespa.deployment import VespaDocker\n\n\napp_package = create_basic_search_package(name=\"EvaluationApp\")\nvespa_docker = VespaDocker(port=8082, cfgsrv_port=19072)\napp = vespa_docker.deploy(application_package=app_package)\ndata = PassageData.load()\nresponses = app.feed_df(\n    df=data.get_corpus(), \n    include_id=True, \n    id_field=\"doc_id\"\n)\n\nDefine query models to be evaluated:\n\nfrom learntorank.query import OR, Ranking\n\n\nbm25_query_model = QueryModel(\n    name=\"bm25\", \n    match_phase=OR(), \n    ranking=Ranking(name=\"bm25\")\n)\nnative_query_model = QueryModel(\n    name=\"native_rank\", \n    match_phase=OR(), \n    ranking=Ranking(name=\"native_rank\")\n)\n\nDefine metrics to compute during evaluation:\n\nmetrics = [\n    Recall(at=10), \n    ReciprocalRank(at=3), \n    NormalizedDiscountedCumulativeGain(at=3)\n]\n\nGet labeled data:\n\nlabeled_data = data.get_labels(type=\"dev\")\nlabeled_data[0:2]\n\n[{'query_id': '1101971',\n  'query': 'why say the sky is the limit',\n  'relevant_docs': [{'id': '7407715', 'score': 1}]},\n {'query_id': '712898',\n  'query': 'what is an cvc in radiology',\n  'relevant_docs': [{'id': '7661336', 'score': 1}]}]\n\n\nEvaluate:\n\nevaluation = evaluate(\n    app=app,\n    labeled_data=labeled_data, \n    eval_metrics=metrics, \n    query_model=[native_query_model, bm25_query_model], \n    id_field=\"doc_id\",\n)\nevaluation\n\n\n\n\n\n\n\n\nmodel\nbm25\nnative_rank\n\n\n\n\nrecall_10\nmean\n0.935833\n0.845833\n\n\nmedian\n1.000000\n1.000000\n\n\nstd\n0.215444\n0.342749\n\n\nreciprocal_rank_3\nmean\n0.935000\n0.746667\n\n\nmedian\n1.000000\n1.000000\n\n\nstd\n0.231977\n0.399551\n\n\nndcg_3\nmean\n0.912839\n0.740814\n\n\nmedian\n1.000000\n1.000000\n\n\nstd\n0.242272\n0.387611\n\n\n\n\n\n\n\nThe evaluate function also accepts labeled data as a data frame:\n\nlabeled_df.head()\n\n\n\n\n\n\n\n\nqid\nquery\ndoc_id\nrelevance\n\n\n\n\n0\n1101971\nwhy say the sky is the limit\n7407715\n1\n\n\n1\n712898\nwhat is an cvc in radiology\n7661336\n1\n\n\n2\n154469\ndmv california how long does it take to get id\n7914544\n1\n\n\n3\n930015\nwhat's an epigraph\n7928705\n1\n\n\n4\n860085\nwhat is va tax\n2915383\n1\n\n\n\n\n\n\n\n\nevaluation_df = evaluate(\n    app=app,\n    labeled_data=labeled_df, \n    eval_metrics=metrics, \n    query_model=[native_query_model, bm25_query_model], \n    id_field=\"doc_id\",\n)\nevaluation_df\n\n\n\n\n\n\n\n\nmodel\nbm25\nnative_rank\n\n\n\n\nrecall_10\nmean\n0.935833\n0.845833\n\n\nmedian\n1.000000\n1.000000\n\n\nstd\n0.215444\n0.342749\n\n\nreciprocal_rank_3\nmean\n0.935000\n0.746667\n\n\nmedian\n1.000000\n1.000000\n\n\nstd\n0.231977\n0.399551\n\n\nndcg_3\nmean\n0.912839\n0.740814\n\n\nmedian\n1.000000\n1.000000\n\n\nstd\n0.242272\n0.387611\n\n\n\n\n\n\n\nControl which aggregators are computed:\n\nevaluation = evaluate(\n    app=app,\n    labeled_data=labeled_data, \n    eval_metrics=metrics, \n    query_model=[native_query_model, bm25_query_model], \n    id_field=\"doc_id\",\n    aggregators=[\"mean\", \"std\"]\n)\nevaluation\n\n\n\n\n\n\n\n\nmodel\nbm25\nnative_rank\n\n\n\n\nrecall_10\nmean\n0.935833\n0.845833\n\n\nstd\n0.215444\n0.342749\n\n\nreciprocal_rank_3\nmean\n0.935000\n0.746667\n\n\nstd\n0.231977\n0.399551\n\n\nndcg_3\nmean\n0.912839\n0.740814\n\n\nstd\n0.242272\n0.387611\n\n\n\n\n\n\n\nInclude detailed metrics when available, this includes intermediate steps that are available for some of the metrics:\n\nevaluation = evaluate(\n    app=app,\n    labeled_data=labeled_data, \n    eval_metrics=metrics, \n    query_model=[native_query_model, bm25_query_model], \n    id_field=\"doc_id\",\n    aggregators=[\"mean\", \"std\"],\n    detailed_metrics=True\n)\nevaluation\n\n\n\n\n\n\n\n\nmodel\nbm25\nnative_rank\n\n\n\n\nrecall_10\nmean\n0.935833\n0.845833\n\n\nstd\n0.215444\n0.342749\n\n\nreciprocal_rank_3\nmean\n0.935000\n0.746667\n\n\nstd\n0.231977\n0.399551\n\n\nndcg_3\nmean\n0.912839\n0.740814\n\n\nstd\n0.242272\n0.387611\n\n\nndcg_3_ideal_dcg\nmean\n1.054165\n1.054165\n\n\nstd\n0.207315\n0.207315\n\n\nndcg_3_dcg\nmean\n0.938928\n0.765474\n\n\nstd\n0.225533\n0.387161\n\n\n\n\n\n\n\nGenerate results per query:\n\nevaluation = evaluate(\n    app=app,\n    labeled_data=labeled_data, \n    eval_metrics=metrics, \n    query_model=[native_query_model, bm25_query_model], \n    id_field=\"doc_id\",\n    per_query=True\n)\nevaluation.head()\n\n\n\n\n\n\n\n\nmodel\nquery_id\nrecall_10\nreciprocal_rank_3\nndcg_3\n\n\n\n\n0\nnative_rank\n1101971\n1.0\n1.0\n1.0\n\n\n1\nnative_rank\n712898\n0.0\n0.0\n0.0\n\n\n2\nnative_rank\n154469\n1.0\n0.0\n0.0\n\n\n3\nnative_rank\n930015\n1.0\n0.0\n0.0\n\n\n4\nnative_rank\n860085\n0.0\n0.0\n0.0"
  },
  {
    "objectID": "module_evaluation.html#evaluate-specific-query",
    "href": "module_evaluation.html#evaluate-specific-query",
    "title": "evaluation",
    "section": "Evaluate specific query",
    "text": "Evaluate specific query\n\nsource\n\nevaluate_query\n\n evaluate_query (app:vespa.application.Vespa,\n                 eval_metrics:List[__main__.EvalMetric],\n                 query_model:learntorank.query.QueryModel, query_id:str,\n                 query:str, id_field:str, relevant_docs:List[Dict],\n                 default_score:int=0, detailed_metrics=False, **kwargs)\n\nEvaluate a single query according to evaluation metrics\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\napp\nVespa\n\nConnection to a Vespa application.\n\n\neval_metrics\ntyping.List[main.EvalMetric]\n\nEvaluation metrics\n\n\nquery_model\nQueryModel\n\nQuery model to be evaluated\n\n\nquery_id\nstr\n\nQuery id represented as str.\n\n\nquery\nstr\n\nQuery string.\n\n\nid_field\nstr\n\nThe Vespa field representing the document id.\n\n\nrelevant_docs\ntyping.List[typing.Dict]\n\nEach dict contains a doc id a optionally a doc score.\n\n\ndefault_score\nint\n0\nScore to assign to the additional documents that are not relevant.\n\n\ndetailed_metrics\nbool\nFalse\nReturn intermediate computations if available.\n\n\nkwargs\n\n\n\n\n\nReturns\ntyping.Dict\n\nContains query_id and metrics according to the selected evaluation metrics.\n\n\n\nUsage:\n\napp = Vespa(url = \"https://api.cord19.vespa.ai\")\nquery_model = QueryModel(\n    match_phase = OR(),\n    ranking = Ranking(name=\"bm25\", list_features=True))\n\nEvaluate a single query:\n\nquery_evaluation = evaluate_query(\n    app=app,\n    eval_metrics = eval_metrics, \n    query_model = bm25_query_model, \n    query_id = \"0\", \n    query = \"Intrauterine virus infections and congenital heart disease\", \n    id_field = \"id\",\n    relevant_docs = [{\"id\": 0, \"score\": 1}, {\"id\": 3, \"score\": 1}],\n    default_score = 0\n)\nquery_evaluation\n\n{'model': 'bm25',\n 'query_id': '0',\n 'match_ratio': 0.814424921006077,\n 'recall_10': 0.0,\n 'reciprocal_rank_10': 0}"
  },
  {
    "objectID": "module_evaluation.html#evaluate-query-under-specific-document-ids",
    "href": "module_evaluation.html#evaluate-query-under-specific-document-ids",
    "title": "evaluation",
    "section": "Evaluate query under specific document ids",
    "text": "Evaluate query under specific document ids\nUse recall to specify which documents should be included in the evaluation.\nIn the example below, we include documents with id equal to 0, 1 and 2. Since the relevant documents for this query are the documents with id 0 and 3, we should get recall equal to 0.5.\n\nquery_evaluation = evaluate_query(\n    app=app,\n    eval_metrics = eval_metrics, \n    query_model = query_model, \n    query_id = 0, \n    query = \"Intrauterine virus infections and congenital heart disease\", \n    id_field = \"id\",\n    relevant_docs = [{\"id\": 0, \"score\": 1}, {\"id\": 3, \"score\": 1}],\n    default_score = 0,\n    recall = (\"id\", [0, 1, 2])\n)\nquery_evaluation\n\n{'model': 'default_name',\n 'query_id': 0,\n 'match_ratio': 9.70242657688688e-06,\n 'recall_10': 0.5,\n 'reciprocal_rank_10': 1.0}\n\n\nWe now include documents with id equal to 0, 1, 2 and 3. This should give a recall equal to 1.\n\nquery_evaluation = evaluate_query(\n    app=app,\n    eval_metrics = eval_metrics, \n    query_model = query_model, \n    query_id = 0, \n    query = \"Intrauterine virus infections and congenital heart disease\", \n    id_field = \"id\",\n    relevant_docs = [{\"id\": 0, \"score\": 1}, {\"id\": 3, \"score\": 1}],\n    default_score = 0,\n    recall = (\"id\", [0, 1, 2, 3])\n)\nquery_evaluation\n\n{'model': 'default_name',\n 'query_id': 0,\n 'match_ratio': 1.2936568769182506e-05,\n 'recall_10': 1.0,\n 'reciprocal_rank_10': 1.0}"
  },
  {
    "objectID": "module_ml.html",
    "href": "module_ml.html",
    "title": "ml",
    "section": "",
    "text": "source\n\n\n\n Task (model_id:str)\n\nBase class for ML Tasks.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nmodel_id\nstr\nId used to identify the model on Vespa applications.\n\n\n\n\nsource\n\n\n\n\n TextTask (model_id:str, model:str, tokenizer:Optional[str]=None,\n           output_file:&lt;class'IO'&gt;=&lt;_io.StringIO object at\n           0x7fb07610f550&gt;)\n\nBase class for Tasks involving text inputs.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nmodel_id\nstr\n\nId used to identify the model on Vespa applications.\n\n\nmodel\nstr\n\nId of the model as used by the model hub.\n\n\ntokenizer\ntyping.Optional[str]\nNone\nId of the tokenizer as used by the model hub.\n\n\noutput_file\nIO\n&lt;_io.StringIO object at 0x7fb07610f550&gt;\nOutput file to write output messages.\n\n\n\n\nsource\n\n\n\n\n TextTask.export_to_onnx (output_path:str)\n\nExport a model to ONNX\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\noutput_path\nstr\nRelative output path for the onnx model, should end in ‘.onnx’\n\n\nReturns\nNone\n\n\n\n\n\nsource\n\n\n\n\n TextTask.predict (text:str)\n\nPredict using a local instance of the model\n\n\n\n\nType\nDetails\n\n\n\n\ntext\nstr\ntext input for the task.\n\n\nReturns\ntyping.List\nPredictions.\n\n\n\n\nsource\n\n\n\n\n SequenceClassification (model_id:str, model:str,\n                         tokenizer:Optional[str]=None,\n                         output_file:&lt;class'IO'&gt;=&lt;_io.StringIO object at\n                         0x7fb0124a5790&gt;)\n\nSequence Classification task.\nIt takes a text input and returns an array of floats depending on which model is used to solve the task.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nmodel_id\nstr\n\nId used to identify the model on Vespa applications.\n\n\nmodel\nstr\n\nId of the model as used by the model hub. Alternatively, it can also be the path to the folder containing the model files, as long as the model config is also there.\n\n\ntokenizer\ntyping.Optional[str]\nNone\nId of the tokenizer as used by the model hub. Alternatively, it can also be the path to the folder containing the tokenizer files, as long as the model config is also there.\n\n\noutput_file\nIO\n&lt;_io.StringIO object at 0x7fb0124a5790&gt;\nOutput file to write output messages."
  },
  {
    "objectID": "module_ml.html#ml-tasks-involving-text-inputs",
    "href": "module_ml.html#ml-tasks-involving-text-inputs",
    "title": "ml",
    "section": "",
    "text": "source\n\n\n\n Task (model_id:str)\n\nBase class for ML Tasks.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nmodel_id\nstr\nId used to identify the model on Vespa applications.\n\n\n\n\nsource\n\n\n\n\n TextTask (model_id:str, model:str, tokenizer:Optional[str]=None,\n           output_file:&lt;class'IO'&gt;=&lt;_io.StringIO object at\n           0x7fb07610f550&gt;)\n\nBase class for Tasks involving text inputs.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nmodel_id\nstr\n\nId used to identify the model on Vespa applications.\n\n\nmodel\nstr\n\nId of the model as used by the model hub.\n\n\ntokenizer\ntyping.Optional[str]\nNone\nId of the tokenizer as used by the model hub.\n\n\noutput_file\nIO\n&lt;_io.StringIO object at 0x7fb07610f550&gt;\nOutput file to write output messages.\n\n\n\n\nsource\n\n\n\n\n TextTask.export_to_onnx (output_path:str)\n\nExport a model to ONNX\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\noutput_path\nstr\nRelative output path for the onnx model, should end in ‘.onnx’\n\n\nReturns\nNone\n\n\n\n\n\nsource\n\n\n\n\n TextTask.predict (text:str)\n\nPredict using a local instance of the model\n\n\n\n\nType\nDetails\n\n\n\n\ntext\nstr\ntext input for the task.\n\n\nReturns\ntyping.List\nPredictions.\n\n\n\n\nsource\n\n\n\n\n SequenceClassification (model_id:str, model:str,\n                         tokenizer:Optional[str]=None,\n                         output_file:&lt;class'IO'&gt;=&lt;_io.StringIO object at\n                         0x7fb0124a5790&gt;)\n\nSequence Classification task.\nIt takes a text input and returns an array of floats depending on which model is used to solve the task.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nmodel_id\nstr\n\nId used to identify the model on Vespa applications.\n\n\nmodel\nstr\n\nId of the model as used by the model hub. Alternatively, it can also be the path to the folder containing the model files, as long as the model config is also there.\n\n\ntokenizer\ntyping.Optional[str]\nNone\nId of the tokenizer as used by the model hub. Alternatively, it can also be the path to the folder containing the tokenizer files, as long as the model config is also there.\n\n\noutput_file\nIO\n&lt;_io.StringIO object at 0x7fb0124a5790&gt;\nOutput file to write output messages."
  },
  {
    "objectID": "module_ml.html#model-config-for-vespa-applications",
    "href": "module_ml.html#model-config-for-vespa-applications",
    "title": "ml",
    "section": "Model config for Vespa applications",
    "text": "Model config for Vespa applications\n\nsource\n\nModelConfig\n\n ModelConfig (model_id)\n\nBase model configuration for Vespa applications.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nmodel_id\n\nUnique model id to represent the model within a Vespa application.\n\n\nReturns\nNone\n\n\n\n\n\nsource\n\n\nBertModelConfig\n\n BertModelConfig (model_id:str, query_input_size:int, doc_input_size:int,\n                  tokenizer:Union[str,os.PathLike],\n                  model:Union[str,os.PathLike,NoneType]=None)\n\nBERT model configuration for Vespa applications.\n\n\n\nbert_config = BertModelConfig( … model_id=“pretrained_bert_tiny”, … query_input_size=32, … doc_input_size=96, … tokenizer=“google/bert_uncased_L-2_H-128_A-2”, … model=“google/bert_uncased_L-2_H-128_A-2”, … ) # doctest: +SKIP BertModelConfig(‘pretrained_bert_tiny’, 32, 96, ‘google/bert_uncased_L-2_H-128_A-2’, ‘google/bert_uncased_L-2_H-128_A-2’)\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nmodel_id\nstr\n\nUnique model id to represent the model within a Vespa application.\n\n\nquery_input_size\nint\n\nThe size of the input vector dedicated to the query text.\n\n\ndoc_input_size\nint\n\nThe size of the input vector dedicated to the document text.\n\n\ntokenizer\ntyping.Union[str, os.PathLike]\n\nThe name or a path to a saved BERT model tokenizer from the transformers library.\n\n\nmodel\ntyping.Union[str, os.PathLike, NoneType]\nNone\nThe name or a path to a saved model that is compatible with the tokenizer. The model is optional at construction since you might want to train it first. You must add a model via :func:add_model before deploying a Vespa application that uses this class.\n\n\nReturns\nNone\n\n\n\n\n\n\n#bert_config = BertModelConfig(\n#    model_id=\"pretrained_bert_tiny\",\n#    query_input_size=32,\n#    doc_input_size=96,\n#    tokenizer=\"google/bert_uncased_L-2_H-128_A-2\"\n#)\n\n\n#bert_config = BertModelConfig(\n#    model_id=\"pretrained_bert_tiny\",\n#    query_input_size=32,\n#    doc_input_size=96,\n#    tokenizer=\"google/bert_uncased_L-2_H-128_A-2\",\n#    model=\"google/bert_uncased_L-2_H-128_A-2\",\n#)\n\n\nsource\n\n\nBertModelConfig.predict\n\n BertModelConfig.predict (queries, docs)\n\nPredict (forward pass) given queries and docs texts\n\n\n\n\nType\nDetails\n\n\n\n\nqueries\n\nA List of query texts.\n\n\ndocs\n\nA List of document texts.\n\n\nReturns\ntyping.List\nLogits\n\n\n\n\nsource\n\n\nBertModelConfig.add_model\n\n BertModelConfig.add_model (model:Union[str,os.PathLike])\n\nAdd a BERT model\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nmodel\ntyping.Union[str, os.PathLike]\nThe name or a path to a saved model that is compatible with the tokenizer.\n\n\nReturns\nNone\n\n\n\n\n\nsource\n\n\nBertModelConfig.doc_fields\n\n BertModelConfig.doc_fields (text:str)\n\nGenerate document fields related to the model that needs to be fed to Vespa.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\ntext\nstr\nThe text related to the document to be used as input to the bert model\n\n\nReturns\ntyping.Dict\nDict with key and values as expected by Vespa.\n\n\n\n\nsource\n\n\nBertModelConfig.query_tensor_mapping\n\n BertModelConfig.query_tensor_mapping (text:str)\n\nMaps query text to a tensor expected by Vespa at run time.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\ntext\nstr\nQuery text to be used as input to the BERT model.\n\n\nReturns\ntyping.List[float]\nInput ids expected by Vespa.\n\n\n\n\nsource\n\n\nBertModelConfig.create_encodings\n\n BertModelConfig.create_encodings (queries:List[str], docs:List[str],\n                                   return_tensors=False)\n\nCreate BERT model encodings.\nCreate BERT encodings following the same pattern used during Vespa serving. Useful to generate training data and ensuring training and serving compatibility.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nqueries\ntyping.List[str]\n\nQuery texts.\n\n\ndocs\ntyping.List[str]\n\nDocument texts.\n\n\nreturn_tensors\nbool\nFalse\nReturn tensors\n\n\nReturns\ntyping.Dict\n\nDict containing input_ids, token_type_ids and attention_mask encodings.\n\n\n\n\nsource\n\n\nBertModelConfig.export_to_onnx\n\n BertModelConfig.export_to_onnx (output_path:str)\n\nExport a model to ONNX\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\noutput_path\nstr\nRelative output path for the onnx model, should end in ‘.onnx’\n\n\nReturns\nNone\n\n\n\n\n\nsource\n\n\nBertModelConfig.onnx_model\n\n BertModelConfig.onnx_model ()\n\n\nsource\n\n\nBertModelConfig.query_profile_type_fields\n\n BertModelConfig.query_profile_type_fields ()\n\n\nsource\n\n\nBertModelConfig.document_fields\n\n BertModelConfig.document_fields (document_field_indexing)\n\n\nsource\n\n\nBertModelConfig.rank_profile\n\n BertModelConfig.rank_profile (include_model_summary_features, **kwargs)"
  },
  {
    "objectID": "module_ml.html#model-server",
    "href": "module_ml.html#model-server",
    "title": "ml",
    "section": "Model Server",
    "text": "Model Server\n\nsource\n\nModelServer\n\n ModelServer (name:str, tasks:Optional[List[__main__.Task]]=None)\n\nCreate a Vespa stateless model evaluation server.\nA Vespa stateless model evaluation server is a simplified Vespa application without content clusters.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nname\nstr\n\nApplication name.\n\n\ntasks\ntyping.Optional[typing.List[main.Task]]\nNone\nList of tasks to be served."
  },
  {
    "objectID": "module_ml.html#add-ranking-model",
    "href": "module_ml.html#add-ranking-model",
    "title": "ml",
    "section": "Add ranking model",
    "text": "Add ranking model\n\nsource\n\nadd_ranking_model\n\n add_ranking_model (app_package:vespa.package.ApplicationPackage,\n                    model_config:__main__.ModelConfig, schema=None,\n                    include_model_summary_features=False,\n                    document_field_indexing=None, **kwargs)\n\nAdd ranking profile based on a specific model config.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\napp_package\nApplicationPackage\n\nApplication package to include ranking model\n\n\nmodel_config\nModelConfig\n\nModel config instance specifying the model to be used on the RankProfile.\n\n\nschema\nNoneType\nNone\nName of the schema to add model ranking to.\n\n\ninclude_model_summary_features\nbool\nFalse\nTrue to include model specific summary features, such as inputs and outputs that are useful for debugging. Default to False as this requires an extra model evaluation when fetching summary features.\n\n\ndocument_field_indexing\nNoneType\nNone\nList of indexing attributes for the document fields required by the ranking model.\n\n\nkwargs\n\n\n\n\n\nReturns\nNone\n\nFurther arguments to be passed to RankProfile."
  },
  {
    "objectID": "notebooks/collect-training-data.html",
    "href": "notebooks/collect-training-data.html",
    "title": "Collect training data from application",
    "section": "",
    "text": "Connect to the application and define a query model.\n\nfrom vespa.application import Vespa\nfrom learntorank.query import QueryModel, Ranking, OR\n\napp = Vespa(url = \"https://api.cord19.vespa.ai\")\nquery_model = QueryModel(\n    match_phase = OR(),\n    ranking = Ranking(name=\"bm25\", list_features=True)\n)\n\nDefine some labeled data.\n\nlabeled_data = [\n    {\n        \"query_id\": 0, \n        \"query\": \"Intrauterine virus infections and congenital heart disease\",\n        \"relevant_docs\": [{\"id\": 0, \"score\": 1}, {\"id\": 3, \"score\": 1}]\n    },\n    {\n        \"query_id\": 1, \n        \"query\": \"Clinical and immunologic studies in identical twins discordant for systemic lupus erythematosus\",\n        \"relevant_docs\": [{\"id\": 1, \"score\": 1}, {\"id\": 5, \"score\": 1}]\n    }\n]"
  },
  {
    "objectID": "notebooks/collect-training-data.html#example-setup",
    "href": "notebooks/collect-training-data.html#example-setup",
    "title": "Collect training data from application",
    "section": "",
    "text": "Connect to the application and define a query model.\n\nfrom vespa.application import Vespa\nfrom learntorank.query import QueryModel, Ranking, OR\n\napp = Vespa(url = \"https://api.cord19.vespa.ai\")\nquery_model = QueryModel(\n    match_phase = OR(),\n    ranking = Ranking(name=\"bm25\", list_features=True)\n)\n\nDefine some labeled data.\n\nlabeled_data = [\n    {\n        \"query_id\": 0, \n        \"query\": \"Intrauterine virus infections and congenital heart disease\",\n        \"relevant_docs\": [{\"id\": 0, \"score\": 1}, {\"id\": 3, \"score\": 1}]\n    },\n    {\n        \"query_id\": 1, \n        \"query\": \"Clinical and immunologic studies in identical twins discordant for systemic lupus erythematosus\",\n        \"relevant_docs\": [{\"id\": 1, \"score\": 1}, {\"id\": 5, \"score\": 1}]\n    }\n]"
  },
  {
    "objectID": "notebooks/collect-training-data.html#collect-training-data-in-batch",
    "href": "notebooks/collect-training-data.html#collect-training-data-in-batch",
    "title": "Collect training data from application",
    "section": "Collect training data in batch",
    "text": "Collect training data in batch\n\nfrom learntorank.query import collect_vespa_features\n\ntraining_data_batch = collect_vespa_features(\n    app=app,\n    labeled_data = labeled_data,\n    id_field = \"id\",\n    query_model = query_model,\n    number_additional_docs = 2,\n    fields=[\"rankfeatures\"]\n)\ntraining_data_batch\n\n\n\n\n\n\n\n\ndocument_id\nquery_id\nlabel\nattributeMatch(authors.first)\nattributeMatch(authors.first).averageWeight\nattributeMatch(authors.first).completeness\nattributeMatch(authors.first).fieldCompleteness\nattributeMatch(authors.first).importance\nattributeMatch(authors.first).matches\nattributeMatch(authors.first).maxWeight\n...\ntextSimilarity(results).fieldCoverage\ntextSimilarity(results).order\ntextSimilarity(results).proximity\ntextSimilarity(results).queryCoverage\ntextSimilarity(results).score\ntextSimilarity(title).fieldCoverage\ntextSimilarity(title).order\ntextSimilarity(title).proximity\ntextSimilarity(title).queryCoverage\ntextSimilarity(title).score\n\n\n\n\n0\n0\n0\n1\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.062500\n0.0\n0.0000\n0.142857\n0.055357\n\n\n1\n3\n0\n1\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.142857\n0.0\n0.4375\n0.142857\n0.224554\n\n\n4\n255164\n0\n0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n1.000000\n1.0\n1.0000\n1.000000\n1.000000\n\n\n5\n120761\n0\n0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.300000\n1.0\n1.0000\n0.428571\n0.688571\n\n\n2\n1\n1\n1\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.111111\n0.0\n0.0000\n0.083333\n0.047222\n\n\n3\n5\n1\n1\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.083333\n0.0\n0.0000\n0.083333\n0.041667\n\n\n8\n232555\n1\n0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n1.000000\n1.0\n1.0000\n1.000000\n1.000000\n\n\n9\n13944\n1\n0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.187500\n1.0\n1.0000\n0.250000\n0.612500\n\n\n\n\n8 rows × 1038 columns"
  },
  {
    "objectID": "notebooks/compare-pre-trained-clip-for-text-image-search.html",
    "href": "notebooks/compare-pre-trained-clip-for-text-image-search.html",
    "title": "Compare pre-trained CLIP models for text-image retrieval",
    "section": "",
    "text": "Get the Vespa sample application, install requirements:\n! git clone --depth 1 https://github.com/vespa-engine/sample-apps.git\n\n! pip install -r requirements.txt\nThere are multiple CLIP model variations:\nimport clip\n\nclip.available_models()\nTo limit run-time of this notebook, do not use all models - modify the set:\n# Limit number of models to test - here, first two models\nuse_models = clip.available_models()[0:2]\nEach model has an embedding size, needed in the text-image search application schema:\nembedding_info = {name: clip.load(name)[0].visual.output_dim for name in use_models}\nembedding_info"
  },
  {
    "objectID": "notebooks/compare-pre-trained-clip-for-text-image-search.html#create-and-deploy-a-text-image-search-app",
    "href": "notebooks/compare-pre-trained-clip-for-text-image-search.html#create-and-deploy-a-text-image-search-app",
    "title": "Compare pre-trained CLIP models for text-image retrieval",
    "section": "Create and deploy a text-image search app",
    "text": "Create and deploy a text-image search app\n\nCreate the Vespa application package\nThe function create_text_image_app below uses the Vespa python API to create an application package with fields to store each of the different types of image embedding associated with the CLIP models. It also declares the types of the text embeddings that we are going to send along with the query when searching for images, and creates one ranking profile for each (text, image) embedding model:\n\nfrom embedding import create_text_image_app\n\napp_package = create_text_image_app(embedding_info)\n\nInspect the schema of the resulting application package:\n\nprint(app_package.schema.schema_to_text)\n\n\n\nDeploy\n\nimport os\nfrom vespa.deployment import VespaDocker\n\nvespa_docker = VespaDocker()\napp = vespa_docker.deploy(application_package=app_package)"
  },
  {
    "objectID": "notebooks/compare-pre-trained-clip-for-text-image-search.html#compute-and-feed-image-embeddings",
    "href": "notebooks/compare-pre-trained-clip-for-text-image-search.html#compute-and-feed-image-embeddings",
    "title": "Compare pre-trained CLIP models for text-image retrieval",
    "section": "Compute and feed image embeddings",
    "text": "Compute and feed image embeddings\nGet a sample data set. See download_flickr8k.sh for how to download images. Set location of images:\nFor each of the CLIP models, compute the image embeddings and send it to the Vespa app:\n\nfrom embedding import compute_and_send_image_embeddings\n\ncompute_and_send_image_embeddings(app=app, batch_size=128, clip_model_names=use_models)"
  },
  {
    "objectID": "notebooks/compare-pre-trained-clip-for-text-image-search.html#define-querymodels-to-be-evaluated",
    "href": "notebooks/compare-pre-trained-clip-for-text-image-search.html#define-querymodels-to-be-evaluated",
    "title": "Compare pre-trained CLIP models for text-image retrieval",
    "section": "Define QueryModel’s to be evaluated",
    "text": "Define QueryModel’s to be evaluated\nCreate one QueryModel for each of the CLIP models. In order to do that, we need to have a function that takes a query as input and outputs the body function of a Vespa query request - example:\n\nfrom embedding import create_vespa_query_body_function\n\nvespa_query_body_function = create_vespa_query_body_function(\"RN50\")\nvespa_query_body_function(\"this is a test query\")[\"yql\"]\n\nWith a method to create Vespa query body functions, we can create QueryModels that will be used to evaluate each search configuration that is to be tested. In this case, each query model will represent a CLIP model text-image representation:\n\nfrom learntorank.query import QueryModel\n\nquery_models = [QueryModel(\n    name=model_name, \n    body_function=create_vespa_query_body_function(model_name)\n) for model_name in use_models]\n\nA query model contains all the information that is necessary to define how the search app will match and rank documents. Use it to query the application:\n\nfrom embedding import plot_images\nfrom learntorank.query import send_query\n\nquery_result = send_query(app, query=\"a person surfing\", query_model=query_models[-1], hits = 4)\n\nTo inspect the results, use query_result.hits[0]. Display top two:\n\nfrom IPython.display import Image, display\n\nimage_file_names = [ hit[\"fields\"][\"image_file_name\"] for hit in query_result.hits[:2] ]\n\nfor image in image_file_names:\n    display(Image(filename=os.path.join(os.environ[\"IMG_DIR\"], image)))"
  },
  {
    "objectID": "notebooks/compare-pre-trained-clip-for-text-image-search.html#evaluate",
    "href": "notebooks/compare-pre-trained-clip-for-text-image-search.html#evaluate",
    "title": "Compare pre-trained CLIP models for text-image retrieval",
    "section": "Evaluate",
    "text": "Evaluate\nNow that there is one QueryModel for each CLIP model available, it is posible to evaluate and compare them.\nDefine search evaluation metrics:\n\nfrom learntorank.evaluation import MatchRatio, Recall, ReciprocalRank\n\neval_metrics = [\n    MatchRatio(), # Match ratio is just to show the % of documents that are matched by ANN\n    Recall(at=100), \n    ReciprocalRank(at=100)\n]\n\nLoad labeled data. It was assumed that a (caption, image) pair is relevant if all three experts agreed that the caption accurately described the image:\n\nfrom pandas import read_csv\n\nlabeled_data = read_csv(\"https://data.vespa.oath.cloud/blog/flickr8k/labeled_data.csv\", sep = \"\\t\")\nlabeled_data.head()\n\nEvaluate the application and return per query results:\n\nfrom learntorank.evaluation import evaluate\n\nresult = evaluate(\n    app=app,\n    labeled_data=labeled_data, \n    eval_metrics=eval_metrics, \n    query_model=query_models, \n    id_field=\"image_file_name\",\n    per_query=True\n)\nresult.head()\n\nVisualize RR@100:\n\nimport matplotlib.pyplot as plt\n\nplt.plot(result.reciprocal_rank_100)\nplt.ylabel(\"reciprocal_rank_100\")\nplt.show()\n\nCompute mean and median across models:\n\nresult[[\"model\", \"reciprocal_rank_100\"]].groupby(\n    \"model\"\n).agg(\n    Mean=('reciprocal_rank_100', 'mean'), \n    Median=('reciprocal_rank_100', 'median')\n)"
  },
  {
    "objectID": "notebooks/compare-pre-trained-clip-for-text-image-search.html#cleanup",
    "href": "notebooks/compare-pre-trained-clip-for-text-image-search.html#cleanup",
    "title": "Compare pre-trained CLIP models for text-image retrieval",
    "section": "Cleanup",
    "text": "Cleanup\nStop and remove the Docker container:\n\nvespa_docker.container.stop()\nvespa_docker.container.remove()"
  },
  {
    "objectID": "notebooks/cord19/cord19_connect_evaluate.html",
    "href": "notebooks/cord19/cord19_connect_evaluate.html",
    "title": "How to evaluate Vespa ranking functions from python",
    "section": "",
    "text": "We can start by downloading the data that we have processed before.\n\nimport requests, json\nfrom pandas import read_csv\n\ntopics = json.loads(\n    requests.get(\"https://thigm85.github.io/data/cord19/topics.json\").text\n)\nrelevance_data = read_csv(\"https://thigm85.github.io/data/cord19/relevance_data.csv\")\n\ntopics contain data about the 50 topics available, including query, question and narrative.\n\ntopics[\"1\"]\n\n{'query': 'coronavirus origin',\n 'question': 'what is the origin of COVID-19',\n 'narrative': \"seeking range of information about the SARS-CoV-2 virus's origin, including its evolution, animal source, and first transmission into humans\"}\n\n\nrelevance_data contains the relevance judgments for each of the 50 topics.\n\nrelevance_data.head(5)\n\n\n\n\n\n\n\n\ntopic_id\nround_id\ncord_uid\nrelevancy\n\n\n\n\n0\n1\n4.5\n005b2j4b\n2\n\n\n1\n1\n4.0\n00fmeepz\n1\n\n\n2\n1\n0.5\n010vptx3\n2\n\n\n3\n1\n2.5\n0194oljo\n1\n\n\n4\n1\n4.0\n021q9884\n1"
  },
  {
    "objectID": "notebooks/cord19/cord19_connect_evaluate.html#download-processed-data",
    "href": "notebooks/cord19/cord19_connect_evaluate.html#download-processed-data",
    "title": "How to evaluate Vespa ranking functions from python",
    "section": "",
    "text": "We can start by downloading the data that we have processed before.\n\nimport requests, json\nfrom pandas import read_csv\n\ntopics = json.loads(\n    requests.get(\"https://thigm85.github.io/data/cord19/topics.json\").text\n)\nrelevance_data = read_csv(\"https://thigm85.github.io/data/cord19/relevance_data.csv\")\n\ntopics contain data about the 50 topics available, including query, question and narrative.\n\ntopics[\"1\"]\n\n{'query': 'coronavirus origin',\n 'question': 'what is the origin of COVID-19',\n 'narrative': \"seeking range of information about the SARS-CoV-2 virus's origin, including its evolution, animal source, and first transmission into humans\"}\n\n\nrelevance_data contains the relevance judgments for each of the 50 topics.\n\nrelevance_data.head(5)\n\n\n\n\n\n\n\n\ntopic_id\nround_id\ncord_uid\nrelevancy\n\n\n\n\n0\n1\n4.5\n005b2j4b\n2\n\n\n1\n1\n4.0\n00fmeepz\n1\n\n\n2\n1\n0.5\n010vptx3\n2\n\n\n3\n1\n2.5\n0194oljo\n1\n\n\n4\n1\n4.0\n021q9884\n1"
  },
  {
    "objectID": "notebooks/cord19/cord19_connect_evaluate.html#format-the-labeled-data-into-expected-pyvespa-format",
    "href": "notebooks/cord19/cord19_connect_evaluate.html#format-the-labeled-data-into-expected-pyvespa-format",
    "title": "How to evaluate Vespa ranking functions from python",
    "section": "Format the labeled data into expected pyvespa format",
    "text": "Format the labeled data into expected pyvespa format\npyvespa expects labeled data to follow the format illustrated below. It is a list of dict where each dict represents a query containing query_id, query and a list of relevant_docs. Each relevant document contains a required id key and an optional score key.\n\nlabeled_data = [\n    {\n        'query_id': 1,\n        'query': 'coronavirus origin',\n        'relevant_docs': [{'id': '005b2j4b', 'score': 2}, {'id': '00fmeepz', 'score': 1}]\n    },\n    {\n        'query_id': 2,\n        'query': 'coronavirus response to weather changes',\n        'relevant_docs': [{'id': '01goni72', 'score': 2}, {'id': '03h85lvy', 'score': 2}]\n    }\n]\n\nWe can create labeled_data from the topics and relevance_data that we downloaded before. We are only going to include documents with relevance score &gt; 0 into the final list.\n\nlabeled_data = [\n    {\n        \"query_id\": int(topic_id), \n        \"query\": topics[topic_id][\"query\"], \n        \"relevant_docs\": [\n            {\n                \"id\": row[\"cord_uid\"], \n                \"score\": row[\"relevancy\"]\n            } for idx, row in relevance_data[relevance_data.topic_id == int(topic_id)].iterrows() if row[\"relevancy\"] &gt; 0\n        ]\n    } for topic_id in topics.keys()]"
  },
  {
    "objectID": "notebooks/cord19/cord19_connect_evaluate.html#define-query-models-to-be-evaluated",
    "href": "notebooks/cord19/cord19_connect_evaluate.html#define-query-models-to-be-evaluated",
    "title": "How to evaluate Vespa ranking functions from python",
    "section": "Define query models to be evaluated",
    "text": "Define query models to be evaluated\nWe are going to define two query models to be evaluated here. Both will match all the documents that share at least one term with the query. This is defined by setting match_phase = OR().\nThe difference between the query models happens in the ranking phase. The or_default model will rank documents based on nativeRank while the or_bm25 model will rank documents based on BM25. Discussion about those two types of ranking is out of the scope of this tutorial. It is enough to know that they rank documents according to two different formulas.\nThose ranking profiles were defined by the team behind the cord19 app and can be found here.\n\nfrom learntorank.query import QueryModel, Ranking, OR\n\nquery_models = [\n    QueryModel(\n        name=\"or_default\",\n        match_phase = OR(),\n        ranking = Ranking(name=\"default\")\n    ),\n    QueryModel(\n        name=\"or_bm25\",\n        match_phase = OR(),\n        ranking = Ranking(name=\"bm25t5\")\n    )\n]"
  },
  {
    "objectID": "notebooks/cord19/cord19_connect_evaluate.html#define-metrics-to-be-used-in-the-evaluation",
    "href": "notebooks/cord19/cord19_connect_evaluate.html#define-metrics-to-be-used-in-the-evaluation",
    "title": "How to evaluate Vespa ranking functions from python",
    "section": "Define metrics to be used in the evaluation",
    "text": "Define metrics to be used in the evaluation\nWe would like to compute the following metrics:\n\nThe percentage of documents matched by the query\nRecall @ 10\nReciprocal rank @ 10\nNDCG @ 10\n\n\nfrom learntorank.evaluation import MatchRatio, Recall, ReciprocalRank, NormalizedDiscountedCumulativeGain\n\neval_metrics = [\n    MatchRatio(), \n    Recall(at=10), \n    ReciprocalRank(at=10), \n    NormalizedDiscountedCumulativeGain(at=10)\n]"
  },
  {
    "objectID": "notebooks/cord19/cord19_connect_evaluate.html#evaluate",
    "href": "notebooks/cord19/cord19_connect_evaluate.html#evaluate",
    "title": "How to evaluate Vespa ranking functions from python",
    "section": "Evaluate",
    "text": "Evaluate\nConnect to a running Vespa instance:\n\nfrom vespa.application import Vespa\n\napp = Vespa(url = \"https://api.cord19.vespa.ai\")\n\nCompute the metrics defined above for each query model.\n\nfrom learntorank.evaluation import evaluate\nevaluations = evaluate(\n    app=app,\n    labeled_data = labeled_data,\n    eval_metrics = eval_metrics,\n    query_model = query_models,\n    id_field = \"cord_uid\",\n    hits = 10\n)\nevaluations\n\n\n\n\n\n\n\n\nmodel\nor_bm25\nor_default\n\n\n\n\nmatch_ratio\nmean\n0.411789\n0.411789\n\n\nmedian\n0.282227\n0.282227\n\n\nstd\n0.238502\n0.238502\n\n\nrecall_10\nmean\n0.007720\n0.005457\n\n\nmedian\n0.006089\n0.003753\n\n\nstd\n0.006386\n0.005458\n\n\nreciprocal_rank_10\nmean\n0.594357\n0.561579\n\n\nmedian\n0.500000\n0.500000\n\n\nstd\n0.397597\n0.401255\n\n\nndcg_10\nmean\n0.353095\n0.274515\n\n\nmedian\n0.355978\n0.253619\n\n\nstd\n0.216460\n0.203170\n\n\n\n\n\n\n\nWe can also return per query raw evaluation metrics:\n\nevaluations = evaluate(\n    app=app,\n    labeled_data = labeled_data,\n    eval_metrics = eval_metrics,\n    query_model = query_models,\n    id_field = \"cord_uid\",\n    hits = 10,\n    per_query = True\n)\nevaluations.head()\n\n\n\n\n\n\n\n\nmodel\nquery_id\nmatch_ratio\nrecall_10\nreciprocal_rank_10\nndcg_10\n\n\n\n\n0\nor_default\n1\n0.230847\n0.008584\n1.000000\n0.519431\n\n\n1\nor_default\n2\n0.755230\n0.000000\n0.000000\n0.000000\n\n\n2\nor_default\n3\n0.264601\n0.001534\n0.142857\n0.036682\n\n\n3\nor_default\n4\n0.843341\n0.001764\n0.333333\n0.110046\n\n\n4\nor_default\n5\n0.901317\n0.003096\n0.250000\n0.258330"
  },
  {
    "objectID": "notebooks/tensorflow-via-onnx.html",
    "href": "notebooks/tensorflow-via-onnx.html",
    "title": "TensorFlow: Deploy model to Vespa through ONNX",
    "section": "",
    "text": "This tutorial will cover the following steps:"
  },
  {
    "objectID": "notebooks/tensorflow-via-onnx.html#install-packages",
    "href": "notebooks/tensorflow-via-onnx.html#install-packages",
    "title": "TensorFlow: Deploy model to Vespa through ONNX",
    "section": "Install packages",
    "text": "Install packages\n\n!pip3 install -Uqq pyvespa learntorank numpy==1.23.5 pandas tensorflow tensorflow_ranking onnx tf2onnx"
  },
  {
    "objectID": "notebooks/tensorflow-via-onnx.html#get-the-data",
    "href": "notebooks/tensorflow-via-onnx.html#get-the-data",
    "title": "TensorFlow: Deploy model to Vespa through ONNX",
    "section": "Get the data",
    "text": "Get the data\n\nimport pandas as pd\n\nDownload labeled data containing Vespa ranking features collected from an MS Marco passage ranking application.\n\ndf = pd.read_csv(\"https://data.vespa.oath.cloud/blog/ranking/train_sample.csv\")\ndf = df[\n    [\"document_id\", \n     \"query_id\", \n     \"label\", \n     \"fieldMatch(body).queryCompleteness\",\n     \"fieldMatch(body).significance\",\n     \"nativeRank\",\n    ]\n]\n\n\ndf.shape\n\n(100000, 6)\n\n\nFor each query_id, there is 9 irrelevant document_id with label = 0 and 1 relevant document_id with label = 1.\n\ndf.head(10)\n\n\n\n\n\n\n\n\ndocument_id\nquery_id\nlabel\nfieldMatch(body).queryCompleteness\nfieldMatch(body).significance\nnativeRank\n\n\n\n\n0\n27061\n3\n0\n0.625\n0.566311\n0.042421\n\n\n1\n257\n3\n0\n0.625\n0.582570\n0.039192\n\n\n2\n363\n3\n0\n0.500\n0.466030\n0.034418\n\n\n3\n22682\n3\n0\n0.625\n0.566311\n0.061149\n\n\n4\n160\n3\n0\n0.500\n0.437808\n0.035017\n\n\n5\n228\n3\n0\n0.500\n0.437808\n0.032697\n\n\n6\n3901893\n3\n0\n0.750\n0.748064\n0.074917\n\n\n7\n1142680\n3\n1\n0.750\n0.748064\n0.099112\n\n\n8\n141\n3\n0\n0.500\n0.442879\n0.038093\n\n\n9\n3060834\n3\n0\n0.750\n0.763933\n0.075347"
  },
  {
    "objectID": "notebooks/tensorflow-via-onnx.html#create-a-listwise-dataset",
    "href": "notebooks/tensorflow-via-onnx.html#create-a-listwise-dataset",
    "title": "TensorFlow: Deploy model to Vespa through ONNX",
    "section": "Create a listwise dataset",
    "text": "Create a listwise dataset\nDefine some parameters required to setup the listwise data pipeline.\n\nnumber_documents_per_query = 10            \nfeature_names = [                         \n    \"fieldMatch(body).queryCompleteness\", \n    \"fieldMatch(body).significance\", \n    \"nativeRank\"\n]\nnumber_features = len(feature_names)\nbatch_size=32\n\nEach feature data point will have the shape equal to (batch_size, number_documents_per_query, number_features) and each label data point will have shape equal to (batch_size, number_documents_per_query).\n\nimport tensorflow as tf\n\nThe code below creates a TensorFlow data pipeline (tf.data.Dataset) from our DataFrame and group the rows by the query_id variable to form a listwise dataset. We then configure the data pipeline to shuffle and set a batch size.\n\nshuffle_buffer_size = 10000\nds = tf.data.Dataset.from_tensor_slices(\n    {\n        \"features\": tf.cast(df[feature_names].values, tf.float32),\n        \"label\": tf.cast(df[\"label\"].values, tf.float32),\n        \"query_id\": tf.cast(df[\"query_id\"].values, tf.int64),\n    }\n)\n\nkey_func = lambda x: x[\"query_id\"]\nreduce_func = lambda key, dataset: dataset.batch(\n    number_documents_per_query, drop_remainder=True\n)\nlistwise_ds = ds.group_by_window(\n    key_func=key_func,\n    reduce_func=reduce_func,\n    window_size=number_documents_per_query,\n)\nlistwise_ds = listwise_ds.map(lambda x: (x[\"features\"], x[\"label\"]))\nlistwise_ds = listwise_ds.shuffle(buffer_size=shuffle_buffer_size).batch(\n    batch_size=batch_size\n)\n\nWe can see the shape of the features and of the labels are as expected.\n\nfor d in listwise_ds.take(1):\n    print(d[0].shape)\n    print(d[1].shape)\n\n(32, 10, 3)\n(32, 10)"
  },
  {
    "objectID": "notebooks/tensorflow-via-onnx.html#create-and-compile-model",
    "href": "notebooks/tensorflow-via-onnx.html#create-and-compile-model",
    "title": "TensorFlow: Deploy model to Vespa through ONNX",
    "section": "Create and compile model",
    "text": "Create and compile model\nWe are going to create a linear model that can take a listwise data as input with shape (batch_size, number_documents_per_query, number_features) and output one prediction per document with shape (batch_size, number_documents_per_query)\n\ninput_layer = tf.keras.layers.Input(shape=(number_documents_per_query, number_features))\ndense_layer = tf.keras.layers.Dense(\n    1,\n    use_bias=False,\n    activation=None,\n    name=\"dense\"\n)\noutput_layer = tf.keras.layers.Reshape((number_documents_per_query,))\n\n\nmodel = tf.keras.Sequential(layers=[input_layer, dense_layer, output_layer])\n\nIn this tutorial, we want to optimize the Normalized Discounted Cumulative Gain at position 10 (NDCG@10). We then select a loss function that is a smooth approximation of the NDCG metric and create a stateless NDCG@10 metric to use when compiling the model defined above.\n\nimport tensorflow_ranking as tfr\n\nndcg = tfr.keras.metrics.NDCGMetric(topn=10)\ndef ndcg_stateless(y_true, y_pred):\n    \"\"\"\n    Create stateless metric so that we can compute the validation metric \n    from scratch at the end of each epoch.\n    \"\"\"\n    ndcg.reset_states()\n    return ndcg(y_true, y_pred)\n\noptimizer = tf.keras.optimizers.Adagrad(learning_rate=2)\nmodel.compile(\n    optimizer=optimizer,\n    loss=tfr.keras.losses.ApproxNDCGLoss(),\n    metrics=ndcg_stateless,\n)\n\nUse the listwise dataset to fit the model:\n\nhistory = model.fit(listwise_ds, epochs=20)\n\nEpoch 1/20\n304/304 [==============================] - 8s 3ms/step - loss: -0.6522 - ndcg_stateless: 0.6874\nEpoch 2/20\n304/304 [==============================] - 1s 921us/step - loss: -0.6959 - ndcg_stateless: 0.7159\nEpoch 3/20\n304/304 [==============================] - 1s 905us/step - loss: -0.7001 - ndcg_stateless: 0.7166\nEpoch 4/20\n304/304 [==============================] - 1s 904us/step - loss: -0.7025 - ndcg_stateless: 0.7168\nEpoch 5/20\n304/304 [==============================] - 1s 901us/step - loss: -0.7043 - ndcg_stateless: 0.7165\nEpoch 6/20\n304/304 [==============================] - 1s 920us/step - loss: -0.7106 - ndcg_stateless: 0.7242\nEpoch 7/20\n304/304 [==============================] - 1s 903us/step - loss: -0.7355 - ndcg_stateless: 0.7647\nEpoch 8/20\n304/304 [==============================] - 1s 898us/step - loss: -0.7399 - ndcg_stateless: 0.7662\nEpoch 9/20\n304/304 [==============================] - 1s 923us/step - loss: -0.7430 - ndcg_stateless: 0.7679\nEpoch 10/20\n304/304 [==============================] - 1s 911us/step - loss: -0.7450 - ndcg_stateless: 0.7679\nEpoch 11/20\n304/304 [==============================] - 1s 955us/step - loss: -0.7464 - ndcg_stateless: 0.7682\nEpoch 12/20\n304/304 [==============================] - 1s 914us/step - loss: -0.7475 - ndcg_stateless: 0.7683\nEpoch 13/20\n304/304 [==============================] - 1s 919us/step - loss: -0.7485 - ndcg_stateless: 0.7689\nEpoch 14/20\n304/304 [==============================] - 1s 909us/step - loss: -0.7493 - ndcg_stateless: 0.7682\nEpoch 15/20\n304/304 [==============================] - 1s 904us/step - loss: -0.7499 - ndcg_stateless: 0.7692\nEpoch 16/20\n304/304 [==============================] - 1s 900us/step - loss: -0.7506 - ndcg_stateless: 0.7691\nEpoch 17/20\n304/304 [==============================] - 1s 893us/step - loss: -0.7513 - ndcg_stateless: 0.7699\nEpoch 18/20\n304/304 [==============================] - 1s 1ms/step - loss: -0.7516 - ndcg_stateless: 0.7694\nEpoch 19/20\n304/304 [==============================] - 1s 910us/step - loss: -0.7520 - ndcg_stateless: 0.7694\nEpoch 20/20\n304/304 [==============================] - 1s 830us/step - loss: -0.7524 - ndcg_stateless: 0.7686"
  },
  {
    "objectID": "notebooks/tensorflow-via-onnx.html#simplify-model-inputoutput-for-deployment",
    "href": "notebooks/tensorflow-via-onnx.html#simplify-model-inputoutput-for-deployment",
    "title": "TensorFlow: Deploy model to Vespa through ONNX",
    "section": "Simplify model input/output for deployment",
    "text": "Simplify model input/output for deployment\nAfter training the model by minimizing a listwise loss function, we can simplify the model before deploying it to Vespa. At inference time, Vespa will evaluate each document individually and use a ranking function to rank documents.\nTherefore, the input layer will expect a tensor named input with shape equal to (1, number_features).\n\nsimpler_model = tf.keras.Sequential(\n    [tf.keras.layers.Input(shape=(number_features,), batch_size=1, name=\"input\"), \n     dense_layer\n    ]\n)\n\nWe are going to save the simpler_model to disk and then use the tf2onnx tool to convert the model to ONNX format.\n\nsimpler_model.save(\"simpler_keras_model\")\n\nWARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\nINFO:tensorflow:Assets written to: simpler_keras_model/assets\n\n\nINFO:tensorflow:Assets written to: simpler_keras_model/assets\n\n\n\nfrom tf2onnx import convert\n\n!python3 -m tf2onnx.convert --saved-model simpler_keras_model --output simpler_keras_model.onnx\n\n&lt;frozen runpy&gt;:128: RuntimeWarning: 'tf2onnx.convert' found in sys.modules after import of package 'tf2onnx', but prior to execution of 'tf2onnx.convert'; this may result in unpredictable behaviour\n2023-08-08 14:09:40,224 - WARNING - '--tag' not specified for saved_model. Using --tag serve\n2023-08-08 14:09:40,328 - INFO - Signatures found in model: [serving_default].\n2023-08-08 14:09:40,328 - WARNING - '--signature_def' not specified, using first signature: serving_default\n2023-08-08 14:09:40,328 - INFO - Output names: ['dense']\n2023-08-08 14:09:40,328 - WARNING - Could not search for non-variable resources. Concrete function internal representation may have changed.\nWARNING:tensorflow:From /usr/local/lib/python3.11/site-packages/tf2onnx/tf_loader.py:557: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\nInstructions for updating:\nThis API was designed for TensorFlow v1. See https://www.tensorflow.org/guide/migrate for instructions on how to migrate your code to TensorFlow v2.\n2023-08-08 14:09:40,379 - WARNING - From /usr/local/lib/python3.11/site-packages/tf2onnx/tf_loader.py:557: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\nInstructions for updating:\nThis API was designed for TensorFlow v1. See https://www.tensorflow.org/guide/migrate for instructions on how to migrate your code to TensorFlow v2.\n2023-08-08 14:09:40,388 - INFO - Using tensorflow=2.13.0, onnx=1.14.0, tf2onnx=1.8.4/cd55bf\n2023-08-08 14:09:40,388 - INFO - Using opset &lt;onnx, 9&gt;\n2023-08-08 14:09:40,389 - INFO - Computed 0 values for constant folding\n2023-08-08 14:09:40,395 - INFO - Optimizing ONNX model\n2023-08-08 14:09:40,402 - INFO - After optimization: Identity -5 (5-&gt;0)\n2023-08-08 14:09:40,403 - INFO - \n2023-08-08 14:09:40,403 - INFO - Successfully converted TensorFlow model simpler_keras_model to ONNX\n2023-08-08 14:09:40,403 - INFO - Model inputs: ['input:0']\n2023-08-08 14:09:40,403 - INFO - Model outputs: ['dense']\n2023-08-08 14:09:40,403 - INFO - ONNX model is saved at simpler_keras_model.onnx\n\n\nWe can inspect the onnx model input and output. We first load the ONNX model:\n\nimport onnx                  \n\nm = onnx.load(\"simpler_keras_model.onnx\")\n\nAs mentioned before, the model expects a tensor named input with shape (1, 3).\n\nm.graph.input\n\n[name: \"input\"\ntype {\n  tensor_type {\n    elem_type: 1\n    shape {\n      dim {\n        dim_value: 1\n      }\n      dim {\n        dim_value: 3\n      }\n    }\n  }\n}\n]\n\n\nThe output will be a tensor named dense with shape (1,1).\n\nm.graph.output\n\n[name: \"dense\"\ntype {\n  tensor_type {\n    elem_type: 1\n    shape {\n      dim {\n        dim_value: 1\n      }\n      dim {\n        dim_value: 1\n      }\n    }\n  }\n}\n]"
  },
  {
    "objectID": "notebooks/tensorflow-via-onnx.html#define-the-application-package",
    "href": "notebooks/tensorflow-via-onnx.html#define-the-application-package",
    "title": "TensorFlow: Deploy model to Vespa through ONNX",
    "section": "Define the application package",
    "text": "Define the application package\nThis section will use the Vespa python API pyvespa to create an application package with a ranking function that uses the tensorflow model exported to ONNX.\nThe data used to train the model was derived from a Vespa application based on the MS Marco passage dataset. So, we are going to name the application msmarco, and start by adding two fields: id to hold the document id and text to hold the passages from the msmarco dataset.\nindexing configuration: We add \"summary\" to the indexing parameter because we want to include both the id and the text field in the query results. The \"attribute\" indicates that the field id will be stored in-memory. The \"index\" indicates that Vespa will create a search index for the text field.\n\nfrom vespa.package import ApplicationPackage, Field\n\napp_package = ApplicationPackage(name=\"msmarco\")\n\napp_package.schema.add_fields(\n    Field(name=\"id\", type=\"string\", indexing=[\"summary\", \"attribute\"]),\n    Field(name=\"text\", type=\"string\", indexing=[\"summary\", \"index\"])\n)\n\nNote that at each step along the application package definition, we can inspect the content of the Vespa search definition file:\n\nprint(app_package.schema.schema_to_text)\n\nschema msmarco {\n    document msmarco {\n        field id type string {\n            indexing: summary | attribute\n        }\n        field text type string {\n            indexing: summary | index\n        }\n    }\n}\n\n\nAdd simpler_keras_model.onnx to the schema. * The model_name is an id that can be used in the ranking function to identify which model to use. * The model_file_path is the current path of the .onnx file. When deploying the application, pyvespa will move the file to the correct location inside the Vespa application package folder. * The inputs maps the name of the inputs contained in the ONNX model to the name of the Vespa source that will be used as input to the model. In this case we will create a function called vespa_input that output a tensor of type float with the expected shape (1, 3). * The outputs maps the output name in the ONNX file to the output name that will be recognized by Vespa.\n\nfrom vespa.package import OnnxModel\n\napp_package.schema.add_model(\n    OnnxModel(\n        model_name=\"ltr_tensorflow\",\n        model_file_path=\"simpler_keras_model.onnx\",\n        inputs={\"input\": \"vespa_input\"},\n        outputs={\"dense\": \"dense\"},\n    )\n)\n\nIt is possible to see the addition of the onnx-model section in the search definition below. Note that the model file is expected to be under the files folder inside the final application package folder, but pyvespa takes care of the model file placement when deploying the application.\n\nprint(app_package.schema.schema_to_text)\n\nschema msmarco {\n    document msmarco {\n        field id type string {\n            indexing: summary | attribute\n        }\n        field text type string {\n            indexing: summary | index\n        }\n    }\n    onnx-model ltr_tensorflow {\n        file: files/ltr_tensorflow.onnx\n        input input:0: vespa_input\n        output dense: dense\n    }\n}\n\n\nAdd a rank profile named tensorflow that uses the TensorFlow model to rank documents. * first_phase: We use the Vespa ranking feature onnx to access the ONNX model named ltr_tensorflow and use the output dense. We apply the sum because Vespa requires the relevance score to be a scaler and the output of the ONNX model in this case is a tensor of shape (1,1). * vespa_input function: The ONNX model was trained with the features fieldMatch(text).queryCompleteness, fieldMatch(text).significance and nativeRank(text) and expects and tensor of shape (1,3) containing those features. * summary_features: Summary features allow us to specify Vespa features to be included in the output of a query. In this case, we want to access to the model inputs and output to check if the Vespa model evaluation is the same as if we use the original TensorFlow model.\n\nfrom vespa.package import RankProfile, Function\n\napp_package.schema.add_rank_profile(\n    RankProfile(\n        name=\"tensorflow\", \n        first_phase=\"sum(onnx(ltr_tensorflow).dense)\", \n        functions=[\n            Function(\n                name=\"vespa_input\", \n                expression=\"tensor&lt;float&gt;(x[1],y[3]):[[\"\n                    \"fieldMatch(text).queryCompleteness, \"\n                    \"fieldMatch(text).significance, \"\n                    \"nativeRank(text)\"\n                \"]]\"\n            )\n        ],\n        summary_features=[\n            \"onnx(ltr_tensorflow)\", \n            \"fieldMatch(text).queryCompleteness\", \n            \"fieldMatch(text).significance\", \n            \"nativeRank(text)\"\n        ]\n    )\n)\n\nThe rank-profile called tensorflow can be seen below:\n\nprint(app_package.schema.schema_to_text)\n\nschema msmarco {\n    document msmarco {\n        field id type string {\n            indexing: summary | attribute\n        }\n        field text type string {\n            indexing: summary | index\n        }\n    }\n    onnx-model ltr_tensorflow {\n        file: files/ltr_tensorflow.onnx\n        input input:0: vespa_input\n        output dense: dense\n    }\n    rank-profile tensorflow {\n        function vespa_input() {\n            expression {\n                tensor&lt;float&gt;(x[1],y[3]):[[fieldMatch(text).queryCompleteness, fieldMatch(text).significance, nativeRank(text)]]\n            }\n        }\n        first-phase {\n            expression {\n                sum(onnx(ltr_tensorflow).dense)\n            }\n        }\n        summary-features {\n            onnx(ltr_tensorflow)\n            fieldMatch(text).queryCompleteness\n            fieldMatch(text).significance\n            nativeRank(text)\n        }\n    }\n}\n\n\nNow that we are done with the application package definition. We can deploy the application:\n\nfrom vespa.deployment import VespaDocker\n\nvespa_docker = VespaDocker()\napp = vespa_docker.deploy(application_package=app_package)\n\nWaiting for configuration server, 0/300 seconds...\nWaiting for configuration server, 5/300 seconds...\nWaiting for application status, 0/300 seconds...\nWaiting for application status, 5/300 seconds...\nWaiting for application status, 10/300 seconds...\nWaiting for application status, 15/300 seconds...\nWaiting for application status, 20/300 seconds...\nWaiting for application status, 25/300 seconds...\nFinished deployment."
  },
  {
    "objectID": "notebooks/tensorflow-via-onnx.html#feed-the-application",
    "href": "notebooks/tensorflow-via-onnx.html#feed-the-application",
    "title": "TensorFlow: Deploy model to Vespa through ONNX",
    "section": "Feed the application",
    "text": "Feed the application\nOnce the application is running, it is time to feed msmarco passage data to it.\n\nfrom learntorank.passage import PassageData\n\ndataset = PassageData.load()\n\nWe are going to use only 10 documents because our goal here is to show that Vespa returns the correct predictions from the TensorFlow model.\n\ndata = dataset.get_corpus().head(10)\ndata.rename(columns={'doc_id': 'id'}, inplace=True)\n\n\ndata.head()\n\n\n\n\n\n\n\n\nid\ntext\n\n\n\n\n0\n5954248\nWhy GameStop is excited for Dragon Age: Inquis...\n\n\n1\n7290700\nmetaplasia definition: 1. abnormal change of o...\n\n\n2\n5465518\nCandice Net Worth. According to the report of ...\n\n\n3\n3100518\nUnder the Base Closure Act, March AFB was down...\n\n\n4\n3207764\nThere are a number of career opportunities for...\n\n\n\n\n\n\n\nFeed the data to the application.\n\nresult = app.feed_df(df=data, include_id=True)\n\nSuccessful documents fed: 10/10.\nBatch progress: 1/1."
  },
  {
    "objectID": "notebooks/tensorflow-via-onnx.html#validate-vespa-predictions",
    "href": "notebooks/tensorflow-via-onnx.html#validate-vespa-predictions",
    "title": "TensorFlow: Deploy model to Vespa through ONNX",
    "section": "Validate Vespa predictions",
    "text": "Validate Vespa predictions\nGet query from the small dev set to use to validate Vespa TensorFlow predictions.\n\nquery_text = dataset.get_queries(type=\"dev\").iloc[0,1]\nquery_text = query_text.replace(\"'\", \"\")\n\n\nquery_text\n\n'why say the sky is the limit'\n\n\nThe code below shows the YQL expression that will be used to select the documents to be ranked.\n\n\"select * from sources * where ({{grammar: 'any', defaultIndex: 'text'}}userInput('{}'))\".format(query_text)\n\n\"select * from sources * where ({grammar: 'any', defaultIndex: 'text'}userInput('why say the sky is the limit'))\"\n\n\nThe function get_vespa_prediction_and_features will match documents using the YQL expression above and rank the documents with the rank-profile tensorflow that we defined in the Vespa application package.\n\ndef get_vespa_prediction_and_features(query_text):\n    # Send query and extract hits\n    hits = app.query(\n                body={\n                    \"yql\": \"select * from sources * where ({{'grammar': 'any', 'defaultIndex': 'text'}}userInput('{}'));\".format(query_text),\n                    \"ranking\": \"tensorflow\"\n                }\n            ).hits\n    result =[]\n    # For each hit, extract the inputs to the model along with model predictions computed by Vespa\n    for hit in hits:\n        result.append({\n            \"fieldMatch(text).queryCompleteness\": hit[\"fields\"][\"summaryfeatures\"][\"fieldMatch(text).queryCompleteness\"],\n            \"fieldMatch(text).significance\": hit[\"fields\"][\"summaryfeatures\"][\"fieldMatch(text).significance\"],\n            \"nativeRank(text)\": hit[\"fields\"][\"summaryfeatures\"][\"nativeRank(text)\"],\n            \"vespa_prediction\": hit[\"relevance\"],             \n        })\n    return pd.DataFrame.from_records(result)\n\nInputs and vespa predictions:\n\npredictions = get_vespa_prediction_and_features(query_text=query_text)\npredictions\n\n\n\n\n\n\n\n\nfieldMatch(text).queryCompleteness\nfieldMatch(text).significance\nnativeRank(text)\nvespa_prediction\n\n\n\n\n0\n0.285714\n0.199799\n0.061853\n0.360788\n\n\n1\n0.571429\n0.415687\n0.086940\n-0.128510\n\n\n2\n0.428571\n0.302071\n0.065154\n-0.240481\n\n\n3\n0.428571\n0.302071\n0.050600\n-0.670632\n\n\n4\n0.428571\n0.302071\n0.049802\n-0.694231\n\n\n5\n0.285714\n0.199799\n0.025552\n-0.712175\n\n\n6\n0.428571\n0.302071\n0.045398\n-0.824390\n\n\n\n\n\n\n\nCompute predictions from the TensorFlow model simpler_model directly:\n\npredictions[\"tf_prediction\"] = predictions[\n    [\"fieldMatch(text).queryCompleteness\", \"fieldMatch(text).significance\", \"nativeRank(text)\"]\n].apply(lambda x: simpler_model.predict([x.tolist()])[0][0], axis=1)\n\n1/1 [==============================] - 0s 71ms/step\n1/1 [==============================] - 0s 28ms/step\n1/1 [==============================] - 0s 29ms/step\n1/1 [==============================] - 0s 29ms/step\n1/1 [==============================] - 0s 28ms/step\n1/1 [==============================] - 0s 26ms/step\n1/1 [==============================] - 0s 26ms/step\n\n\n\npredictions\n\n\n\n\n\n\n\n\nfieldMatch(text).queryCompleteness\nfieldMatch(text).significance\nnativeRank(text)\nvespa_prediction\ntf_prediction\n\n\n\n\n0\n0.285714\n0.199799\n0.061853\n0.360788\n0.360788\n\n\n1\n0.571429\n0.415687\n0.086940\n-0.128510\n-0.128510\n\n\n2\n0.428571\n0.302071\n0.065154\n-0.240481\n-0.240481\n\n\n3\n0.428571\n0.302071\n0.050600\n-0.670632\n-0.670632\n\n\n4\n0.428571\n0.302071\n0.049802\n-0.694231\n-0.694231\n\n\n5\n0.285714\n0.199799\n0.025552\n-0.712175\n-0.712176\n\n\n6\n0.428571\n0.302071\n0.045398\n-0.824390\n-0.824390\n\n\n\n\n\n\n\nCheck that the predictions from the model deployed in Vespa are (almost) equal to the predictions obtained directly from the model.\n\nfrom numpy.testing import assert_almost_equal\n\nassert_almost_equal(predictions[\"vespa_prediction\"].tolist(), predictions[\"tf_prediction\"].tolist(), 5)"
  },
  {
    "objectID": "notebooks/tensorflow-via-onnx.html#clean-environment",
    "href": "notebooks/tensorflow-via-onnx.html#clean-environment",
    "title": "TensorFlow: Deploy model to Vespa through ONNX",
    "section": "Clean environment",
    "text": "Clean environment\n\nimport shutil\n\nshutil.rmtree(\"simpler_keras_model\") \nvespa_docker.container.stop(timeout=600)\nvespa_docker.container.remove()"
  },
  {
    "objectID": "module_ranking.html",
    "href": "module_ranking.html",
    "title": "ranking",
    "section": "",
    "text": "source\n\nkeras_linear_model\n\n keras_linear_model (number_documents_per_query, number_features)\n\nlinear model with a lasso constrain on the kernel weights.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nnumber_documents_per_query\n\nNumber of documents per query to reshape the listwise prediction.\n\n\nnumber_features\n\nNumber of features used per document.\n\n\nReturns\nSequential\nThe uncompiled Keras model.\n\n\n\nUsage:\n\nklm = keras_linear_model(\n    number_documents_per_query=10, \n    number_features=5\n)\n\n\nsource\n\n\nkeras_lasso_linear_model\n\n keras_lasso_linear_model (number_documents_per_query, number_features,\n                           l1_penalty, normalization_layer:Optional=None)\n\nlinear model with a lasso constrain on the kernel weights.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nnumber_documents_per_query\n\n\nNumber of documents per query to reshape the listwise prediction.\n\n\nnumber_features\n\n\nNumber of features used per document.\n\n\nl1_penalty\n\n\nControls the L1-norm penalty.\n\n\nnormalization_layer\ntyping.Optional\nNone\nInitialized normalization layers. Used when performing feature selection.\n\n\nReturns\nSequential\n\nThe uncompiled Keras model.\n\n\n\nUsage:\n\nkllm = keras_lasso_linear_model(\n    number_documents_per_query=10, \n    number_features=5, \n    l1_penalty=0.01\n)\n\n\nsource\n\n\nkeras_ndcg_compiled_model\n\n keras_ndcg_compiled_model (model, learning_rate, top_n)\n\nCompile listwise Keras model with NDCG stateless metric and ApproxNDCGLoss\n\n\n\n\nDetails\n\n\n\n\nmodel\nUncompiled Keras model\n\n\nlearning_rate\nLearning rate used in the Adagrad optim algo.\n\n\ntop_n\nTop n used when computing the NDCG metric\n\n\n\nUsage:\n\ncompiled_klm = keras_ndcg_compiled_model(\n    model=klm, \n    learning_rate=0.1, \n    top_n=10\n)\n\n\nsource\n\n\nLinearHyperModel\n\n LinearHyperModel (number_documents_per_query, number_features, top_n=10,\n                   learning_rate_range=None)\n\nDefine a KerasTuner search space for linear models\n\nlinear_hyper_model = LinearHyperModel(\n    number_documents_per_query=10, \n    number_features=10, \n    top_n=10, \n    learning_rate_range=[1e-2, 1e2]\n)\n\n\nsource\n\n\nLassoHyperModel\n\n LassoHyperModel (number_documents_per_query, number_features,\n                  trained_normalization_layer, top_n=10,\n                  l1_penalty_range=None, learning_rate_range=None)\n\nDefine a KerasTuner search space for lasso models\n\nsource\n\n\nListwiseRankingFramework\n\n ListwiseRankingFramework (number_documents_per_query, batch_size=32,\n                           shuffle_buffer_size=1000, tuner_max_trials=3,\n                           tuner_executions_per_trial=1, tuner_epochs=1,\n                           tuner_early_stop_patience=None, final_epochs=1,\n                           top_n=10, l1_penalty_range=None,\n                           learning_rate_range=None,\n                           folder_dir='/home/runner/work/learntorank-\n                           DEPRECATED/learntorank-DEPRECATED')\n\nListwise ranking framework"
  }
]